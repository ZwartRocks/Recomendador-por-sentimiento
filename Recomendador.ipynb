{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6764e2b5-b25a-4df7-8699-eab81df6e031",
   "metadata": {},
   "source": [
    "## Arquitectura General\n",
    "Descripción General\n",
    "Sistema completo de recomendación turística que combina Deep Learning con factores dinámicos en tiempo real para generar recomendaciones personalizadas.\n",
    "Componentes Principales\n",
    "\n",
    "Procesamiento de Datos: Limpieza y preparación de múltiples fuentes\n",
    "Modelo de Deep Learning: Red neuronal híbrida optimizada para GPU\n",
    "Re-ranking Dinámico: Ajuste en tiempo real basado en clima y tendencias\n",
    "Pipeline Automatizado: Orquestación y actualización diaria\n",
    "\n",
    "Flujo de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e46cc2-a6a7-452b-843f-2427141c6c93",
   "metadata": {},
   "source": [
    "Datos Crudos → Preprocesamiento → Entrenamiento → Modelo Base → Re-ranking → Recomendaciones Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c34a04-31ca-4803-bd17-56c9eca57c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import requests\n",
    "import json\n",
    "import sqlite3\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Web scraping (opcional)\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    SELENIUM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SELENIUM_AVAILABLE = False\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c23eb860-9bee-4f01-b9c6-b3d5fa256c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        physical_devices[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3584)]\n",
    "    )\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('tourism_recommender_system.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# OpenWeatherMap API Configuration\n",
    "WEATHER_API_KEY = \"1e74b5d9bcc3b9b36252ad21109989ec\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c622bad-b852-44fb-bb61-471ee82b3875",
   "metadata": {},
   "source": [
    "### Limpieza y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd77600-1a3f-4924-9c43-55030c9c3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourismDataPreprocessor:\n",
    "    \"\"\"Clase completa para preprocesar datos de turismo con todas las funcionalidades\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        \n",
    "    def load_and_clean_datasets(self, file_paths):\n",
    "        \"\"\"Carga y limpia todos los datasets\"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        logging.info(\"Cargando datasets principales...\")\n",
    "        \n",
    "        # Cargar datasets principales\n",
    "        # Actividades (5 archivos)\n",
    "        if 'activities' in file_paths:\n",
    "            activities_list = []\n",
    "            for file in file_paths['activities']:\n",
    "                if os.path.exists(file):\n",
    "                    df = pd.read_csv(file)\n",
    "                    activities_list.append(df)\n",
    "                    logging.info(f\"  Cargado: {file} ({len(df)} registros)\")\n",
    "            if activities_list:\n",
    "                datasets['activities'] = pd.concat(activities_list, ignore_index=True)\n",
    "                logging.info(f\"Total actividades: {len(datasets['activities'])}\")\n",
    "        \n",
    "        # Comentarios con análisis de sentimiento y clima\n",
    "        if 'reviews' in file_paths and os.path.exists(file_paths['reviews']):\n",
    "            datasets['reviews'] = pd.read_csv(file_paths['reviews'])\n",
    "            logging.info(f\"Reviews cargadas: {len(datasets['reviews'])}\")\n",
    "        \n",
    "        # Datos de turismo ONU (7 archivos) - opcional\n",
    "        if 'un_tourism' in file_paths:\n",
    "            un_data_list = []\n",
    "            for file in file_paths['un_tourism']:\n",
    "                if os.path.exists(file):\n",
    "                    df = pd.read_csv(file)\n",
    "                    un_data_list.append(df)\n",
    "            if un_data_list:\n",
    "                datasets['un_tourism'] = pd.concat(un_data_list, ignore_index=True)\n",
    "        \n",
    "        # Meta Data for Good - opcional\n",
    "        if 'commuting_zones' in file_paths and os.path.exists(file_paths['commuting_zones']):\n",
    "            datasets['commuting_zones'] = pd.read_csv(file_paths['commuting_zones'])\n",
    "        \n",
    "        if 'movement_data' in file_paths and os.path.exists(file_paths['movement_data']):\n",
    "            datasets['movement_data'] = pd.read_csv(file_paths['movement_data'])\n",
    "        \n",
    "        # Google Trends - opcional\n",
    "        if 'search_trends' in file_paths and os.path.exists(file_paths['search_trends']):\n",
    "            datasets['search_trends'] = pd.read_csv(file_paths['search_trends'])\n",
    "        \n",
    "        if 'monthly_interest' in file_paths and os.path.exists(file_paths['monthly_interest']):\n",
    "            datasets['monthly_interest'] = pd.read_csv(file_paths['monthly_interest'])\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def match_reviews_to_activities(self, activities_df, sentiment_df):\n",
    "        \"\"\"Empareja reviews con actividades basándose en títulos - versión completa\"\"\"\n",
    "        logging.info(\"Emparejando reviews con actividades...\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        required_activity_cols = ['titulo', 'ciudad', 'id', 'precio', 'rating']\n",
    "        required_sentiment_cols = ['texto', 'ciudad', 'sentimiento', 'confianza']\n",
    "        \n",
    "        missing_activity_cols = set(required_activity_cols) - set(activities_df.columns)\n",
    "        missing_sentiment_cols = set(required_sentiment_cols) - set(sentiment_df.columns)\n",
    "        \n",
    "        if missing_activity_cols:\n",
    "            logging.warning(f\"Columnas faltantes en activities: {missing_activity_cols}\")\n",
    "        if missing_sentiment_cols:\n",
    "            logging.warning(f\"Columnas faltantes en sentiment: {missing_sentiment_cols}\")\n",
    "        \n",
    "        # NORMALIZAR NOMBRES DE CIUDADES\n",
    "        logging.info(\"Normalizando nombres de ciudades...\")\n",
    "        activities_df = activities_df.copy()\n",
    "        sentiment_df = sentiment_df.copy()\n",
    "        \n",
    "        # Función para normalizar nombres de ciudades\n",
    "        def normalize_city_name(city_name):\n",
    "            if pd.isna(city_name):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            city_str = str(city_name).lower().strip()\n",
    "            \n",
    "            # Mapeo completo de normalizaciones\n",
    "            city_mappings = {\n",
    "                'barcelona': 'Barcelona',\n",
    "                'barcelona1': 'Barcelona',\n",
    "                'madrid': 'Madrid',\n",
    "                'malaga': 'Malaga',\n",
    "                'málaga': 'Malaga',\n",
    "                'sevilla': 'Sevilla',\n",
    "                'valencia': 'Valencia',\n",
    "                'tenerife': 'Tenerife',\n",
    "                'gran canaria': 'Gran Canaria',\n",
    "                'canaria': 'Gran Canaria',\n",
    "                'canarias': 'Gran Canaria',\n",
    "                'mallorca': 'Mallorca',\n",
    "                'palma de mallorca': 'Mallorca',\n",
    "                'palma': 'Mallorca'\n",
    "            }\n",
    "            \n",
    "            return city_mappings.get(city_str, city_name)\n",
    "        \n",
    "        # Aplicar normalización\n",
    "        activities_df['ciudad_original'] = activities_df['ciudad']\n",
    "        sentiment_df['ciudad_original'] = sentiment_df['ciudad']\n",
    "        \n",
    "        activities_df['ciudad'] = activities_df['ciudad'].apply(normalize_city_name)\n",
    "        sentiment_df['ciudad'] = sentiment_df['ciudad'].apply(normalize_city_name)\n",
    "        \n",
    "        logging.info(\"Ciudades después de normalización:\")\n",
    "        logging.info(f\"  Activities: {sorted(activities_df['ciudad'].unique())}\")\n",
    "        logging.info(f\"  Sentiment: {sorted(sentiment_df['ciudad'].unique())}\")\n",
    "        \n",
    "        # Crear dataset expandido donde cada review se asocia con su actividad\n",
    "        matched_reviews = []\n",
    "        total_matches = 0\n",
    "        successful_matches = 0\n",
    "        errors_count = 0\n",
    "        \n",
    "        logging.info(f\"Procesando {len(activities_df)} actividades...\")\n",
    "        \n",
    "        for idx, activity in activities_df.iterrows():\n",
    "            if idx % 1000 == 0:\n",
    "                logging.info(f\"Progreso: {idx}/{len(activities_df)} ({idx/len(activities_df)*100:.1f}%) - Matches: {total_matches}\")\n",
    "            \n",
    "            try:\n",
    "                activity_title = str(activity['titulo']).lower().strip()\n",
    "                activity_city = activity['ciudad']\n",
    "                item_id = activity.get('id', f'item_{idx}')\n",
    "                \n",
    "                # Saltear si el título está vacío o es muy corto\n",
    "                if len(activity_title) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # NORMALIZAR TÍTULO - Remover caracteres especiales problemáticos\n",
    "                import string\n",
    "                # Crear tabla de traducción para remover puntuación problemática\n",
    "                translator = str.maketrans('', '', '!¡¿?()[]{}*+^$|\\\\')\n",
    "                activity_title_clean = activity_title.translate(translator)\n",
    "                \n",
    "                # Método 1: Búsqueda exacta sin regex\n",
    "                try:\n",
    "                    # Buscar reviews que contengan el título (búsqueda simple)\n",
    "                    matching_reviews_mask = (\n",
    "                        sentiment_df['texto'].str.lower().str.contains(activity_title_clean, na=False, regex=False) &\n",
    "                        (sentiment_df['ciudad'] == activity_city)\n",
    "                    )\n",
    "                    \n",
    "                    matching_reviews_subset = sentiment_df[matching_reviews_mask].copy()\n",
    "                    \n",
    "                except Exception as simple_error:\n",
    "                    # Método 2: Búsqueda palabra por palabra\n",
    "                    try:\n",
    "                        words = activity_title_clean.split()\n",
    "                        if len(words) >= 2:  # Solo si tiene al menos 2 palabras\n",
    "                            # Buscar reviews que contengan al menos 70% de las palabras del título\n",
    "                            mask = sentiment_df['ciudad'] == activity_city\n",
    "                            word_matches = 0\n",
    "                            for word in words:\n",
    "                                if len(word) > 3:  # Solo palabras significativas\n",
    "                                    try:\n",
    "                                        word_mask = sentiment_df['texto'].str.lower().str.contains(word, na=False, regex=False)\n",
    "                                        if word_mask.any():\n",
    "                                            mask = mask & word_mask\n",
    "                                            word_matches += 1\n",
    "                                    except:\n",
    "                                        continue\n",
    "                            \n",
    "                            # Solo considerar match si encontramos suficientes palabras\n",
    "                            if word_matches >= max(1, len([w for w in words if len(w) > 3]) * 0.7):\n",
    "                                matching_reviews_subset = sentiment_df[mask].copy()\n",
    "                            else:\n",
    "                                matching_reviews_subset = pd.DataFrame()\n",
    "                        else:\n",
    "                            matching_reviews_subset = pd.DataFrame()\n",
    "                            \n",
    "                    except Exception as word_error:\n",
    "                        errors_count += 1\n",
    "                        if errors_count < 10:\n",
    "                            logging.warning(f\"Error procesando '{activity_title[:30]}...': {word_error}\")\n",
    "                        continue\n",
    "                \n",
    "                # Agregar información de la actividad a cada review matching\n",
    "                if not matching_reviews_subset.empty:\n",
    "                    matching_reviews_subset['item_id'] = item_id\n",
    "                    matching_reviews_subset['precio'] = activity.get('precio', 0)\n",
    "                    matching_reviews_subset['rating_actividad'] = activity.get('rating', 3.0)\n",
    "                    matching_reviews_subset['titulo_actividad'] = activity['titulo']\n",
    "                    \n",
    "                    matched_reviews.append(matching_reviews_subset)\n",
    "                    total_matches += len(matching_reviews_subset)\n",
    "                    successful_matches += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors_count += 1\n",
    "                if errors_count < 10:\n",
    "                    logging.error(f\"Error general procesando actividad {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Consolidar resultados\n",
    "        if matched_reviews:\n",
    "            all_matched_reviews = pd.concat(matched_reviews, ignore_index=True)\n",
    "            \n",
    "            logging.info(f\"\\nRESULTADOS DEL EMPAREJAMIENTO:\")\n",
    "            logging.info(f\"   Reviews emparejadas: {len(all_matched_reviews):,}\")\n",
    "            logging.info(f\"   Actividades con reviews: {all_matched_reviews['item_id'].nunique()}\")\n",
    "            logging.info(f\"   Tasa de cobertura: {successful_matches/len(activities_df)*100:.1f}%\")\n",
    "            logging.info(f\"   Actividades procesadas exitosamente: {successful_matches}\")\n",
    "            logging.info(f\"   Errores encontrados: {errors_count}\")\n",
    "            \n",
    "            # Estadísticas adicionales\n",
    "            reviews_per_activity = all_matched_reviews.groupby('item_id').size()\n",
    "            logging.info(f\"   Reviews promedio por actividad: {reviews_per_activity.mean():.1f}\")\n",
    "            logging.info(f\"   Actividad con más reviews: {reviews_per_activity.max()}\")\n",
    "            \n",
    "            # Estadísticas por ciudad\n",
    "            city_stats = all_matched_reviews.groupby('ciudad').agg({\n",
    "                'item_id': 'nunique',\n",
    "                'texto': 'count'\n",
    "            }).rename(columns={'item_id': 'actividades', 'texto': 'reviews'})\n",
    "            \n",
    "            logging.info(f\"\\n   Distribución por ciudad:\")\n",
    "            for city, stats in city_stats.iterrows():\n",
    "                logging.info(f\"     {city}: {stats['actividades']} actividades, {stats['reviews']} reviews\")\n",
    "            \n",
    "        else:\n",
    "            logging.warning(\"NO SE ENCONTRARON EMPAREJAMIENTOS\")\n",
    "            # Crear DataFrame vacío con columnas esperadas\n",
    "            all_matched_reviews = pd.DataFrame(columns=[\n",
    "                'texto', 'ciudad', 'categoria', 'fecha', 'fuente', \n",
    "                'sentimiento', 'confianza', 'descripcion_sencilla',\n",
    "                'item_id', 'precio', 'rating_actividad', 'titulo_actividad'\n",
    "            ])\n",
    "        \n",
    "        return all_matched_reviews\n",
    "    \n",
    "    def create_synthetic_user_ids(self, reviews_df):\n",
    "        \"\"\"Crea user_ids sintéticos basándose en patrones\"\"\"\n",
    "        logging.info(\"Creando user_ids sintéticos...\")\n",
    "        \n",
    "        if reviews_df.empty:\n",
    "            return reviews_df\n",
    "            \n",
    "        reviews_df = reviews_df.copy()\n",
    "        reviews_df['fecha_comentario'] = pd.to_datetime(reviews_df.get('fecha', datetime.now()), errors='coerce')\n",
    "        \n",
    "        # Crear grupos base\n",
    "        reviews_df['grupo_base'] = (\n",
    "            reviews_df['ciudad'].astype(str) + '_' + \n",
    "            reviews_df.get('fuente', 'unknown').astype(str)\n",
    "        )\n",
    "        \n",
    "        # Asignar user_ids basados en patrones\n",
    "        user_id_counter = 0\n",
    "        reviews_df['user_id'] = None\n",
    "        \n",
    "        for grupo in reviews_df['grupo_base'].unique():\n",
    "            grupo_reviews = reviews_df[reviews_df['grupo_base'] == grupo]\n",
    "            \n",
    "            # Asignar IDs secuenciales por grupo\n",
    "            for i in range(len(grupo_reviews)):\n",
    "                if i % 5 == 0:  # Nuevo usuario cada 5 reviews\n",
    "                    user_id_counter += 1\n",
    "                reviews_df.loc[grupo_reviews.index[i], 'user_id'] = f'user_{user_id_counter}'\n",
    "        \n",
    "        logging.info(f\"Creados {user_id_counter} user_ids sintéticos\")\n",
    "        return reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d707f2df-ff81-44a2-87f9-0a201c84ce00",
   "metadata": {},
   "source": [
    "### Modelo DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "185f848b-cd0d-42b2-a3d7-043670c6ab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourismRecommenderModel:\n",
    "    \"\"\"Modelo de recomendación con Deep Learning optimizado para RTX 3050 Ti\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim=32, dense_units=64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dense_units = dense_units\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        self.user_to_idx = {}\n",
    "        self.item_to_idx = {}\n",
    "        self.city_to_idx = {}\n",
    "        \n",
    "    def build_hybrid_model(self, num_users, num_items, num_cities, \n",
    "                          contextual_features_dim=10, weather_features_dim=15):\n",
    "        \"\"\"Construye modelo híbrido compacto optimizado para 4GB VRAM\"\"\"\n",
    "        logging.info(\"Construyendo modelo híbrido compacto para RTX 3050 Ti...\")\n",
    "        \n",
    "        # Entradas\n",
    "        user_input = layers.Input(shape=(), name='user_id')\n",
    "        item_input = layers.Input(shape=(), name='item_id')\n",
    "        city_input = layers.Input(shape=(), name='city_id')\n",
    "        contextual_input = layers.Input(shape=(contextual_features_dim,), name='contextual_features')\n",
    "        temporal_input = layers.Input(shape=(4,), name='temporal_features')\n",
    "        weather_sentiment_input = layers.Input(shape=(weather_features_dim,), name='weather_sentiment')\n",
    "        \n",
    "        # Embeddings compactos\n",
    "        user_embedding = layers.Embedding(\n",
    "            num_users + 1, self.embedding_dim, \n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)\n",
    "        )(user_input)\n",
    "        user_vec = layers.Flatten()(user_embedding)\n",
    "        \n",
    "        item_embedding = layers.Embedding(\n",
    "            num_items + 1, self.embedding_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)\n",
    "        )(item_input)\n",
    "        item_vec = layers.Flatten()(item_embedding)\n",
    "        \n",
    "        city_embedding = layers.Embedding(\n",
    "            num_cities + 1, self.embedding_dim // 2,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)\n",
    "        )(city_input)\n",
    "        city_vec = layers.Flatten()(city_embedding)\n",
    "        \n",
    "        # Procesamiento de features\n",
    "        contextual_dense = layers.Dense(self.dense_units//2, activation='relu')(contextual_input)\n",
    "        contextual_dense = layers.Dropout(0.2)(contextual_dense)\n",
    "        \n",
    "        temporal_dense = layers.Dense(16, activation='relu')(temporal_input)\n",
    "        \n",
    "        weather_dense = layers.Dense(32, activation='relu', name='climate_layer')(weather_sentiment_input)\n",
    "        weather_dense = layers.Dropout(0.2)(weather_dense)\n",
    "        \n",
    "        # Combinación\n",
    "        combined = layers.Concatenate(name='feature_fusion')([\n",
    "            user_vec, item_vec, city_vec,\n",
    "            contextual_dense, temporal_dense, weather_dense\n",
    "        ])\n",
    "        \n",
    "        # Red principal\n",
    "        x = layers.Dense(self.dense_units, activation='relu', name='main_layer')(combined)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        x = layers.Dense(self.dense_units//2, activation='relu', name='output_layer')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # Salidas\n",
    "        rating_output = layers.Dense(1, activation='sigmoid', name='rating')(x)\n",
    "        sentiment_output = layers.Dense(1, activation='sigmoid', name='sentiment')(x)\n",
    "        interaction_output = layers.Dense(1, activation='sigmoid', name='interaction')(x)\n",
    "        \n",
    "        # Modelo final\n",
    "        self.model = Model(\n",
    "            inputs=[user_input, item_input, city_input, \n",
    "                   contextual_input, temporal_input, weather_sentiment_input],\n",
    "            outputs=[rating_output, sentiment_output, interaction_output],\n",
    "            name='TourismRecommenderCompact'\n",
    "        )\n",
    "        \n",
    "        # Compilar\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.002, epsilon=1e-7)\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={\n",
    "                'rating': 'mse',\n",
    "                'sentiment': 'mse', \n",
    "                'interaction': 'binary_crossentropy'\n",
    "            },\n",
    "            loss_weights={\n",
    "                'rating': 1.0,\n",
    "                'sentiment': 0.7,\n",
    "                'interaction': 0.3\n",
    "            },\n",
    "            metrics={\n",
    "                'rating': ['mae'],\n",
    "                'sentiment': ['mae'],\n",
    "                'interaction': ['accuracy']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        total_params = self.model.count_params()\n",
    "        logging.info(f\"Modelo compacto construido: {total_params:,} parámetros\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def prepare_training_data(self, matched_reviews_df):\n",
    "        \"\"\"Prepara datos para entrenamiento\"\"\"\n",
    "        if matched_reviews_df.empty:\n",
    "            raise ValueError(\"No hay datos de entrenamiento\")\n",
    "        \n",
    "        # Crear mapeos de índices\n",
    "        unique_users = matched_reviews_df['user_id'].unique()\n",
    "        unique_items = matched_reviews_df['item_id'].unique()\n",
    "        unique_cities = matched_reviews_df['ciudad'].unique()\n",
    "        \n",
    "        self.user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        self.item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        self.city_to_idx = {city: idx for idx, city in enumerate(unique_cities)}\n",
    "        \n",
    "        training_data = []\n",
    "        \n",
    "        for idx, row in matched_reviews_df.iterrows():\n",
    "            user_idx = self.user_to_idx.get(row['user_id'], 0)\n",
    "            item_idx = self.item_to_idx.get(row['item_id'], 0)\n",
    "            city_idx = self.city_to_idx.get(row['ciudad'], 0)\n",
    "            \n",
    "            # Features simplificadas\n",
    "            contextual_features = [0.5] * 10  # Placeholder\n",
    "            temporal_features = self._extract_temporal_features(row.get('fecha'))\n",
    "            weather_features = [0.5] * 15  # Placeholder\n",
    "            \n",
    "            # Targets basados en sentimiento\n",
    "            sentiment = row.get('sentimiento', 'neutro')\n",
    "            if sentiment == 'negativo':\n",
    "                rating_target = 0.25\n",
    "                sentiment_target = 0.0\n",
    "            elif sentiment == 'positivo':\n",
    "                rating_target = 0.875\n",
    "                sentiment_target = 1.0\n",
    "            else:\n",
    "                rating_target = 0.6\n",
    "                sentiment_target = 0.5\n",
    "            \n",
    "            training_data.append({\n",
    "                'user_id': user_idx,\n",
    "                'item_id': item_idx,\n",
    "                'city_id': city_idx,\n",
    "                'contextual_features': contextual_features,\n",
    "                'temporal_features': temporal_features,\n",
    "                'weather_sentiment': weather_features,\n",
    "                'rating': rating_target,\n",
    "                'sentiment': sentiment_target,\n",
    "                'interaction': 1.0,\n",
    "                'sample_weight': row.get('confianza', 0.8)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(training_data)\n",
    "    \n",
    "    def _extract_temporal_features(self, date):\n",
    "        \"\"\"Extrae features temporales (estación)\"\"\"\n",
    "        try:\n",
    "            date = pd.to_datetime(date, errors='coerce')\n",
    "            if pd.isna(date):\n",
    "                return [0.25, 0.25, 0.25, 0.25]\n",
    "            \n",
    "            month = date.month\n",
    "            if month in [12, 1, 2]:\n",
    "                return [1, 0, 0, 0]  # Invierno\n",
    "            elif month in [3, 4, 5]:\n",
    "                return [0, 1, 0, 0]  # Primavera\n",
    "            elif month in [6, 7, 8]:\n",
    "                return [0, 0, 1, 0]  # Verano\n",
    "            else:\n",
    "                return [0, 0, 0, 1]  # Otoño\n",
    "        except:\n",
    "            return [0.25, 0.25, 0.25, 0.25]\n",
    "    \n",
    "    def train_model(self, training_data, validation_split=0.2, epochs=30, batch_size=128):\n",
    "        \"\"\"Entrena el modelo optimizado para RTX 3050 Ti\"\"\"\n",
    "        logging.info(\"Iniciando entrenamiento optimizado...\")\n",
    "        \n",
    "        # Preparar datos\n",
    "        X = {\n",
    "            'user_id': training_data['user_id'].values.astype('int32'),\n",
    "            'item_id': training_data['item_id'].values.astype('int32'),\n",
    "            'city_id': training_data['city_id'].values.astype('int32'),\n",
    "            'contextual_features': np.array(training_data['contextual_features'].tolist(), dtype='float32'),\n",
    "            'temporal_features': np.array(training_data['temporal_features'].tolist(), dtype='float32'),\n",
    "            'weather_sentiment': np.array(training_data['weather_sentiment'].tolist(), dtype='float32')\n",
    "        }\n",
    "        \n",
    "        y = {\n",
    "            'rating': training_data['rating'].values.astype('float32'),\n",
    "            'sentiment': training_data['sentiment'].values.astype('float32'),\n",
    "            'interaction': training_data['interaction'].values.astype('float32')\n",
    "        }\n",
    "        \n",
    "        sample_weights = training_data['sample_weight'].values.astype('float32')\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.7, patience=3, min_lr=1e-6, verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entrenar\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        logging.info(\"Entrenamiento completado!\")\n",
    "        return self.history\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"Guarda el modelo y mapeos\"\"\"\n",
    "        self.model.save(model_path)\n",
    "        \n",
    "        mappings = {\n",
    "            'user_to_idx': self.user_to_idx,\n",
    "            'item_to_idx': self.item_to_idx,\n",
    "            'city_to_idx': self.city_to_idx\n",
    "        }\n",
    "        \n",
    "        with open(model_path.replace('.h5', '_mappings.pkl'), 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "        \n",
    "        logging.info(f\"Modelo guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c60ba7f-db54-4821-9a66-8b31a67c4a89",
   "metadata": {},
   "source": [
    "### Sistema Re-Ranking Diario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6287698-33bf-437b-9274-60304fc562dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedScraper:\n",
    "    \"\"\"Scraper optimizado y simplificado para reviews de turismo\"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True):\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        \n",
    "    def _setup_driver(self):\n",
    "        \"\"\"Configura driver de Selenium optimizado\"\"\"\n",
    "        if not SELENIUM_AVAILABLE:\n",
    "            logging.warning(\"Selenium no disponible. Scraping deshabilitado.\")\n",
    "            return None\n",
    "            \n",
    "        if self.driver:\n",
    "            return self.driver\n",
    "            \n",
    "        options = webdriver.ChromeOptions()\n",
    "        if self.headless:\n",
    "            options.add_argument('--headless=new')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.page_load_strategy = 'eager'\n",
    "        \n",
    "        try:\n",
    "            self.driver = webdriver.Chrome(options=options)\n",
    "            self.driver.set_page_load_timeout(15)\n",
    "            return self.driver\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error configurando driver: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _quit_driver(self):\n",
    "        \"\"\"Cierra el driver\"\"\"\n",
    "        if self.driver:\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except:\n",
    "                pass\n",
    "            self.driver = None\n",
    "    \n",
    "    def scrape_quick_reviews(self, city, date=None, max_items=10):\n",
    "        \"\"\"Scraping rápido y ligero de reviews recientes\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now().strftime('%Y-%m-%d')\n",
    "            \n",
    "        reviews = []\n",
    "        \n",
    "        try:\n",
    "            driver = self._setup_driver()\n",
    "            if not driver:\n",
    "                return reviews\n",
    "            \n",
    "            # URL simplificada de búsqueda\n",
    "            search_url = f\"https://www.google.com/search?q={city}+turismo+opiniones+{date}\"\n",
    "            driver.get(search_url)\n",
    "            \n",
    "            # Espera mínima\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Extracción básica de snippets de Google\n",
    "            snippets = driver.find_elements(By.CSS_SELECTOR, '.VwiC3b, .s3v9rd')\n",
    "            \n",
    "            for snippet in snippets[:max_items]:\n",
    "                try:\n",
    "                    text = snippet.text.strip()\n",
    "                    if text and len(text) > 20:\n",
    "                        reviews.append({\n",
    "                            'texto': text,\n",
    "                            'fecha': date,\n",
    "                            'fuente': 'web_scraping',\n",
    "                            'ciudad': city\n",
    "                        })\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error en scraping rápido: {e}\")\n",
    "        finally:\n",
    "            self._quit_driver()\n",
    "            \n",
    "        return reviews\n",
    "    \n",
    "    def load_or_scrape(self, city, csv_path=None):\n",
    "        \"\"\"Intenta cargar datos de CSV, si no existe hace scraping ligero\"\"\"\n",
    "        # Primero intentar cargar CSV si existe\n",
    "        if csv_path and os.path.exists(csv_path):\n",
    "            try:\n",
    "                df = pd.read_csv(csv_path)\n",
    "                # Filtrar por ciudad si está en el CSV\n",
    "                if 'ciudad' in df.columns:\n",
    "                    df = df[df['ciudad'].str.contains(city, case=False, na=False)]\n",
    "                elif 'titulo' in df.columns:\n",
    "                    # Buscar ciudad en título si no hay columna ciudad\n",
    "                    df = df[df['titulo'].str.contains(city, case=False, na=False)]\n",
    "                \n",
    "                logging.info(f\"Cargados {len(df)} registros desde CSV para {city}\")\n",
    "                return df\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Error cargando CSV: {e}\")\n",
    "        \n",
    "        # Si no hay CSV y Selenium está disponible, hacer scraping mínimo\n",
    "        if SELENIUM_AVAILABLE:\n",
    "            logging.info(f\"Realizando scraping rápido para {city}\")\n",
    "            reviews = self.scrape_quick_reviews(city, max_items=5)\n",
    "            \n",
    "            if reviews:\n",
    "                return pd.DataFrame(reviews)\n",
    "        \n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47a6bf5-ac7d-45a4-8b8a-a152ae347dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DailyRerankingSystem:\n",
    "    \"\"\"Sistema de re-ranking con factores dinámicos, clima real y datos de scraping opcional\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, enable_scraping=False):\n",
    "        self.weather_cache = {}\n",
    "        self.trend_cache = {}\n",
    "        self.scraping_cache = {}\n",
    "        self.api_key = WEATHER_API_KEY\n",
    "        self.enable_scraping = enable_scraping\n",
    "        \n",
    "        if self.enable_scraping:\n",
    "            self.scraper = OptimizedScraper(headless=True)\n",
    "        else:\n",
    "            self.scraper = None\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            mappings_path = model_path.replace('.h5', '_mappings.pkl')\n",
    "            if os.path.exists(mappings_path):\n",
    "                with open(mappings_path, 'rb') as f:\n",
    "                    mappings = pickle.load(f)\n",
    "                    self.user_to_idx = mappings.get('user_to_idx', {})\n",
    "                    self.item_to_idx = mappings.get('item_to_idx', {})\n",
    "                    self.city_to_idx = mappings.get('city_to_idx', {})\n",
    "    \n",
    "    def get_scraping_insights(self, city, csv_path=None):\n",
    "        \"\"\"Obtiene insights de scraping si está disponible\"\"\"\n",
    "        if not self.enable_scraping or not self.scraper:\n",
    "            return None\n",
    "            \n",
    "        # Verificar caché\n",
    "        cache_key = f\"{city}_{datetime.now().strftime('%Y-%m-%d')}\"\n",
    "        if cache_key in self.scraping_cache:\n",
    "            return self.scraping_cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Intentar cargar CSV o hacer scraping ligero\n",
    "            df = self.scraper.load_or_scrape(city, csv_path)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Análisis básico de sentimientos si existe la columna\n",
    "                insights = {\n",
    "                    'total_reviews': len(df),\n",
    "                    'recent_topics': [],\n",
    "                    'sentiment_trend': 'neutral'\n",
    "                }\n",
    "                \n",
    "                if 'sentimiento' in df.columns:\n",
    "                    # Calcular tendencia de sentimiento\n",
    "                    sentiment_counts = df['sentimiento'].value_counts()\n",
    "                    if 'positivo' in sentiment_counts and sentiment_counts['positivo'] > len(df) * 0.6:\n",
    "                        insights['sentiment_trend'] = 'positive'\n",
    "                    elif 'negativo' in sentiment_counts and sentiment_counts['negativo'] > len(df) * 0.4:\n",
    "                        insights['sentiment_trend'] = 'negative'\n",
    "                \n",
    "                if 'texto' in df.columns:\n",
    "                    # Extraer temas frecuentes (muy simplificado)\n",
    "                    all_text = ' '.join(df['texto'].dropna().astype(str))\n",
    "                    keywords = ['playa', 'museo', 'restaurante', 'hotel', 'tour', 'transporte']\n",
    "                    for keyword in keywords:\n",
    "                        if keyword in all_text.lower():\n",
    "                            insights['recent_topics'].append(keyword)\n",
    "                \n",
    "                self.scraping_cache[cache_key] = insights\n",
    "                return insights\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error obteniendo insights de scraping: {e}\")\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    def get_real_time_weather(self, city):\n",
    "        \"\"\"Obtiene clima real usando OpenWeatherMap API\"\"\"\n",
    "        # Verificar caché\n",
    "        if city in self.weather_cache:\n",
    "            cache_time = self.weather_cache[city]['timestamp']\n",
    "            if (datetime.now() - cache_time).seconds < 3600:  # Cache por 1 hora\n",
    "                return self.weather_cache[city]['data']\n",
    "        \n",
    "        try:\n",
    "            # Llamada a la API\n",
    "            url = f\"https://api.openweathermap.org/data/2.5/weather?q={city}&appid={self.api_key}&units=metric&lang=es\"\n",
    "            response = requests.get(url, timeout=5)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                datos = response.json()\n",
    "                \n",
    "                weather_data = {\n",
    "                    'description': datos[\"weather\"][0][\"description\"],\n",
    "                    'temperature': datos[\"main\"][\"temp\"],\n",
    "                    'temp_min': datos[\"main\"][\"temp_min\"],\n",
    "                    'temp_max': datos[\"main\"][\"temp_max\"],\n",
    "                    'humidity': datos[\"main\"][\"humidity\"],\n",
    "                    'feels_like': datos[\"main\"][\"feels_like\"],\n",
    "                    'pressure': datos[\"main\"][\"pressure\"],\n",
    "                    'wind_speed': datos[\"wind\"][\"speed\"] if \"wind\" in datos else 0,\n",
    "                    'clouds': datos[\"clouds\"][\"all\"] if \"clouds\" in datos else 0,\n",
    "                    'rain': datos.get(\"rain\", {}).get(\"1h\", 0),\n",
    "                    'condition': datos[\"weather\"][0][\"main\"]\n",
    "                }\n",
    "                \n",
    "                # Guardar en caché\n",
    "                self.weather_cache[city] = {\n",
    "                    'data': weather_data,\n",
    "                    'timestamp': datetime.now()\n",
    "                }\n",
    "                \n",
    "                return weather_data\n",
    "                \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error obteniendo clima para {city}: {e}\")\n",
    "        \n",
    "        # Datos por defecto si falla la API\n",
    "        return {\n",
    "            'description': 'parcialmente nublado',\n",
    "            'temperature': 20,\n",
    "            'humidity': 60,\n",
    "            'wind_speed': 5,\n",
    "            'rain': 0,\n",
    "            'condition': 'Clouds'\n",
    "        }\n",
    "    \n",
    "    def calculate_weather_factor(self, weather_data):\n",
    "        \"\"\"Calcula factor de ajuste basado en clima real\"\"\"\n",
    "        temp = weather_data.get('temperature', 20)\n",
    "        rain = weather_data.get('rain', 0)\n",
    "        condition = weather_data.get('condition', 'Clear')\n",
    "        humidity = weather_data.get('humidity', 60)\n",
    "        wind = weather_data.get('wind_speed', 5)\n",
    "        \n",
    "        # Factor de temperatura (óptima entre 18-25°C)\n",
    "        if 18 <= temp <= 25:\n",
    "            temp_factor = 1.0\n",
    "        elif temp < 10 or temp > 35:\n",
    "            temp_factor = 0.5\n",
    "        else:\n",
    "            temp_factor = 0.7 + (0.3 * (1 - abs(temp - 21.5) / 15))\n",
    "        \n",
    "        # Factor de lluvia\n",
    "        if rain == 0:\n",
    "            rain_factor = 1.0\n",
    "        elif rain < 2:\n",
    "            rain_factor = 0.8\n",
    "        elif rain < 5:\n",
    "            rain_factor = 0.6\n",
    "        else:\n",
    "            rain_factor = 0.3\n",
    "        \n",
    "        # Factor de condición general\n",
    "        condition_factors = {\n",
    "            'Clear': 1.0,\n",
    "            'Clouds': 0.85,\n",
    "            'Rain': 0.5,\n",
    "            'Drizzle': 0.7,\n",
    "            'Thunderstorm': 0.3,\n",
    "            'Snow': 0.4,\n",
    "            'Mist': 0.7,\n",
    "            'Fog': 0.6\n",
    "        }\n",
    "        condition_factor = condition_factors.get(condition, 0.7)\n",
    "        \n",
    "        # Factor de humedad (óptima 40-60%)\n",
    "        if 40 <= humidity <= 60:\n",
    "            humidity_factor = 1.0\n",
    "        else:\n",
    "            humidity_factor = 0.9 - abs(humidity - 50) * 0.002\n",
    "        \n",
    "        # Factor de viento (óptimo < 10 km/h)\n",
    "        if wind < 10:\n",
    "            wind_factor = 1.0\n",
    "        elif wind < 20:\n",
    "            wind_factor = 0.8\n",
    "        else:\n",
    "            wind_factor = 0.6\n",
    "        \n",
    "        # Cálculo del factor general\n",
    "        overall_factor = (\n",
    "            temp_factor * 0.3 +\n",
    "            rain_factor * 0.25 +\n",
    "            condition_factor * 0.25 +\n",
    "            humidity_factor * 0.1 +\n",
    "            wind_factor * 0.1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'overall': overall_factor,\n",
    "            'temperature': temp_factor,\n",
    "            'rain': rain_factor,\n",
    "            'condition': condition_factor,\n",
    "            'humidity': humidity_factor,\n",
    "            'wind': wind_factor,\n",
    "            'indoor_boost': 1.5 if rain > 2 or temp < 10 or temp > 30 else 1.0,\n",
    "            'outdoor_penalty': 0.5 if rain > 5 or wind > 20 else 1.0\n",
    "        }\n",
    "    \n",
    "    def calculate_temporal_factor(self, current_time=None):\n",
    "        \"\"\"Calcula factor temporal\"\"\"\n",
    "        if current_time is None:\n",
    "            current_time = datetime.now()\n",
    "        \n",
    "        hour = current_time.hour\n",
    "        weekday = current_time.weekday()\n",
    "        \n",
    "        # Factor por hora del día\n",
    "        if 10 <= hour <= 20:\n",
    "            hour_factor = 1.0\n",
    "        elif 8 <= hour < 10 or 20 < hour <= 22:\n",
    "            hour_factor = 0.8\n",
    "        else:\n",
    "            hour_factor = 0.4\n",
    "        \n",
    "        # Factor por día de la semana\n",
    "        if weekday >= 5:  # Fin de semana\n",
    "            weekday_factor = 1.2\n",
    "        elif weekday == 4:  # Viernes\n",
    "            weekday_factor = 1.1\n",
    "        else:\n",
    "            weekday_factor = 0.9\n",
    "        \n",
    "        return {\n",
    "            'hour_factor': hour_factor,\n",
    "            'weekday_factor': weekday_factor,\n",
    "            'overall': hour_factor * weekday_factor,\n",
    "            'peak_time': 10 <= hour <= 20,\n",
    "            'weekend': weekday >= 5\n",
    "        }\n",
    "    \n",
    "    def rerank_recommendations(self, base_recommendations, city, user_preferences=None, csv_path=None):\n",
    "        \"\"\"Re-rankea recomendaciones con factores dinámicos, clima real y datos de scraping opcional\"\"\"\n",
    "        logging.info(f\"Re-rankeando recomendaciones para {city}...\")\n",
    "        \n",
    "        # Obtener clima real\n",
    "        weather_data = self.get_real_time_weather(city)\n",
    "        weather_factors = self.calculate_weather_factor(weather_data)\n",
    "        \n",
    "        # Obtener factores temporales\n",
    "        temporal_factors = self.calculate_temporal_factor()\n",
    "        \n",
    "        # Obtener insights de scraping si está habilitado\n",
    "        scraping_insights = None\n",
    "        if self.enable_scraping:\n",
    "            scraping_insights = self.get_scraping_insights(city, csv_path)\n",
    "        \n",
    "        # Re-rankear cada recomendación\n",
    "        reranked = []\n",
    "        for rec in base_recommendations:\n",
    "            base_score = rec.get('combined_score', 0.5)\n",
    "            \n",
    "            # Aplicar factores dinámicos\n",
    "            weather_adjustment = weather_factors['overall']\n",
    "            temporal_adjustment = temporal_factors['overall']\n",
    "            \n",
    "            # Factor adicional basado en scraping insights\n",
    "            scraping_boost = 1.0\n",
    "            if scraping_insights:\n",
    "                # Boost si hay sentimiento positivo general\n",
    "                if scraping_insights['sentiment_trend'] == 'positive':\n",
    "                    scraping_boost *= 1.1\n",
    "                elif scraping_insights['sentiment_trend'] == 'negative':\n",
    "                    scraping_boost *= 0.9\n",
    "                \n",
    "                # Boost si el item está relacionado con temas trending\n",
    "                if 'titulo' in rec:\n",
    "                    item_title = rec['titulo'].lower()\n",
    "                    for topic in scraping_insights.get('recent_topics', []):\n",
    "                        if topic in item_title:\n",
    "                            scraping_boost *= 1.15\n",
    "                            break\n",
    "            \n",
    "            # Ajuste específico por tipo de actividad\n",
    "            if 'tipo' in rec:\n",
    "                if rec['tipo'] == 'outdoor' and weather_factors['rain'] < 0.7:\n",
    "                    weather_adjustment *= weather_factors['outdoor_penalty']\n",
    "                elif rec['tipo'] == 'indoor' and weather_factors['rain'] < 0.7:\n",
    "                    weather_adjustment *= weather_factors['indoor_boost']\n",
    "            \n",
    "            # Score final con todos los factores\n",
    "            final_score = base_score * (\n",
    "                0.4 + \n",
    "                0.25 * weather_adjustment + \n",
    "                0.25 * temporal_adjustment + \n",
    "                0.1 * scraping_boost\n",
    "            )\n",
    "            \n",
    "            reranked_rec = rec.copy()\n",
    "            reranked_rec.update({\n",
    "                'original_score': base_score,\n",
    "                'final_score': final_score,\n",
    "                'weather_factor': weather_adjustment,\n",
    "                'temporal_factor': temporal_adjustment,\n",
    "                'scraping_boost': scraping_boost,\n",
    "                'weather_data': weather_data,\n",
    "                'scraping_insights': scraping_insights,\n",
    "                'adjustments_applied': True\n",
    "            })\n",
    "            \n",
    "            reranked.append(reranked_rec)\n",
    "        \n",
    "        # Ordenar por score final\n",
    "        reranked.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        \n",
    "        return reranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf99c2-2787-4a70-afaa-7927995af713",
   "metadata": {},
   "source": [
    "### Pipeline Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d99f45-072e-47da-b9e4-4f921dfe1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourismRecommenderPipeline:\n",
    "    \"\"\"Pipeline completo automatizado con soporte para scraping opcional\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or self._get_default_config()\n",
    "        self.preprocessor = TourismDataPreprocessor()\n",
    "        self.model = TourismRecommenderModel()\n",
    "        \n",
    "        # Inicializar reranking con o sin scraping\n",
    "        enable_scraping = self.config.get('enable_scraping', False)\n",
    "        self.reranking_system = DailyRerankingSystem(enable_scraping=enable_scraping)\n",
    "        \n",
    "        self.db_path = self.config.get('db_path', 'tourism_recommender.db')\n",
    "        self._setup_database()\n",
    "        \n",
    "    def _get_default_config(self):\n",
    "        \"\"\"Configuración por defecto\"\"\"\n",
    "        return {\n",
    "            'model_path': 'tourism_model.h5',\n",
    "            'db_path': 'tourism_recommender.db',\n",
    "            'batch_size': 128,\n",
    "            'epochs': 30,\n",
    "            'embedding_dim': 32,\n",
    "            'dense_units': 64,\n",
    "            'max_users': 10000,\n",
    "            'max_items': 5000,\n",
    "            'enable_scraping': False,  # Deshabilitado por defecto\n",
    "            'scraping_csv_path': None  # Path al CSV de scraping si existe\n",
    "        }\n",
    "    \n",
    "    def _setup_database(self):\n",
    "        \"\"\"Configura base de datos SQLite\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Tabla de recomendaciones\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS recommendations (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                user_id TEXT,\n",
    "                city TEXT,\n",
    "                item_id TEXT,\n",
    "                score REAL,\n",
    "                weather_factor REAL,\n",
    "                temporal_factor REAL,\n",
    "                timestamp DATETIME,\n",
    "                metadata TEXT\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Tabla de métricas del modelo\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS model_metrics (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                timestamp DATETIME,\n",
    "                val_loss REAL,\n",
    "                val_accuracy REAL,\n",
    "                training_time REAL,\n",
    "                num_users INTEGER,\n",
    "                num_items INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def run_complete_pipeline(self, file_paths, retrain=False):\n",
    "        \"\"\"Ejecuta el pipeline completo\"\"\"\n",
    "        logging.info(\"=== INICIANDO PIPELINE COMPLETO ===\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. Cargar y limpiar datos\n",
    "            logging.info(\"Paso 1: Cargando datos...\")\n",
    "            datasets = self.preprocessor.load_and_clean_datasets(file_paths)\n",
    "            \n",
    "            if not datasets.get('activities') or datasets['activities'].empty:\n",
    "                raise ValueError(\"No se encontraron datos de actividades\")\n",
    "            \n",
    "            if not datasets.get('reviews') or datasets['reviews'].empty:\n",
    "                raise ValueError(\"No se encontraron datos de reviews\")\n",
    "            \n",
    "            # 2. Emparejar reviews con actividades\n",
    "            logging.info(\"Paso 2: Emparejando reviews con actividades...\")\n",
    "            matched_reviews = self.preprocessor.match_reviews_to_activities(\n",
    "                datasets['activities'], \n",
    "                datasets['reviews']\n",
    "            )\n",
    "            \n",
    "            if matched_reviews.empty:\n",
    "                raise ValueError(\"No se pudieron emparejar reviews con actividades\")\n",
    "            \n",
    "            # 3. Crear user_ids sintéticos\n",
    "            logging.info(\"Paso 3: Creando user_ids sintéticos...\")\n",
    "            matched_reviews = self.preprocessor.create_synthetic_user_ids(matched_reviews)\n",
    "            \n",
    "            # 4. Entrenar o cargar modelo\n",
    "            model_path = self.config['model_path']\n",
    "            \n",
    "            if retrain or not os.path.exists(model_path):\n",
    "                logging.info(\"Paso 4: Entrenando modelo...\")\n",
    "                \n",
    "                # Preparar datos de entrenamiento\n",
    "                training_data = self.model.prepare_training_data(matched_reviews)\n",
    "                \n",
    "                # Limitar tamaño para GPU\n",
    "                if len(training_data) > self.config['max_users']:\n",
    "                    training_data = training_data.sample(self.config['max_users'])\n",
    "                \n",
    "                # Construir modelo\n",
    "                num_users = len(self.model.user_to_idx)\n",
    "                num_items = len(self.model.item_to_idx) \n",
    "                num_cities = len(self.model.city_to_idx)\n",
    "                \n",
    "                self.model.build_hybrid_model(\n",
    "                    num_users=num_users,\n",
    "                    num_items=num_items,\n",
    "                    num_cities=num_cities,\n",
    "                    contextual_features_dim=10,\n",
    "                    weather_features_dim=15\n",
    "                )\n",
    "                \n",
    "                # Entrenar\n",
    "                history = self.model.train_model(\n",
    "                    training_data,\n",
    "                    epochs=self.config['epochs'],\n",
    "                    batch_size=self.config['batch_size']\n",
    "                )\n",
    "                \n",
    "                # Guardar modelo\n",
    "                self.model.save_model(model_path)\n",
    "                \n",
    "                # Guardar métricas\n",
    "                self._save_metrics(history, num_users, num_items)\n",
    "                \n",
    "            else:\n",
    "                logging.info(\"Paso 4: Cargando modelo existente...\")\n",
    "                self.model.model = tf.keras.models.load_model(model_path)\n",
    "                \n",
    "                mappings_path = model_path.replace('.h5', '_mappings.pkl')\n",
    "                with open(mappings_path, 'rb') as f:\n",
    "                    mappings = pickle.load(f)\n",
    "                    self.model.user_to_idx = mappings['user_to_idx']\n",
    "                    self.model.item_to_idx = mappings['item_to_idx']\n",
    "                    self.model.city_to_idx = mappings['city_to_idx']\n",
    "            \n",
    "            # 5. Actualizar sistema de re-ranking\n",
    "            logging.info(\"Paso 5: Actualizando sistema de re-ranking...\")\n",
    "            self.reranking_system = DailyRerankingSystem(\n",
    "                model_path=model_path,\n",
    "                enable_scraping=self.config.get('enable_scraping', False)\n",
    "            )\n",
    "            \n",
    "            duration = time.time() - start_time\n",
    "            logging.info(f\"=== PIPELINE COMPLETADO EN {duration/60:.1f} MINUTOS ===\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error en pipeline: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _save_metrics(self, history, num_users, num_items):\n",
    "        \"\"\"Guarda métricas del modelo en BD\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        val_acc = max(history.history.get('val_interaction_accuracy', [0]))\n",
    "        \n",
    "        cursor.execute('''\n",
    "            INSERT INTO model_metrics \n",
    "            (timestamp, val_loss, val_accuracy, training_time, num_users, num_items)\n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "        ''', (datetime.now(), val_loss, val_acc, 0, num_users, num_items))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_recommendations(self, user_id, city, num_recommendations=10):\n",
    "        \"\"\"Genera recomendaciones personalizadas con re-ranking dinámico y scraping opcional\"\"\"\n",
    "        logging.info(f\"Generando recomendaciones para {user_id} en {city}\")\n",
    "        \n",
    "        # Verificar si el modelo está cargado\n",
    "        if self.model.model is None:\n",
    "            raise ValueError(\"Modelo no cargado. Ejecute el pipeline primero.\")\n",
    "        \n",
    "        # Obtener índices\n",
    "        user_idx = self.model.user_to_idx.get(user_id, 0)\n",
    "        city_idx = self.model.city_to_idx.get(city, 0)\n",
    "        \n",
    "        # Generar predicciones base para todos los items\n",
    "        base_recommendations = []\n",
    "        \n",
    "        for item_id, item_idx in list(self.model.item_to_idx.items())[:100]:  # Limitar para velocidad\n",
    "            # Preparar entrada\n",
    "            X = {\n",
    "                'user_id': np.array([user_idx]),\n",
    "                'item_id': np.array([item_idx]),\n",
    "                'city_id': np.array([city_idx]),\n",
    "                'contextual_features': np.array([[0.5] * 10]),\n",
    "                'temporal_features': np.array([[0.25, 0.25, 0.25, 0.25]]),\n",
    "                'weather_sentiment': np.array([[0.5] * 15])\n",
    "            }\n",
    "            \n",
    "            # Predicción\n",
    "            try:\n",
    "                pred = self.model.model.predict(X, verbose=0)\n",
    "                \n",
    "                base_recommendations.append({\n",
    "                    'item_id': item_id,\n",
    "                    'predicted_rating': float(pred[0][0][0]),\n",
    "                    'predicted_sentiment': float(pred[1][0][0]),\n",
    "                    'interaction_probability': float(pred[2][0][0]),\n",
    "                    'combined_score': float(\n",
    "                        pred[0][0][0] * 0.4 + \n",
    "                        pred[1][0][0] * 0.3 + \n",
    "                        pred[2][0][0] * 0.3\n",
    "                    )\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Aplicar re-ranking con clima real y scraping opcional\n",
    "        csv_path = self.config.get('scraping_csv_path')\n",
    "        reranked = self.reranking_system.rerank_recommendations(\n",
    "            base_recommendations, city, csv_path=csv_path\n",
    "        )\n",
    "        \n",
    "        # Guardar en BD\n",
    "        self._save_recommendations(user_id, city, reranked[:num_recommendations])\n",
    "        \n",
    "        return reranked[:num_recommendations]\n",
    "    \n",
    "    def _save_recommendations(self, user_id, city, recommendations):\n",
    "        \"\"\"Guarda recomendaciones en BD\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        for rec in recommendations:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO recommendations \n",
    "                (user_id, city, item_id, score, weather_factor, temporal_factor, timestamp, metadata)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            ''', (\n",
    "                user_id, \n",
    "                city, \n",
    "                rec['item_id'],\n",
    "                rec.get('final_score', 0),\n",
    "                rec.get('weather_factor', 1.0),\n",
    "                rec.get('temporal_factor', 1.0),\n",
    "                datetime.now(),\n",
    "                json.dumps(rec.get('weather_data', {}))\n",
    "            ))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753f4d2-db6f-41c9-8f88-b3988dfc76b8",
   "metadata": {},
   "source": [
    "### Sistema de explicabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4a45731-ea9b-43c4-84ac-26f8ef69728b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainabilityEngine:\n",
    "    \"\"\"Motor de explicabilidad para las recomendaciones\"\"\"\n",
    "    \n",
    "    def generate_explanation(self, recommendation, city):\n",
    "        \"\"\"Genera explicación legible para una recomendación\"\"\"\n",
    "        explanations = []\n",
    "        \n",
    "        # Score base\n",
    "        base_score = recommendation.get('original_score', recommendation.get('combined_score', 0))\n",
    "        explanations.append(f\"Puntuación base: {base_score:.2%}\")\n",
    "        \n",
    "        # Factor clima\n",
    "        if 'weather_data' in recommendation:\n",
    "            weather = recommendation['weather_data']\n",
    "            explanations.append(f\"Clima actual en {city}: {weather['description']}\")\n",
    "            explanations.append(f\"Temperatura: {weather['temperature']:.1f}°C\")\n",
    "            \n",
    "            if weather.get('rain', 0) > 0:\n",
    "                explanations.append(f\"Lluvia: {weather['rain']:.1f}mm/h\")\n",
    "        \n",
    "        # Factor temporal\n",
    "        if recommendation.get('temporal_factor', 1.0) > 1.1:\n",
    "            explanations.append(\"Horario ideal para esta actividad\")\n",
    "        \n",
    "        # Insights de scraping\n",
    "        if 'scraping_insights' in recommendation and recommendation['scraping_insights']:\n",
    "            insights = recommendation['scraping_insights']\n",
    "            if insights['sentiment_trend'] == 'positive':\n",
    "                explanations.append(\"Opiniones recientes muy positivas\")\n",
    "            elif insights['sentiment_trend'] == 'negative':\n",
    "                explanations.append(\"Algunas opiniones negativas recientes\")\n",
    "            \n",
    "            if insights.get('recent_topics'):\n",
    "                explanations.append(f\"Temas populares: {', '.join(insights['recent_topics'][:3])}\")\n",
    "        \n",
    "        # Score final\n",
    "        final_score = recommendation.get('final_score', base_score)\n",
    "        if final_score != base_score:\n",
    "            improvement = ((final_score / base_score) - 1) * 100\n",
    "            if improvement > 0:\n",
    "                explanations.append(f\"Mejora por condiciones actuales: +{improvement:.0f}%\")\n",
    "            else:\n",
    "                explanations.append(f\"Ajuste por condiciones: {improvement:.0f}%\")\n",
    "        \n",
    "        return {\n",
    "            'item_id': recommendation['item_id'],\n",
    "            'score': final_score,\n",
    "            'explanations': explanations\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984748f6-63c1-41fb-9f80-d497132cc4c9",
   "metadata": {},
   "source": [
    "### Función principal y Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40b27d07-9ba2-4e8f-ab12-af3de89c946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_daily_update(config_file='config.json'):\n",
    "    \"\"\"Ejecuta actualización diaria del sistema\"\"\"\n",
    "    logging.info(\"Iniciando actualización diaria...\")\n",
    "    \n",
    "    # Cargar configuración\n",
    "    if os.path.exists(config_file):\n",
    "        with open(config_file, 'r') as f:\n",
    "            config = json.load(f)\n",
    "    else:\n",
    "        config = {\n",
    "            'file_paths': {\n",
    "                'activities': ['activities.csv'],\n",
    "                'reviews': 'reviews.csv'\n",
    "            },\n",
    "            'model_path': 'tourism_model.h5',\n",
    "            'db_path': 'tourism_recommender.db',\n",
    "            'enable_scraping': False\n",
    "        }\n",
    "    \n",
    "    # Ejecutar pipeline\n",
    "    pipeline = TourismRecommenderPipeline(config)\n",
    "    success = pipeline.run_complete_pipeline(\n",
    "        config['file_paths'],\n",
    "        retrain=False  # Solo reentrenar si es necesario\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        logging.info(\"Actualización diaria completada exitosamente\")\n",
    "    else:\n",
    "        logging.error(\"Error en actualización diaria\")\n",
    "    \n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6386fce7-e35a-4c4a-a867-6c9e2830d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_daily_updates():\n",
    "    \"\"\"Programa actualizaciones diarias\"\"\"\n",
    "    import schedule\n",
    "    \n",
    "    # Programar actualización diaria a las 3 AM\n",
    "    schedule.every().day.at(\"03:00\").do(run_daily_update)\n",
    "    \n",
    "    logging.info(\"Scheduler iniciado. Actualización diaria programada a las 03:00\")\n",
    "    \n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "380a7cf6-e36f-47b1-90d3-6b980eb5f8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Función principal de ejemplo\"\"\"\n",
    "    \n",
    "    # Configuración con scraping opcional\n",
    "    config = {\n",
    "        'file_paths': {\n",
    "            'activities': [\n",
    "                'atracciones_civitatis_procesado.csv',\n",
    "                'booking_atracciones_limpios.csv',\n",
    "                'booking_hoteles_limpio.csv',\n",
    "                'detripadvisor_procesado.csv',\n",
    "                'getyourguide_procesado.csv'\n",
    "            ],\n",
    "            'reviews': 'comentarios_final_definitivo_con_descripcion.csv'\n",
    "        },\n",
    "        'model_path': 'tourism_model_rtx3050ti.h5',\n",
    "        'db_path': 'tourism_recommender.db',\n",
    "        'batch_size': 128,\n",
    "        'epochs': 30,\n",
    "        'max_users': 10000,\n",
    "        'max_items': 5000,\n",
    "        'enable_scraping': False,  # Cambiar a True para habilitar scraping\n",
    "        'scraping_csv_path': 'reviews_scraping_20250916_204537.csv'  # Path al CSV de scraping\n",
    "    }\n",
    "    \n",
    "    # Crear pipeline\n",
    "    pipeline = TourismRecommenderPipeline(config)\n",
    "    \n",
    "    # Opción 1: Ejecutar pipeline completo (entrenamiento)\n",
    "    print(\"Ejecutando pipeline completo...\")\n",
    "    success = pipeline.run_complete_pipeline(\n",
    "        config['file_paths'],\n",
    "        retrain=True  # Forzar reentrenamiento\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        # Opción 2: Generar recomendaciones con scraping opcional\n",
    "        print(\"\\nGenerando recomendaciones...\")\n",
    "        \n",
    "        # Sin scraping (solo clima)\n",
    "        recommendations_weather_only = pipeline.generate_recommendations(\n",
    "            user_id='user_1',\n",
    "            city='Madrid',\n",
    "            num_recommendations=5\n",
    "        )\n",
    "        \n",
    "        # Con scraping si está habilitado\n",
    "        if config['enable_scraping']:\n",
    "            pipeline.config['enable_scraping'] = True\n",
    "            pipeline.reranking_system = DailyRerankingSystem(enable_scraping=True)\n",
    "            \n",
    "            recommendations_with_scraping = pipeline.generate_recommendations(\n",
    "                user_id='user_1',\n",
    "                city='Barcelona',\n",
    "                num_recommendations=5\n",
    "            )\n",
    "        \n",
    "        # Mostrar resultados con explicaciones\n",
    "        explainer = ExplainabilityEngine()\n",
    "        \n",
    "        print(\"\\n=== RECOMENDACIONES PARA MADRID (Solo Clima) ===\")\n",
    "        if recommendations_weather_only:\n",
    "            weather = recommendations_weather_only[0].get('weather_data', {})\n",
    "            print(f\"Clima actual: {weather.get('description', 'No disponible')}\")\n",
    "            print(f\"Temperatura: {weather.get('temperature', 'N/A')}°C\")\n",
    "            print(\"\\nTop 5 actividades recomendadas:\")\n",
    "            \n",
    "            for i, rec in enumerate(recommendations_weather_only, 1):\n",
    "                print(f\"\\n{i}. {rec['item_id']}\")\n",
    "                print(f\"   Score final: {rec.get('final_score', 0):.3f}\")\n",
    "                print(f\"   Factor clima: {rec.get('weather_factor', 1.0):.2f}\")\n",
    "                print(f\"   Factor temporal: {rec.get('temporal_factor', 1.0):.2f}\")\n",
    "        \n",
    "        if config['enable_scraping'] and 'recommendations_with_scraping' in locals():\n",
    "            print(\"\\n=== RECOMENDACIONES PARA BARCELONA (Con Scraping) ===\")\n",
    "            insights = recommendations_with_scraping[0].get('scraping_insights', {})\n",
    "            if insights:\n",
    "                print(f\"Reviews analizadas: {insights.get('total_reviews', 0)}\")\n",
    "                print(f\"Tendencia de sentimiento: {insights.get('sentiment_trend', 'neutral')}\")\n",
    "                print(f\"Temas populares: {', '.join(insights.get('recent_topics', []))}\")\n",
    "            \n",
    "            print(\"\\nTop 5 actividades recomendadas:\")\n",
    "            for i, rec in enumerate(recommendations_with_scraping[:5], 1):\n",
    "                print(f\"\\n{i}. {rec['item_id']}\")\n",
    "                print(f\"   Score final: {rec.get('final_score', 0):.3f}\")\n",
    "                print(f\"   Boost scraping: {rec.get('scraping_boost', 1.0):.2f}\")\n",
    "    \n",
    "    # Información sobre el uso del scraping\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INFORMACIÓN SOBRE EL SCRAPING:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"El sistema puede funcionar de dos formas:\")\n",
    "    print(\"1. Solo con clima (por defecto): Usa la API de OpenWeatherMap\")\n",
    "    print(\"2. Con scraping opcional: Lee CSV si existe o hace scraping ligero\")\n",
    "    print(\"\\nPara habilitar scraping:\")\n",
    "    print(\"  config['enable_scraping'] = True\")\n",
    "    print(\"  config['scraping_csv_path'] = 'path/al/archivo.csv'\")\n",
    "    print(\"\\nVentajas del modo solo clima:\")\n",
    "    print(\"  - Más rápido y ligero\")\n",
    "    print(\"  - No requiere dependencias adicionales\")\n",
    "    print(\"  - Siempre disponible con API key\")\n",
    "    print(\"\\nVentajas del modo con scraping:\")\n",
    "    print(\"  - Insights adicionales de reviews recientes\")\n",
    "    print(\"  - Detección de tendencias y temas populares\")\n",
    "    print(\"  - Mejor personalización basada en opiniones reales\")\n",
    "\n",
    "    # Descomentar para activar:\n",
    "    # schedule_daily_updates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53082a13-c6cd-40e4-9ea1-521d6f72ff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
