{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7440b47-ced7-40bd-8381-ad7b0106474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f0166b-3a9e-4b8e-a905-45f238455b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourismDataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        \n",
    "    def load_and_clean_datasets(self, file_paths):\n",
    "        \"\"\"\n",
    "        Carga y limpia todos los datasets\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        # Cargar datasets principales\n",
    "        print(\"Cargando datasets principales...\")\n",
    "        \n",
    "        # Actividades (5 archivos)\n",
    "        activity_files = file_paths['activities']\n",
    "        activities_list = []\n",
    "        for file in activity_files:\n",
    "            df = pd.read_csv(file)\n",
    "            activities_list.append(df)\n",
    "        datasets['activities'] = pd.concat(activities_list, ignore_index=True)\n",
    "        \n",
    "        # Comentarios con análisis de sentimiento y clima\n",
    "        datasets['reviews'] = pd.read_csv(file_paths['reviews'])\n",
    "        \n",
    "        # Datos de turismo ONU (7 archivos)\n",
    "        un_tourism_files = file_paths['un_tourism']\n",
    "        un_data_list = []\n",
    "        for file in un_tourism_files:\n",
    "            df = pd.read_csv(file)\n",
    "            un_data_list.append(df)\n",
    "        datasets['un_tourism'] = pd.concat(un_data_list, ignore_index=True)\n",
    "        \n",
    "        # Meta Data for Good\n",
    "        datasets['commuting_zones'] = pd.read_csv(file_paths['commuting_zones'])\n",
    "        datasets['movement_data'] = pd.read_csv(file_paths['movement_data'])\n",
    "        \n",
    "        # Google Trends\n",
    "        datasets['search_trends'] = pd.read_csv(file_paths['search_trends'])\n",
    "        datasets['monthly_interest'] = pd.read_csv(file_paths['monthly_interest'])\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def match_reviews_to_activities(self, activities_df, sentiment_df):\n",
    "        \"\"\"\n",
    "        Empareja reviews con actividades basándose en títulos y crea dataset expandido\n",
    "        \"\"\"\n",
    "        print(\"Emparejando reviews con actividades...\")\n",
    "        \n",
    "        # Verificar columnas necesarias\n",
    "        required_activity_cols = ['titulo', 'ciudad', 'id', 'precio', 'rating']\n",
    "        required_sentiment_cols = ['texto', 'ciudad', 'sentimiento', 'confianza']\n",
    "        \n",
    "        missing_activity_cols = set(required_activity_cols) - set(activities_df.columns)\n",
    "        missing_sentiment_cols = set(required_sentiment_cols) - set(sentiment_df.columns)\n",
    "        \n",
    "        if missing_activity_cols:\n",
    "            print(f\"Columnas faltantes en activities: {missing_activity_cols}\")\n",
    "        if missing_sentiment_cols:\n",
    "            print(f\"Columnas faltantes en sentiment: {missing_sentiment_cols}\")\n",
    "        \n",
    "        # NORMALIZAR NOMBRES DE CIUDADES\n",
    "        print(\"Normalizando nombres de ciudades...\")\n",
    "        activities_df = activities_df.copy()\n",
    "        sentiment_df = sentiment_df.copy()\n",
    "        \n",
    "        # Función para normalizar nombres de ciudades\n",
    "        def normalize_city_name(city_name):\n",
    "            if pd.isna(city_name):\n",
    "                return 'Unknown'\n",
    "            \n",
    "            city_str = str(city_name).lower().strip()\n",
    "            \n",
    "            # Mapeo de normalizaciones\n",
    "            city_mappings = {\n",
    "                'barcelona': 'Barcelona',\n",
    "                'barcelona1': 'Barcelona',\n",
    "                'madrid': 'Madrid',\n",
    "                'malaga': 'Malaga',\n",
    "                'málaga': 'Malaga',\n",
    "                'sevilla': 'Sevilla',\n",
    "                'valencia': 'Valencia',\n",
    "                'tenerife': 'Tenerife',\n",
    "                'gran canaria': 'Gran Canaria',\n",
    "                'canaria': 'Gran Canaria',\n",
    "                'canarias': 'Gran Canaria',\n",
    "                'mallorca': 'Mallorca',\n",
    "                'palma de mallorca': 'Mallorca',\n",
    "                'palma': 'Mallorca'\n",
    "            }\n",
    "            \n",
    "            return city_mappings.get(city_str, city_name)\n",
    "        \n",
    "        # Aplicar normalización\n",
    "        activities_df['ciudad_original'] = activities_df['ciudad']\n",
    "        sentiment_df['ciudad_original'] = sentiment_df['ciudad']\n",
    "        \n",
    "        activities_df['ciudad'] = activities_df['ciudad'].apply(normalize_city_name)\n",
    "        sentiment_df['ciudad'] = sentiment_df['ciudad'].apply(normalize_city_name)\n",
    "        \n",
    "        print(\"Ciudades después de normalización:\")\n",
    "        print(f\"  Activities: {sorted(activities_df['ciudad'].unique())}\")\n",
    "        print(f\"  Sentiment: {sorted(sentiment_df['ciudad'].unique())}\")\n",
    "        \n",
    "        # Crear dataset expandido donde cada review se asocia con su actividad\n",
    "        matched_reviews = []\n",
    "        total_matches = 0\n",
    "        successful_matches = 0\n",
    "        errors_count = 0\n",
    "        \n",
    "        print(f\"Procesando {len(activities_df)} actividades...\")\n",
    "        \n",
    "        for idx, activity in activities_df.iterrows():\n",
    "            if idx % 1000 == 0:\n",
    "                print(f\"Progreso: {idx}/{len(activities_df)} ({idx/len(activities_df)*100:.1f}%) - Matches: {total_matches}\")\n",
    "            \n",
    "            try:\n",
    "                activity_title = str(activity['titulo']).lower().strip()\n",
    "                activity_city = activity['ciudad']\n",
    "                item_id = activity.get('id', f'item_{idx}')\n",
    "                \n",
    "                # Saltear si el título está vacío o es muy corto\n",
    "                if len(activity_title) < 3:\n",
    "                    continue\n",
    "                \n",
    "                # NORMALIZAR TÍTULO - Remover caracteres especiales problemáticos\n",
    "                import string\n",
    "                # Crear tabla de traducción para remover puntuación problemática\n",
    "                translator = str.maketrans('', '', '!¡¿?()[]{}*+^$|\\\\')\n",
    "                activity_title_clean = activity_title.translate(translator)\n",
    "                \n",
    "                # Método 1: Búsqueda exacta sin regex\n",
    "                try:\n",
    "                    # Buscar reviews que contengan el título (búsqueda simple)\n",
    "                    matching_reviews_mask = (\n",
    "                        sentiment_df['texto'].str.lower().str.contains(activity_title_clean, na=False, regex=False) &\n",
    "                        (sentiment_df['ciudad'] == activity_city)\n",
    "                    )\n",
    "                    \n",
    "                    matching_reviews_subset = sentiment_df[matching_reviews_mask].copy()\n",
    "                    \n",
    "                except Exception as simple_error:\n",
    "                    # Método 2: Búsqueda palabra por palabra\n",
    "                    try:\n",
    "                        words = activity_title_clean.split()\n",
    "                        if len(words) >= 2:  # Solo si tiene al menos 2 palabras\n",
    "                            # Buscar reviews que contengan al menos 70% de las palabras del título\n",
    "                            mask = sentiment_df['ciudad'] == activity_city\n",
    "                            word_matches = 0\n",
    "                            for word in words:\n",
    "                                if len(word) > 3:  # Solo palabras significativas\n",
    "                                    try:\n",
    "                                        word_mask = sentiment_df['texto'].str.lower().str.contains(word, na=False, regex=False)\n",
    "                                        if word_mask.any():\n",
    "                                            mask = mask & word_mask\n",
    "                                            word_matches += 1\n",
    "                                    except:\n",
    "                                        continue\n",
    "                            \n",
    "                            # Solo considerar match si encontramos suficientes palabras\n",
    "                            if word_matches >= max(1, len([w for w in words if len(w) > 3]) * 0.7):\n",
    "                                matching_reviews_subset = sentiment_df[mask].copy()\n",
    "                            else:\n",
    "                                matching_reviews_subset = pd.DataFrame()\n",
    "                        else:\n",
    "                            matching_reviews_subset = pd.DataFrame()\n",
    "                            \n",
    "                    except Exception as word_error:\n",
    "                        # Método 3: Búsqueda aproximada con palabras clave\n",
    "                        try:\n",
    "                            # Extraer palabras clave más importantes del título\n",
    "                            important_words = [w for w in activity_title_clean.split() if len(w) > 4]\n",
    "                            if important_words:\n",
    "                                # Buscar al menos una palabra clave importante\n",
    "                                combined_mask = sentiment_df['ciudad'] == activity_city\n",
    "                                word_found = False\n",
    "                                for word in important_words[:3]:  # Solo primeras 3 palabras importantes\n",
    "                                    try:\n",
    "                                        word_mask = sentiment_df['texto'].str.lower().str.contains(word, na=False, regex=False)\n",
    "                                        if word_mask.any():\n",
    "                                            combined_mask = combined_mask & word_mask\n",
    "                                            word_found = True\n",
    "                                            break\n",
    "                                    except:\n",
    "                                        continue\n",
    "                                \n",
    "                                if word_found:\n",
    "                                    matching_reviews_subset = sentiment_df[combined_mask].copy()\n",
    "                                else:\n",
    "                                    matching_reviews_subset = pd.DataFrame()\n",
    "                            else:\n",
    "                                matching_reviews_subset = pd.DataFrame()\n",
    "                                \n",
    "                        except Exception as final_error:\n",
    "                            errors_count += 1\n",
    "                            if errors_count < 10:\n",
    "                                print(f\"  Error final procesando '{activity_title[:30]}...': {final_error}\")\n",
    "                            continue\n",
    "                \n",
    "                # Agregar información de la actividad a cada review matching\n",
    "                if not matching_reviews_subset.empty:\n",
    "                    matching_reviews_subset['item_id'] = item_id\n",
    "                    matching_reviews_subset['precio'] = activity.get('precio', 0)\n",
    "                    matching_reviews_subset['rating_actividad'] = activity.get('rating', 3.0)\n",
    "                    matching_reviews_subset['titulo_actividad'] = activity['titulo']\n",
    "                    \n",
    "                    matched_reviews.append(matching_reviews_subset)\n",
    "                    total_matches += len(matching_reviews_subset)\n",
    "                    successful_matches += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                errors_count += 1\n",
    "                if errors_count < 10:\n",
    "                    print(f\"  Error general procesando actividad {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Consolidar resultados\n",
    "        if matched_reviews:\n",
    "            all_matched_reviews = pd.concat(matched_reviews, ignore_index=True)\n",
    "            \n",
    "            print(f\"\\nRESULTADOS DEL EMPAREJAMIENTO:\")\n",
    "            print(f\"   Reviews emparejadas: {len(all_matched_reviews):,}\")\n",
    "            print(f\"   Actividades con reviews: {all_matched_reviews['item_id'].nunique()}\")\n",
    "            print(f\"   Tasa de cobertura: {successful_matches/len(activities_df)*100:.1f}%\")\n",
    "            print(f\"   Actividades procesadas exitosamente: {successful_matches}\")\n",
    "            print(f\"   Errores encontrados: {errors_count}\")\n",
    "            \n",
    "            # Estadísticas adicionales\n",
    "            reviews_per_activity = all_matched_reviews.groupby('item_id').size()\n",
    "            print(f\"   Reviews promedio por actividad: {reviews_per_activity.mean():.1f}\")\n",
    "            print(f\"   Actividad con más reviews: {reviews_per_activity.max()}\")\n",
    "            \n",
    "            # Estadísticas por ciudad\n",
    "            city_stats = all_matched_reviews.groupby('ciudad').agg({\n",
    "                'item_id': 'nunique',\n",
    "                'texto': 'count'\n",
    "            }).rename(columns={'item_id': 'actividades', 'texto': 'reviews'})\n",
    "            \n",
    "            print(f\"\\n   Distribución por ciudad:\")\n",
    "            for city, stats in city_stats.iterrows():\n",
    "                print(f\"     {city}: {stats['actividades']} actividades, {stats['reviews']} reviews\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nNO SE ENCONTRARON EMPAREJAMIENTOS\")\n",
    "            print(f\"   Actividades procesadas: {len(activities_df)}\")\n",
    "            print(f\"   Reviews disponibles: {len(sentiment_df)}\")\n",
    "            print(f\"   Errores encontrados: {errors_count}\")\n",
    "            \n",
    "            # Análisis más detallado para debugging\n",
    "            print(f\"\\nANÁLISIS DETALLADO:\")\n",
    "            \n",
    "            # Muestrear algunos títulos de actividades\n",
    "            sample_titles = activities_df['titulo'].head(10).tolist()\n",
    "            print(f\"   Muestra de títulos de actividades:\")\n",
    "            for i, title in enumerate(sample_titles):\n",
    "                print(f\"     {i+1}. {title[:100]}...\")\n",
    "            \n",
    "            # Muestrear algunos textos de reviews\n",
    "            sample_texts = sentiment_df['texto'].head(5).tolist()\n",
    "            print(f\"   Muestra de textos de reviews:\")\n",
    "            for i, text in enumerate(sample_texts):\n",
    "                print(f\"     {i+1}. {text[:100]}...\")\n",
    "            \n",
    "            # Crear DataFrame vacío con columnas esperadas\n",
    "            all_matched_reviews = pd.DataFrame(columns=[\n",
    "                'texto', 'ciudad', 'categoria', 'fecha', 'fuente', \n",
    "                'sentimiento', 'confianza', 'descripcion_sencilla',\n",
    "                'item_id', 'precio', 'rating_actividad', 'titulo_actividad'\n",
    "            ])\n",
    "        \n",
    "        return all_matched_reviews\n",
    "    \n",
    "    def test_matching_algorithm(self, activities_df, sentiment_df, sample_size=100):\n",
    "        \"\"\"\n",
    "        Función de prueba para validar el algoritmo de emparejamiento\n",
    "        \"\"\"\n",
    "        print(f\"\\nPRUEBA DE ALGORITMO DE EMPAREJAMIENTO\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Normalizar ciudades primero\n",
    "        activities_test = activities_df.copy()\n",
    "        sentiment_test = sentiment_df.copy()\n",
    "        \n",
    "        def normalize_city_name(city_name):\n",
    "            if pd.isna(city_name):\n",
    "                return 'Unknown'\n",
    "            city_str = str(city_name).lower().strip()\n",
    "            city_mappings = {\n",
    "                'barcelona': 'Barcelona', 'barcelona1': 'Barcelona',\n",
    "                'madrid': 'Madrid', 'malaga': 'Malaga', 'málaga': 'Malaga',\n",
    "                'sevilla': 'Sevilla', 'valencia': 'Valencia', 'tenerife': 'Tenerife',\n",
    "                'gran canaria': 'Gran Canaria', 'canaria': 'Gran Canaria', 'canarias': 'Gran Canaria',\n",
    "                'mallorca': 'Mallorca', 'palma de mallorca': 'Mallorca', 'palma': 'Mallorca'\n",
    "            }\n",
    "            return city_mappings.get(city_str, city_name)\n",
    "        \n",
    "        activities_test['ciudad'] = activities_test['ciudad'].apply(normalize_city_name)\n",
    "        sentiment_test['ciudad'] = sentiment_test['ciudad'].apply(normalize_city_name)\n",
    "        \n",
    "        # Tomar muestra pequeña para prueba\n",
    "        sample_activities = activities_test.head(sample_size)\n",
    "        \n",
    "        print(f\"Probando con {len(sample_activities)} actividades...\")\n",
    "        print(f\"Cities en activities: {sorted(sample_activities['ciudad'].unique())}\")\n",
    "        print(f\"Cities en sentiment: {sorted(sentiment_test['ciudad'].unique())}\")\n",
    "        \n",
    "        matches_found = 0\n",
    "        test_results = []\n",
    "        \n",
    "        for idx, activity in sample_activities.iterrows():\n",
    "            activity_title = str(activity['titulo']).lower().strip()\n",
    "            activity_city = activity['ciudad']\n",
    "            \n",
    "            # Limpiar título\n",
    "            import string\n",
    "            translator = str.maketrans('', '', '!¡¿?()[]{}*+^$|\\\\')\n",
    "            activity_title_clean = activity_title.translate(translator)\n",
    "            \n",
    "            # Buscar matches\n",
    "            city_reviews = sentiment_test[sentiment_test['ciudad'] == activity_city]\n",
    "            \n",
    "            if len(city_reviews) > 0:\n",
    "                # Método simple: buscar título completo\n",
    "                full_matches = city_reviews[\n",
    "                    city_reviews['texto'].str.lower().str.contains(activity_title_clean, na=False, regex=False)\n",
    "                ]\n",
    "                \n",
    "                # Método alternativo: buscar palabras clave\n",
    "                words = [w for w in activity_title_clean.split() if len(w) > 3]\n",
    "                word_matches = pd.DataFrame()\n",
    "                \n",
    "                if words:\n",
    "                    mask = city_reviews['ciudad'] == activity_city\n",
    "                    for word in words[:3]:  # Solo primeras 3 palabras importantes\n",
    "                        try:\n",
    "                            word_mask = city_reviews['texto'].str.lower().str.contains(word, na=False, regex=False)\n",
    "                            mask = mask & word_mask\n",
    "                        except:\n",
    "                            continue\n",
    "                    word_matches = city_reviews[mask]\n",
    "                \n",
    "                total_matches = len(full_matches) + len(word_matches)\n",
    "                if total_matches > 0:\n",
    "                    matches_found += 1\n",
    "                \n",
    "                test_results.append({\n",
    "                    'titulo': activity['titulo'][:50],\n",
    "                    'ciudad': activity_city,\n",
    "                    'reviews_ciudad': len(city_reviews),\n",
    "                    'matches_exactos': len(full_matches),\n",
    "                    'matches_palabras': len(word_matches),\n",
    "                    'total_matches': total_matches\n",
    "                })\n",
    "        \n",
    "        print(f\"\\nResultados de la prueba:\")\n",
    "        print(f\"   Actividades con matches: {matches_found}/{len(sample_activities)} ({matches_found/len(sample_activities)*100:.1f}%)\")\n",
    "        \n",
    "        # Mostrar algunos ejemplos\n",
    "        successful_matches = [r for r in test_results if r['total_matches'] > 0]\n",
    "        if successful_matches:\n",
    "            print(f\"\\n   Ejemplos de matches exitosos:\")\n",
    "            for i, match in enumerate(successful_matches[:5]):\n",
    "                print(f\"     {i+1}. {match['titulo']} ({match['ciudad']}) - {match['total_matches']} matches\")\n",
    "        \n",
    "        failed_matches = [r for r in test_results if r['total_matches'] == 0 and r['reviews_ciudad'] > 0]\n",
    "        if failed_matches:\n",
    "            print(f\"\\n   Ejemplos de matches fallidos (con reviews disponibles):\")\n",
    "            for i, fail in enumerate(failed_matches[:5]):\n",
    "                print(f\"     {i+1}. {fail['titulo']} ({fail['ciudad']}) - {fail['reviews_ciudad']} reviews disponibles\")\n",
    "        \n",
    "        return matches_found > 0\n",
    "    \n",
    "    def create_synthetic_user_ids(self, reviews_df):\n",
    "        \"\"\"\n",
    "        Crea user_ids sintéticos basándose en patrones de reviews\n",
    "        \"\"\"\n",
    "        print(\"Creando user_ids sintéticos...\")\n",
    "        \n",
    "        # Estrategia: Crear clusters de usuarios basándose en:\n",
    "        # 1. Ciudad + fecha + fuente (mismo usuario puede revisar múltiples actividades)\n",
    "        # 2. Patrones de sentimiento similares\n",
    "        # 3. Proximidad temporal\n",
    "        \n",
    "        reviews_df = reviews_df.copy()\n",
    "        reviews_df['fecha_comentario'] = pd.to_datetime(reviews_df['fecha'], errors='coerce')  # Usar 'fecha' en lugar de 'fecha_comentario'\n",
    "        \n",
    "        # Crear grupos base por ciudad y fuente\n",
    "        reviews_df['grupo_base'] = (\n",
    "            reviews_df['ciudad'].astype(str) + '_' + \n",
    "            reviews_df['fuente'].astype(str)\n",
    "        )\n",
    "        \n",
    "        # Agrupar por proximidad temporal (mismo día) y características similares\n",
    "        user_id_counter = 0\n",
    "        reviews_df['user_id'] = None\n",
    "        \n",
    "        # Convertir sentimiento categórico a numérico para comparación\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        reviews_df['sentimiento_numerico'] = reviews_df['sentimiento'].map(sentiment_to_numeric).fillna(0)\n",
    "        \n",
    "        for grupo in reviews_df['grupo_base'].unique():\n",
    "            grupo_reviews = reviews_df[reviews_df['grupo_base'] == grupo].copy()\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            grupo_reviews = grupo_reviews.sort_values('fecha_comentario')\n",
    "            \n",
    "            current_user_reviews = []\n",
    "            \n",
    "            for idx, review in grupo_reviews.iterrows():\n",
    "                if not current_user_reviews:\n",
    "                    # Primera review del grupo\n",
    "                    current_user_reviews = [idx]\n",
    "                    continue\n",
    "                \n",
    "                # Obtener última review del usuario actual\n",
    "                last_review_idx = current_user_reviews[-1]\n",
    "                last_review = reviews_df.loc[last_review_idx]\n",
    "                \n",
    "                # Criterios para considerar mismo usuario:\n",
    "                try:\n",
    "                    time_diff = abs((review['fecha_comentario'] - last_review['fecha_comentario']).days)\n",
    "                except:\n",
    "                    time_diff = 365  # Si no se puede calcular, asumir diferencia grande\n",
    "                \n",
    "                sentiment_diff = abs(review['sentimiento_numerico'] - last_review['sentimiento_numerico'])\n",
    "                confidence_diff = abs(review['confianza'] - last_review['confianza'])\n",
    "                \n",
    "                # Si las reviews son muy similares en tiempo y características, mismo usuario\n",
    "                if (time_diff <= 7 and  # Máximo 7 días de diferencia\n",
    "                    sentiment_diff <= 0.5 and  # Sentimientos similares (ajustado para categórico)\n",
    "                    confidence_diff <= 0.1):  # Confianza similar\n",
    "                    current_user_reviews.append(idx)\n",
    "                else:\n",
    "                    # Asignar user_id a reviews acumuladas\n",
    "                    for review_idx in current_user_reviews:\n",
    "                        reviews_df.loc[review_idx, 'user_id'] = f\"user_{user_id_counter}\"\n",
    "                    \n",
    "                    user_id_counter += 1\n",
    "                    current_user_reviews = [idx]\n",
    "            \n",
    "            # Asignar user_id a las últimas reviews del grupo\n",
    "            for review_idx in current_user_reviews:\n",
    "                reviews_df.loc[review_idx, 'user_id'] = f\"user_{user_id_counter}\"\n",
    "            user_id_counter += 1\n",
    "        \n",
    "        print(f\"Creados {user_id_counter} user_ids sintéticos\")\n",
    "        return reviews_df\n",
    "    \n",
    "    def create_activity_features(self, activities_df, matched_reviews_df):\n",
    "        \"\"\"\n",
    "        Crea features enriquecidas para las actividades\n",
    "        \"\"\"\n",
    "        print(\"Creando features de actividades...\")\n",
    "        \n",
    "        # Crear columna item_id si no existe, usando 'id'\n",
    "        if 'item_id' not in activities_df.columns and 'id' in activities_df.columns:\n",
    "            activities_df = activities_df.copy()\n",
    "            activities_df['item_id'] = activities_df['id']\n",
    "        \n",
    "        # Agregar métricas de reviews por actividad\n",
    "        if not matched_reviews_df.empty and 'item_id' in matched_reviews_df.columns:\n",
    "            # Convertir sentimiento categórico a numérico para estadísticas\n",
    "            sentiment_to_numeric = {\n",
    "                'negativo': -1,\n",
    "                'neutro': 0,\n",
    "                'positivo': 1\n",
    "            }\n",
    "            matched_reviews_df['sentimiento_numerico'] = matched_reviews_df['sentimiento'].map(sentiment_to_numeric).fillna(0)\n",
    "            \n",
    "            review_metrics = matched_reviews_df.groupby('item_id').agg({\n",
    "                'sentimiento_numerico': ['mean', 'std', 'count'],\n",
    "                'confianza': ['mean', 'std'],\n",
    "                'rating_actividad': ['mean', 'count']\n",
    "            }).reset_index()\n",
    "            \n",
    "            review_metrics.columns = ['item_id', 'avg_sentiment', 'sentiment_std', \n",
    "                                    'review_count', 'avg_confidence', 'confidence_std',\n",
    "                                    'avg_rating', 'rating_count']\n",
    "            \n",
    "            # Merge con actividades\n",
    "            enriched_activities = activities_df.merge(\n",
    "                review_metrics, on='item_id', how='left'\n",
    "            )\n",
    "            \n",
    "            # Rellenar valores faltantes\n",
    "            enriched_activities['avg_sentiment'] = enriched_activities['avg_sentiment'].fillna(0)\n",
    "            enriched_activities['sentiment_std'] = enriched_activities['sentiment_std'].fillna(0)\n",
    "            enriched_activities['review_count'] = enriched_activities['review_count'].fillna(0)\n",
    "            enriched_activities['avg_confidence'] = enriched_activities['avg_confidence'].fillna(0.5)\n",
    "        else:\n",
    "            enriched_activities = activities_df.copy()\n",
    "            enriched_activities['avg_sentiment'] = 0\n",
    "            enriched_activities['sentiment_std'] = 0\n",
    "            enriched_activities['review_count'] = 0\n",
    "            enriched_activities['avg_confidence'] = 0.5\n",
    "        \n",
    "        # Crear categorías de precio\n",
    "        enriched_activities['precio_categoria'] = pd.cut(\n",
    "            enriched_activities['precio'], \n",
    "            bins=[0, 50, 100, 200, float('inf')], \n",
    "            labels=['Bajo', 'Medio', 'Alto', 'Premium']\n",
    "        )\n",
    "        \n",
    "        # Normalizar rating para consistencia\n",
    "        if enriched_activities['rating'].max() > enriched_activities['rating'].min():\n",
    "            enriched_activities['rating_normalizado'] = (\n",
    "                enriched_activities['rating'] - enriched_activities['rating'].min()\n",
    "            ) / (enriched_activities['rating'].max() - enriched_activities['rating'].min())\n",
    "        else:\n",
    "            enriched_activities['rating_normalizado'] = 0.5\n",
    "        \n",
    "        return enriched_activities\n",
    "    \n",
    "    def create_temporal_features(self, monthly_interest_df):\n",
    "        \"\"\"\n",
    "        Crea features temporales desde google trends\n",
    "        \"\"\"\n",
    "        print(\"Creando features temporales...\")\n",
    "        \n",
    "        # Convertir fecha\n",
    "        monthly_interest_df['date'] = pd.to_datetime(monthly_interest_df['date'])\n",
    "        \n",
    "        # Crear features estacionales\n",
    "        monthly_interest_df['mes'] = monthly_interest_df['date'].dt.month\n",
    "        monthly_interest_df['año'] = monthly_interest_df['date'].dt.year\n",
    "        monthly_interest_df['trimestre'] = monthly_interest_df['date'].dt.quarter\n",
    "        \n",
    "        # Definir estaciones\n",
    "        def get_season(month):\n",
    "            if month in [12, 1, 2]:\n",
    "                return 'Invierno'\n",
    "            elif month in [3, 4, 5]:\n",
    "                return 'Primavera'\n",
    "            elif month in [6, 7, 8]:\n",
    "                return 'Verano'\n",
    "            else:\n",
    "                return 'Otoño'\n",
    "        \n",
    "        monthly_interest_df['estacion'] = monthly_interest_df['mes'].apply(get_season)\n",
    "        \n",
    "        # Melt para formato largo\n",
    "        cities = ['Barcelona', 'Madrid', 'Malaga', 'Sevilla', 'Valencia', \n",
    "                 'Tenerife', 'Gran Canaria', 'Palma de Mallorca']\n",
    "        \n",
    "        temporal_features = pd.melt(\n",
    "            monthly_interest_df, \n",
    "            id_vars=['date', 'mes', 'año', 'trimestre', 'estacion'],\n",
    "            value_vars=cities,\n",
    "            var_name='ciudad',\n",
    "            value_name='interes_turistico'\n",
    "        )\n",
    "        \n",
    "        return temporal_features\n",
    "    \n",
    "    def create_geographical_features(self, commuting_zones_df):\n",
    "        \"\"\"\n",
    "        Crea features geográficas\n",
    "        \"\"\"\n",
    "        print(\"Creando features geográficas...\")\n",
    "        \n",
    "        # Extraer información geográfica\n",
    "        geo_features = commuting_zones_df.copy()\n",
    "        \n",
    "        # Calcular densidad poblacional\n",
    "        geo_features['densidad_poblacion'] = (\n",
    "            geo_features['win_population'] / geo_features['area']\n",
    "        )\n",
    "        \n",
    "        # Calcular densidad de carreteras\n",
    "        geo_features['densidad_carreteras'] = (\n",
    "            geo_features['win_roads_km'] / geo_features['area']\n",
    "        )\n",
    "        \n",
    "        # Normalizar features geográficas\n",
    "        scaler = StandardScaler()\n",
    "        geo_cols = ['win_population', 'win_roads_km', 'area', \n",
    "                   'densidad_poblacion', 'densidad_carreteras']\n",
    "        \n",
    "        geo_features[geo_cols] = scaler.fit_transform(geo_features[geo_cols])\n",
    "        self.scalers['geo_features'] = scaler\n",
    "        \n",
    "        return geo_features\n",
    "    \n",
    "    def create_tourism_context_features(self, un_tourism_df):\n",
    "        \"\"\"\n",
    "        Crea features de contexto turístico desde datos ONU con análisis específico por tipo de turismo\n",
    "        \"\"\"\n",
    "        print(\"Creando features de contexto turístico...\")\n",
    "        \n",
    "        # Verificar columnas disponibles y mapear a nombres esperados\n",
    "        print(f\"Columnas disponibles en UN tourism: {list(un_tourism_df.columns)}\")\n",
    "        \n",
    "        # Mapeo flexible de nombres de columnas\n",
    "        column_mapping = {}\n",
    "        for col in un_tourism_df.columns:\n",
    "            col_lower = col.lower()\n",
    "            if 'year' in col_lower or 'año' in col_lower:\n",
    "                column_mapping['Year'] = col\n",
    "            elif 'country' in col_lower or 'pais' in col_lower or 'país' in col_lower:\n",
    "                column_mapping['Country'] = col\n",
    "            elif 'category' in col_lower or 'categoria' in col_lower or 'categoría' in col_lower:\n",
    "                column_mapping['Category'] = col\n",
    "            elif 'indicator' in col_lower or 'indicador' in col_lower:\n",
    "                column_mapping['Indicator'] = col\n",
    "            elif 'value' in col_lower or 'valor' in col_lower:\n",
    "                column_mapping['Value'] = col\n",
    "        \n",
    "        print(f\"Mapeo de columnas: {column_mapping}\")\n",
    "        \n",
    "        # Si no encontramos las columnas necesarias, crear features por defecto\n",
    "        required_cols = ['Year', 'Country', 'Category', 'Indicator', 'Value']\n",
    "        missing_cols = [col for col in required_cols if col not in column_mapping]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"Columnas faltantes en datos de turismo ONU: {missing_cols}\")\n",
    "            print(\"Creando features de turismo por defecto...\")\n",
    "            \n",
    "            # Crear features por defecto\n",
    "            return {\n",
    "                'spain_domestic': {},\n",
    "                'international_inbound': {},\n",
    "                'combined_features': {\n",
    "                    'spending_ratio': 1.0,\n",
    "                    'price_sensitivity_factor': 1.0,\n",
    "                    'tourism_volume_ratio': 1.0,\n",
    "                    'demand_pressure_factor': 1.0,\n",
    "                    'domestic_tourism_strength': 0,\n",
    "                    'local_preference_factor': 1.0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        # Renombrar columnas para trabajar con ellas\n",
    "        un_tourism_renamed = un_tourism_df.rename(columns={v: k for k, v in column_mapping.items()})\n",
    "        \n",
    "        # Filtrar datos más recientes (últimos 3 años)\n",
    "        current_year = datetime.now().year\n",
    "        recent_data = un_tourism_renamed[un_tourism_renamed['Year'] >= current_year - 3]\n",
    "        \n",
    "        if recent_data.empty:\n",
    "            print(\"No hay datos de turismo en los últimos 3 años, usando todos los datos disponibles\")\n",
    "            recent_data = un_tourism_renamed\n",
    "        \n",
    "        # Separar datos de España (turismo doméstico e inbound) vs otros países (outbound)\n",
    "        spain_data = recent_data[recent_data['Country'].str.contains('Spain', case=False, na=False)].copy()\n",
    "        other_countries_data = recent_data[~recent_data['Country'].str.contains('Spain', case=False, na=False)].copy()\n",
    "        \n",
    "        print(f\"Datos de España: {len(spain_data)} registros\")\n",
    "        print(f\"Datos de otros países: {len(other_countries_data)} registros\")\n",
    "        \n",
    "        # Procesar datos de España\n",
    "        spain_metrics = self._process_spain_tourism_data(spain_data)\n",
    "        \n",
    "        # Procesar datos de otros países (outbound hacia España)\n",
    "        outbound_metrics = self._process_outbound_tourism_data(other_countries_data)\n",
    "        \n",
    "        # Combinar métricas\n",
    "        tourism_context = {\n",
    "            'spain_domestic': spain_metrics,\n",
    "            'international_inbound': outbound_metrics,\n",
    "            'combined_features': self._combine_tourism_features(spain_metrics, outbound_metrics)\n",
    "        }\n",
    "        \n",
    "        return tourism_context\n",
    "    \n",
    "    def _process_spain_tourism_data(self, spain_data):\n",
    "        \"\"\"\n",
    "        Procesa datos de turismo doméstico e inbound de España\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Categorías principales para España\n",
    "        categories_of_interest = ['domestic', 'inbound arrivals', 'inbound arrivals by region', \n",
    "                                'inbound expenditure']\n",
    "        \n",
    "        for category in categories_of_interest:\n",
    "            category_data = spain_data[\n",
    "                spain_data['Category'].str.contains(category, case=False, na=False)\n",
    "            ]\n",
    "            \n",
    "            if not category_data.empty:\n",
    "                # Agregar por indicador\n",
    "                category_metrics = category_data.groupby('Indicator').agg({\n",
    "                    'Value': ['mean', 'sum', 'std', 'count']\n",
    "                }).reset_index()\n",
    "                \n",
    "                category_metrics.columns = ['Indicator', f'{category}_mean', f'{category}_sum', \n",
    "                                          f'{category}_std', f'{category}_count']\n",
    "                \n",
    "                metrics[category] = category_metrics\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _process_outbound_tourism_data(self, outbound_data):\n",
    "        \"\"\"\n",
    "        Procesa datos de turismo outbound de otros países (potenciales visitantes a España)\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Categorías de outbound\n",
    "        outbound_categories = ['outbound departures', 'outbound expenditure']\n",
    "        \n",
    "        for category in outbound_categories:\n",
    "            category_data = outbound_data[\n",
    "                outbound_data['Category'].str.contains(category, case=False, na=False)\n",
    "            ]\n",
    "            \n",
    "            if not category_data.empty:\n",
    "                # Agregar por país y indicador para entender patrones de gasto\n",
    "                country_metrics = category_data.groupby(['Country', 'Indicator']).agg({\n",
    "                    'Value': ['mean', 'sum', 'std']\n",
    "                }).reset_index()\n",
    "                \n",
    "                country_metrics.columns = ['Country', 'Indicator', f'{category}_mean', \n",
    "                                         f'{category}_sum', f'{category}_std']\n",
    "                \n",
    "                # Calcular métricas globales\n",
    "                global_metrics = category_data.groupby('Indicator').agg({\n",
    "                    'Value': ['mean', 'sum', 'std', 'count']\n",
    "                }).reset_index()\n",
    "                \n",
    "                global_metrics.columns = ['Indicator', f'{category}_global_mean', \n",
    "                                        f'{category}_global_sum', f'{category}_global_std',\n",
    "                                        f'{category}_global_count']\n",
    "                \n",
    "                metrics[category] = {\n",
    "                    'by_country': country_metrics,\n",
    "                    'global': global_metrics\n",
    "                }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _combine_tourism_features(self, spain_metrics, outbound_metrics):\n",
    "        \"\"\"\n",
    "        Combina métricas de turismo para crear features de contexto\n",
    "        \"\"\"\n",
    "        combined_features = {}\n",
    "        \n",
    "        # Ratio de gasto outbound vs inbound (indica poder adquisitivo de visitantes)\n",
    "        try:\n",
    "            if ('inbound expenditure' in spain_metrics and \n",
    "                'outbound expenditure' in outbound_metrics):\n",
    "                \n",
    "                inbound_spending = spain_metrics['inbound expenditure']\n",
    "                outbound_spending = outbound_metrics['outbound expenditure']['global']\n",
    "                \n",
    "                if not inbound_spending.empty and not outbound_spending.empty:\n",
    "                    avg_inbound = inbound_spending['inbound expenditure_mean'].mean()\n",
    "                    avg_outbound = outbound_spending['outbound expenditure_global_mean'].mean()\n",
    "                    \n",
    "                    combined_features['spending_ratio'] = avg_outbound / max(avg_inbound, 1)\n",
    "                    combined_features['price_sensitivity_factor'] = min(2.0, max(0.5, \n",
    "                        1.0 + (combined_features['spending_ratio'] - 1.0) * 0.1))\n",
    "        except:\n",
    "            combined_features['spending_ratio'] = 1.0\n",
    "            combined_features['price_sensitivity_factor'] = 1.0\n",
    "        \n",
    "        # Volumen de turismo (arrivals vs departures)\n",
    "        try:\n",
    "            if ('inbound arrivals' in spain_metrics and \n",
    "                'outbound departures' in outbound_metrics):\n",
    "                \n",
    "                inbound_arrivals = spain_metrics['inbound arrivals']\n",
    "                outbound_departures = outbound_metrics['outbound departures']['global']\n",
    "                \n",
    "                if not inbound_arrivals.empty and not outbound_departures.empty:\n",
    "                    avg_arrivals = inbound_arrivals['inbound arrivals_mean'].mean()\n",
    "                    avg_departures = outbound_departures['outbound departures_global_mean'].mean()\n",
    "                    \n",
    "                    combined_features['tourism_volume_ratio'] = avg_arrivals / max(avg_departures, 1)\n",
    "                    combined_features['demand_pressure_factor'] = min(2.0, max(0.5,\n",
    "                        1.0 + (combined_features['tourism_volume_ratio'] - 1.0) * 0.05))\n",
    "        except:\n",
    "            combined_features['tourism_volume_ratio'] = 1.0\n",
    "            combined_features['demand_pressure_factor'] = 1.0\n",
    "        \n",
    "        # Factor de turismo doméstico\n",
    "        try:\n",
    "            if 'domestic' in spain_metrics:\n",
    "                domestic_tourism = spain_metrics['domestic']\n",
    "                if not domestic_tourism.empty:\n",
    "                    combined_features['domestic_tourism_strength'] = domestic_tourism['domestic_mean'].mean()\n",
    "                    combined_features['local_preference_factor'] = min(1.5, max(0.7,\n",
    "                        1.0 + (combined_features['domestic_tourism_strength'] / 1000000 - 1.0) * 0.1))\n",
    "        except:\n",
    "            combined_features['domestic_tourism_strength'] = 0\n",
    "            combined_features['local_preference_factor'] = 1.0\n",
    "        \n",
    "        return combined_features\n",
    "    \n",
    "    def create_user_item_matrix(self, matched_reviews_df, activities_df, storage_path=\"AI_Recomendador\"):\n",
    "        \"\"\"\n",
    "        Crea matriz usuario-item usando storage en disco para manejar datasets grandes\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import h5py\n",
    "        from pathlib import Path\n",
    "        \n",
    "        print(\"Creando matriz usuario-item con storage en disco...\")\n",
    "        \n",
    "        if matched_reviews_df.empty or 'user_id' not in matched_reviews_df.columns:\n",
    "            print(\"No hay datos de reviews matched o user_ids, creando matriz vacía\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # Crear directorio de storage\n",
    "        storage_dir = Path(storage_path)\n",
    "        storage_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"Usando directorio de storage: {storage_dir.absolute()}\")\n",
    "        \n",
    "        # Convertir sentimiento categórico a numérico\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        \n",
    "        matched_reviews_df['sentimiento_numerico'] = matched_reviews_df['sentimiento'].map(sentiment_to_numeric)\n",
    "        \n",
    "        # Convertir sentimiento categórico a pseudo-rating [1-5] \n",
    "        def sentiment_to_rating(sentiment_cat):\n",
    "            if sentiment_cat == 'negativo':\n",
    "                return np.random.uniform(1, 2)  # Rating entre 1-2\n",
    "            elif sentiment_cat == 'positivo':\n",
    "                return np.random.uniform(4, 5)  # Rating entre 4-5\n",
    "            else:  # neutro\n",
    "                return 3  # Rating neutro\n",
    "        \n",
    "        matched_reviews_df['pseudo_rating'] = matched_reviews_df['sentimiento'].apply(sentiment_to_rating)\n",
    "        \n",
    "        # Obtener usuarios e items únicos\n",
    "        unique_users = matched_reviews_df['user_id'].unique()\n",
    "        unique_items = matched_reviews_df['item_id'].unique()\n",
    "        \n",
    "        print(f\"Dimensiones de matriz: {len(unique_users):,} usuarios x {len(unique_items):,} items\")\n",
    "        \n",
    "        # Crear mapeos de índices\n",
    "        user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        \n",
    "        # Guardar mapeos para uso posterior\n",
    "        mappings_file = storage_dir / 'user_item_mappings.pkl'\n",
    "        import pickle\n",
    "        with open(mappings_file, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'user_to_idx': user_to_idx,\n",
    "                'item_to_idx': item_to_idx,\n",
    "                'users': unique_users,\n",
    "                'items': unique_items\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Mapeos guardados en {mappings_file}\")\n",
    "        \n",
    "        # MÉTODO EFICIENTE: Procesar en chunks y guardar en formato HDF5\n",
    "        chunk_size = 50000  # Procesar de 50k reviews a la vez\n",
    "        total_chunks = len(matched_reviews_df) // chunk_size + 1\n",
    "        \n",
    "        # Archivos de storage\n",
    "        ratings_file = storage_dir / 'user_item_ratings.h5'\n",
    "        sentiment_file = storage_dir / 'user_item_sentiment.h5'\n",
    "        confidence_file = storage_dir / 'user_item_confidence.h5'\n",
    "        \n",
    "        # Inicializar archivos HDF5\n",
    "        with h5py.File(ratings_file, 'w') as f:\n",
    "            ratings_dataset = f.create_dataset('ratings', \n",
    "                                             (len(unique_users), len(unique_items)), \n",
    "                                             dtype='float32', \n",
    "                                             fillvalue=0.0,\n",
    "                                             compression='gzip')\n",
    "            \n",
    "        with h5py.File(sentiment_file, 'w') as f:\n",
    "            sentiment_dataset = f.create_dataset('sentiment', \n",
    "                                               (len(unique_users), len(unique_items)), \n",
    "                                               dtype='float32', \n",
    "                                               fillvalue=0.0,\n",
    "                                               compression='gzip')\n",
    "            \n",
    "        with h5py.File(confidence_file, 'w') as f:\n",
    "            confidence_dataset = f.create_dataset('confidence', \n",
    "                                                (len(unique_users), len(unique_items)), \n",
    "                                                dtype='float32', \n",
    "                                                fillvalue=0.0,\n",
    "                                                compression='gzip')\n",
    "        \n",
    "        print(\"Procesando reviews en chunks...\")\n",
    "        \n",
    "        # Procesar por chunks\n",
    "        for chunk_idx in range(total_chunks):\n",
    "            start_idx = chunk_idx * chunk_size\n",
    "            end_idx = min((chunk_idx + 1) * chunk_size, len(matched_reviews_df))\n",
    "            \n",
    "            if start_idx >= len(matched_reviews_df):\n",
    "                break\n",
    "                \n",
    "            chunk = matched_reviews_df.iloc[start_idx:end_idx]\n",
    "            print(f\"Procesando chunk {chunk_idx + 1}/{total_chunks} ({len(chunk)} reviews)\")\n",
    "            \n",
    "            # Agrupar por usuario-item y calcular métricas\n",
    "            chunk_aggregated = chunk.groupby(['user_id', 'item_id']).agg({\n",
    "                'pseudo_rating': 'mean',\n",
    "                'sentimiento_numerico': 'mean',\n",
    "                'confianza': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Actualizar matrices en disco\n",
    "            with h5py.File(ratings_file, 'r+') as f:\n",
    "                ratings_dataset = f['ratings']\n",
    "                for _, row in chunk_aggregated.iterrows():\n",
    "                    user_idx = user_to_idx[row['user_id']]\n",
    "                    item_idx = item_to_idx[row['item_id']]\n",
    "                    ratings_dataset[user_idx, item_idx] = row['pseudo_rating']\n",
    "            \n",
    "            with h5py.File(sentiment_file, 'r+') as f:\n",
    "                sentiment_dataset = f['sentiment']\n",
    "                for _, row in chunk_aggregated.iterrows():\n",
    "                    user_idx = user_to_idx[row['user_id']]\n",
    "                    item_idx = item_to_idx[row['item_id']]\n",
    "                    sentiment_dataset[user_idx, item_idx] = row['sentimiento_numerico']\n",
    "            \n",
    "            with h5py.File(confidence_file, 'r+') as f:\n",
    "                confidence_dataset = f['confidence']\n",
    "                for _, row in chunk_aggregated.iterrows():\n",
    "                    user_idx = user_to_idx[row['user_id']]\n",
    "                    item_idx = item_to_idx[row['item_id']]\n",
    "                    confidence_dataset[user_idx, item_idx] = row['confianza']\n",
    "        \n",
    "        # Crear DataFrames pequeños de muestra para uso inmediato (top 1000 usuarios más activos)\n",
    "        print(\"Creando matrices de muestra para uso inmediato...\")\n",
    "        \n",
    "        user_activity = matched_reviews_df['user_id'].value_counts()\n",
    "        top_users = user_activity.head(1000).index.tolist()\n",
    "        \n",
    "        sample_reviews = matched_reviews_df[matched_reviews_df['user_id'].isin(top_users)]\n",
    "        \n",
    "        # Crear matrices de muestra en memoria\n",
    "        sample_user_item_matrix = sample_reviews.pivot_table(\n",
    "            index='user_id',\n",
    "            columns='item_id',\n",
    "            values='pseudo_rating',\n",
    "            aggfunc='mean',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        sample_user_item_sentiment = sample_reviews.pivot_table(\n",
    "            index='user_id',\n",
    "            columns='item_id',\n",
    "            values='sentimiento_numerico',\n",
    "            aggfunc='mean',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        sample_user_item_confidence = sample_reviews.pivot_table(\n",
    "            index='user_id',\n",
    "            columns='item_id',\n",
    "            values='confianza',\n",
    "            aggfunc='mean',\n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        print(f\"Matrices creadas y guardadas en {storage_dir}\")\n",
    "        print(f\"   Archivos principales:\")\n",
    "        print(f\"     - {ratings_file} (ratings completos)\")\n",
    "        print(f\"     - {sentiment_file} (sentimientos completos)\")  \n",
    "        print(f\"     - {confidence_file} (confianza completa)\")\n",
    "        print(f\"     - {mappings_file} (mapeos usuario-item)\")\n",
    "        print(f\"   Matrices de muestra en memoria: {sample_user_item_matrix.shape[0]} usuarios x {sample_user_item_matrix.shape[1]} items\")\n",
    "        \n",
    "        # Estadísticas\n",
    "        print(f\"Distribución de sentimientos:\")\n",
    "        sentiment_counts = matched_reviews_df['sentimiento'].value_counts()\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            print(f\"   {sentiment}: {count:,} ({count/len(matched_reviews_df)*100:.1f}%)\")\n",
    "        \n",
    "        return sample_user_item_matrix, sample_user_item_sentiment, sample_user_item_confidence\n",
    "    \n",
    "    def load_user_item_matrices_from_storage(self, storage_path=\"AI_Recomendador\", user_subset=None, item_subset=None):\n",
    "        \"\"\"\n",
    "        Carga matrices usuario-item desde storage en disco\n",
    "        \"\"\"\n",
    "        import h5py\n",
    "        import pickle\n",
    "        from pathlib import Path\n",
    "        \n",
    "        storage_dir = Path(storage_path)\n",
    "        \n",
    "        # Cargar mapeos\n",
    "        mappings_file = storage_dir / 'user_item_mappings.pkl'\n",
    "        with open(mappings_file, 'rb') as f:\n",
    "            mappings = pickle.load(f)\n",
    "        \n",
    "        user_to_idx = mappings['user_to_idx']\n",
    "        item_to_idx = mappings['item_to_idx']\n",
    "        \n",
    "        # Determinar subset de usuarios e items\n",
    "        if user_subset is None:\n",
    "            user_indices = list(range(len(mappings['users'])))\n",
    "        else:\n",
    "            user_indices = [user_to_idx[u] for u in user_subset if u in user_to_idx]\n",
    "        \n",
    "        if item_subset is None:\n",
    "            item_indices = list(range(len(mappings['items'])))\n",
    "        else:\n",
    "            item_indices = [item_to_idx[i] for i in item_subset if i in item_to_idx]\n",
    "        \n",
    "        print(f\"Cargando subset: {len(user_indices)} usuarios x {len(item_indices)} items\")\n",
    "        \n",
    "        # Cargar datos desde HDF5\n",
    "        ratings_file = storage_dir / 'user_item_ratings.h5'\n",
    "        \n",
    "        with h5py.File(ratings_file, 'r') as f:\n",
    "            ratings_data = f['ratings'][user_indices, :][:, item_indices]\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        user_labels = [mappings['users'][i] for i in user_indices]\n",
    "        item_labels = [mappings['items'][i] for i in item_indices]\n",
    "        \n",
    "        ratings_df = pd.DataFrame(ratings_data, index=user_labels, columns=item_labels)\n",
    "        \n",
    "    def prepare_training_data_for_deep_learning(self, matched_reviews_df, storage_path=\"AI_Recomendador\", sample_size=30000):\n",
    "        \"\"\"\n",
    "        Prepara datos de entrenamiento de manera eficiente para RTX 3050 Ti (4GB VRAM)\n",
    "        \"\"\"\n",
    "        print(f\"Preparando datos de entrenamiento optimizados para RTX 3050 Ti (muestra de {sample_size:,} reviews)...\")\n",
    "        \n",
    "        if matched_reviews_df.empty:\n",
    "            print(\"No hay reviews emparejadas disponibles\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Muestra estratificada más pequeña para 4GB VRAM\n",
    "        if len(matched_reviews_df) > sample_size:\n",
    "            # Priorizar usuarios más activos y reviews de alta confianza\n",
    "            high_confidence_df = matched_reviews_df[matched_reviews_df['confianza'] >= 0.7]\n",
    "            if len(high_confidence_df) >= sample_size:\n",
    "                sample_df = high_confidence_df.sample(sample_size).reset_index(drop=True)\n",
    "            else:\n",
    "                # Completar con reviews de menor confianza\n",
    "                remaining_needed = sample_size - len(high_confidence_df)\n",
    "                low_confidence_df = matched_reviews_df[matched_reviews_df['confianza'] < 0.7]\n",
    "                additional_sample = low_confidence_df.sample(min(len(low_confidence_df), remaining_needed))\n",
    "                sample_df = pd.concat([high_confidence_df, additional_sample]).reset_index(drop=True)\n",
    "        else:\n",
    "            sample_df = matched_reviews_df.copy()\n",
    "        \n",
    "        print(f\"Muestra final optimizada: {len(sample_df):,} reviews\")\n",
    "        \n",
    "        # Convertir sentimientos categóricos\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        \n",
    "        sample_df['sentimiento_numerico'] = sample_df['sentimiento'].map(sentiment_to_numeric)\n",
    "        \n",
    "        # Crear mapeos más pequeños para GPU\n",
    "        unique_users = sample_df['user_id'].unique()[:10000]  # Limitar usuarios para VRAM\n",
    "        sample_df = sample_df[sample_df['user_id'].isin(unique_users)]\n",
    "        \n",
    "        unique_users = sample_df['user_id'].unique()\n",
    "        unique_items = sample_df['item_id'].unique()\n",
    "        unique_cities = sample_df['ciudad'].unique()\n",
    "        \n",
    "        user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "        item_to_idx = {item: idx for idx, item in enumerate(unique_items)}\n",
    "        city_to_idx = {city: idx for idx, city in enumerate(unique_cities)}\n",
    "        \n",
    "        # Guardar mapeos para el modelo\n",
    "        from pathlib import Path\n",
    "        import pickle\n",
    "        \n",
    "        storage_dir = Path(storage_path)\n",
    "        storage_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        training_mappings = {\n",
    "            'user_to_idx': user_to_idx,\n",
    "            'item_to_idx': item_to_idx,\n",
    "            'city_to_idx': city_to_idx,\n",
    "            'idx_to_user': {v: k for k, v in user_to_idx.items()},\n",
    "            'idx_to_item': {v: k for k, v in item_to_idx.items()},\n",
    "            'idx_to_city': {v: k for k, v in city_to_idx.items()}\n",
    "        }\n",
    "        \n",
    "        mappings_file = storage_dir / 'training_mappings_gpu_optimized.pkl'\n",
    "        with open(mappings_file, 'wb') as f:\n",
    "            pickle.dump(training_mappings, f)\n",
    "        \n",
    "        print(f\"Mapeos de entrenamiento GPU optimizados guardados en {mappings_file}\")\n",
    "        print(f\"Dimensiones optimizadas: {len(unique_users):,} usuarios, {len(unique_items):,} items, {len(unique_cities)} ciudades\")\n",
    "        \n",
    "        return sample_df, training_mappings\n",
    "    \n",
    "    def save_processed_data_efficiently(self, processed_data, output_path, storage_path=\"AI_Recomendador\"):\n",
    "        \"\"\"\n",
    "        Guarda datos procesados de manera eficiente usando referencias a archivos en disco\n",
    "        \"\"\"\n",
    "        print(f\"Guardando datos procesados de manera eficiente...\")\n",
    "        \n",
    "        from pathlib import Path\n",
    "        import pickle\n",
    "        \n",
    "        # Crear estructura de datos optimizada\n",
    "        efficient_data = {\n",
    "            'city_features': processed_data['city_features'],\n",
    "            'matched_reviews_summary': {\n",
    "                'total_reviews': len(processed_data.get('matched_reviews', [])),\n",
    "                'total_users': processed_data.get('matched_reviews', pd.DataFrame()).get('user_id', pd.Series()).nunique(),\n",
    "                'total_items': processed_data.get('matched_reviews', pd.DataFrame()).get('item_id', pd.Series()).nunique(),\n",
    "                'cities': list(processed_data.get('matched_reviews', pd.DataFrame()).get('ciudad', pd.Series()).unique())\n",
    "            },\n",
    "            'storage_path': str(Path(storage_path).absolute()),\n",
    "            'matrix_files': {\n",
    "                'ratings': str(Path(storage_path) / 'user_item_ratings.h5'),\n",
    "                'sentiment': str(Path(storage_path) / 'user_item_sentiment.h5'), \n",
    "                'confidence': str(Path(storage_path) / 'user_item_confidence.h5'),\n",
    "                'mappings': str(Path(storage_path) / 'user_item_mappings.pkl')\n",
    "            },\n",
    "            'scalers': self.scalers,\n",
    "            'encoders': self.encoders,\n",
    "            'processing_metadata': {\n",
    "                'timestamp': pd.Timestamp.now(),\n",
    "                'total_activities_processed': len(processed_data.get('raw_datasets', {}).get('activities', [])),\n",
    "                'total_reviews_original': len(processed_data.get('raw_datasets', {}).get('reviews', [])),\n",
    "                'matching_success_rate': processed_data['matched_reviews_summary']['total_reviews'] / len(processed_data.get('raw_datasets', {}).get('reviews', [1])) if processed_data.get('raw_datasets', {}).get('reviews') is not None else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Guardar archivo principal (pequeño)\n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump(efficient_data, f)\n",
    "        \n",
    "        # Guardar muestra de matched_reviews para análisis rápido\n",
    "        if 'matched_reviews' in processed_data and not processed_data['matched_reviews'].empty:\n",
    "            sample_reviews = processed_data['matched_reviews'].sample(min(10000, len(processed_data['matched_reviews'])))\n",
    "            sample_file = Path(output_path).parent / 'matched_reviews_sample.pkl'\n",
    "            sample_reviews.to_pickle(sample_file)\n",
    "            efficient_data['sample_reviews_file'] = str(sample_file)\n",
    "        \n",
    "        print(f\"Datos guardados exitosamente!\")\n",
    "        print(f\"   Archivo principal: {output_path}\")\n",
    "        print(f\"   Matrices grandes en: {storage_path}\")\n",
    "        print(f\"   Espacio utilizado en disco: ~{self._estimate_storage_size(storage_path)} GB\")\n",
    "        \n",
    "        return efficient_data\n",
    "    \n",
    "    def _estimate_storage_size(self, storage_path):\n",
    "        \"\"\"\n",
    "        Estima el tamaño de almacenamiento utilizado\n",
    "        \"\"\"\n",
    "        from pathlib import Path\n",
    "        \n",
    "        total_size = 0\n",
    "        storage_dir = Path(storage_path)\n",
    "        \n",
    "        if storage_dir.exists():\n",
    "            for file_path in storage_dir.rglob('*'):\n",
    "                if file_path.is_file():\n",
    "                    total_size += file_path.stat().st_size\n",
    "        \n",
    "        return round(total_size / (1024**3), 2)  # Convert to GB\n",
    "    \n",
    "    def prepare_deep_learning_features(self, datasets):\n",
    "        \"\"\"\n",
    "        Prepara todas las features para el modelo de deep learning\n",
    "        \"\"\"\n",
    "        print(\"Preparando features para deep learning...\")\n",
    "        \n",
    "        # Validación inicial de datasets\n",
    "        print(\"Validando datasets...\")\n",
    "        if 'activities' not in datasets or datasets['activities'].empty:\n",
    "            raise ValueError(\"Dataset de actividades vacío o no encontrado\")\n",
    "        \n",
    "        if 'reviews' not in datasets or datasets['reviews'].empty:\n",
    "            raise ValueError(\"Dataset de reviews vacío o no encontrado\")\n",
    "        \n",
    "        print(f\"  Activities dataset: {len(datasets['activities'])} registros\")\n",
    "        print(f\"  Reviews dataset: {len(datasets['reviews'])} registros\")\n",
    "        \n",
    "        # Paso 1: Emparejar reviews con actividades\n",
    "        try:\n",
    "            matched_reviews = self.match_reviews_to_activities(\n",
    "                datasets['activities'], datasets['reviews']\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error en emparejamiento: {e}\")\n",
    "            print(\"Creando dataset vacío para continuar procesamiento...\")\n",
    "            matched_reviews = pd.DataFrame()\n",
    "        \n",
    "        # Paso 2: Crear user_ids sintéticos (solo si hay datos emparejados)\n",
    "        if not matched_reviews.empty:\n",
    "            try:\n",
    "                print(\"Generando user_ids sintéticos...\")\n",
    "                matched_reviews = self.create_synthetic_user_ids(matched_reviews)\n",
    "            except Exception as e:\n",
    "                print(f\"Error creando user_ids sintéticos: {e}\")\n",
    "                # Asignar user_ids simples como fallback\n",
    "                matched_reviews['user_id'] = range(len(matched_reviews))\n",
    "        else:\n",
    "            print(\"Sin datos emparejados, saltando generación de user_ids\")\n",
    "        \n",
    "        # Paso 3: Obtener datasets procesados\n",
    "        try:\n",
    "            activities = self.create_activity_features(datasets['activities'], matched_reviews)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creando features de actividades: {e}\")\n",
    "            activities = datasets['activities'].copy()\n",
    "        \n",
    "        try:\n",
    "            temporal_features = self.create_temporal_features(datasets['monthly_interest'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error creando features temporales: {e}\")\n",
    "            temporal_features = pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            geo_features = self.create_geographical_features(datasets['commuting_zones'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error creando features geográficas: {e}\")\n",
    "            geo_features = pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            tourism_context = self.create_tourism_context_features(datasets['un_tourism'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error creando features de turismo: {e}\")\n",
    "            tourism_context = {}\n",
    "        \n",
    "        # Paso 4: Crear features consolidadas por ciudad\n",
    "        city_features = {}\n",
    "        \n",
    "        cities = ['Barcelona', 'Madrid', 'Malaga', 'Sevilla', 'Valencia', \n",
    "                 'Tenerife', 'Gran Canaria', 'Palma de Mallorca']\n",
    "        \n",
    "        for city in cities:\n",
    "            print(f\"Procesando {city}...\")\n",
    "            \n",
    "            try:\n",
    "                # Features de actividades para la ciudad\n",
    "                city_activities = activities[activities['ciudad'] == city]\n",
    "                \n",
    "                # Features temporales\n",
    "                city_temporal = temporal_features[temporal_features['ciudad'] == city] if not temporal_features.empty else pd.DataFrame()\n",
    "                \n",
    "                # Features geográficas (buscar coincidencia por nombre)\n",
    "                city_geo = None\n",
    "                if not geo_features.empty:\n",
    "                    geo_matches = geo_features[geo_features['name'].str.contains(city, case=False, na=False)]\n",
    "                    city_geo = geo_matches.iloc[0] if len(geo_matches) > 0 else None\n",
    "                \n",
    "                # Features de reviews matched para la ciudad\n",
    "                city_reviews = matched_reviews[matched_reviews['ciudad'] == city] if not matched_reviews.empty else pd.DataFrame()\n",
    "                \n",
    "                # Análisis de correlación clima-sentimiento\n",
    "                try:\n",
    "                    climate_correlation = self._analyze_climate_sentiment_correlation(city_reviews)\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error en análisis clima-sentimiento para {city}: {e}\")\n",
    "                    climate_correlation = {'correlation': 0, 'climate_factors': {}}\n",
    "                \n",
    "                # Consolidar features\n",
    "                city_data = {\n",
    "                    'activities': city_activities,\n",
    "                    'temporal': city_temporal,\n",
    "                    'geo': city_geo,\n",
    "                    'tourism_context': tourism_context,\n",
    "                    'reviews': city_reviews,\n",
    "                    'climate_sentiment_correlation': climate_correlation\n",
    "                }\n",
    "                \n",
    "                city_features[city] = city_data\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    Error procesando {city}: {e}\")\n",
    "                # Crear estructura mínima para la ciudad\n",
    "                city_features[city] = {\n",
    "                    'activities': pd.DataFrame(),\n",
    "                    'temporal': pd.DataFrame(),\n",
    "                    'geo': None,\n",
    "                    'tourism_context': {},\n",
    "                    'reviews': pd.DataFrame(),\n",
    "                    'climate_sentiment_correlation': {'correlation': 0, 'climate_factors': {}}\n",
    "                }\n",
    "        \n",
    "        print(f\"Features preparadas para {len(city_features)} ciudades\")\n",
    "        if not matched_reviews.empty:\n",
    "            print(f\"{len(matched_reviews)} reviews emparejadas disponibles para entrenamiento\")\n",
    "        else:\n",
    "            print(\"Sin reviews emparejadas - el modelo entrenará solo con datos base\")\n",
    "        \n",
    "        return city_features, matched_reviews\n",
    "    \n",
    "    def _analyze_climate_sentiment_correlation(self, city_reviews):\n",
    "        \"\"\"\n",
    "        Analiza correlación entre clima y sentimiento categórico usando las 35 combinaciones específicas\n",
    "        \"\"\"\n",
    "        if city_reviews.empty or 'descripcion_sencilla' not in city_reviews.columns:\n",
    "            return {'correlation': 0, 'climate_factors': {}}\n",
    "        \n",
    "        # Convertir sentimiento categórico a numérico\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        \n",
    "        city_reviews['sentimiento_numerico'] = city_reviews['sentimiento'].map(sentiment_to_numeric)\n",
    "        \n",
    "        # Definir categorías principales basadas en las 35 combinaciones\n",
    "        def categorize_detailed_weather(description):\n",
    "            \"\"\"\n",
    "            Categoriza las 35 descripciones climáticas en grupos principales\n",
    "            \"\"\"\n",
    "            if pd.isna(description):\n",
    "                return 'Sin datos'\n",
    "            \n",
    "            desc = str(description).lower()\n",
    "            \n",
    "            # Categorías principales por condiciones del cielo\n",
    "            if 'soleado' in desc:\n",
    "                sky_condition = 'Soleado'\n",
    "            elif 'nublado' in desc:\n",
    "                sky_condition = 'Nublado'\n",
    "            elif 'intervalos' in desc:\n",
    "                sky_condition = 'Parcialmente nublado'\n",
    "            else:\n",
    "                sky_condition = 'Indefinido'\n",
    "            \n",
    "            # Condiciones de precipitación\n",
    "            if 'lluvia intensa' in desc:\n",
    "                precipitation = 'Lluvia intensa'\n",
    "            elif 'lluvia moderada' in desc:\n",
    "                precipitation = 'Lluvia moderada'\n",
    "            elif 'lloviznas' in desc:\n",
    "                precipitation = 'Lloviznas'\n",
    "            elif 'sin lluvia' in desc:\n",
    "                precipitation = 'Sin lluvia'\n",
    "            else:\n",
    "                precipitation = 'Sin especificar'\n",
    "            \n",
    "            # Condiciones de temperatura\n",
    "            if 'muy caluroso' in desc:\n",
    "                temperature = 'Muy caluroso'\n",
    "            elif 'cálido' in desc or 'calido' in desc:\n",
    "                temperature = 'Cálido'\n",
    "            elif 'fresco y agradable' in desc:\n",
    "                temperature = 'Fresco y agradable'\n",
    "            elif 'frío' in desc or 'frio' in desc:\n",
    "                temperature = 'Frío'\n",
    "            else:\n",
    "                temperature = 'Temperatura normal'\n",
    "            \n",
    "            # Viento\n",
    "            wind = 'Con viento' if 'viento' in desc else 'Sin viento especificado'\n",
    "            \n",
    "            return {\n",
    "                'sky_condition': sky_condition,\n",
    "                'precipitation': precipitation,\n",
    "                'temperature': temperature,\n",
    "                'wind': wind,\n",
    "                'full_description': description\n",
    "            }\n",
    "        \n",
    "        # Aplicar categorización detallada\n",
    "        city_reviews['weather_details'] = city_reviews['descripcion_sencilla'].apply(categorize_detailed_weather)\n",
    "        \n",
    "        # Extraer categorías principales\n",
    "        city_reviews['sky_condition'] = city_reviews['weather_details'].apply(lambda x: x.get('sky_condition', 'Sin datos') if isinstance(x, dict) else 'Sin datos')\n",
    "        city_reviews['precipitation'] = city_reviews['weather_details'].apply(lambda x: x.get('precipitation', 'Sin datos') if isinstance(x, dict) else 'Sin datos')\n",
    "        city_reviews['temperature'] = city_reviews['weather_details'].apply(lambda x: x.get('temperature', 'Sin datos') if isinstance(x, dict) else 'Sin datos')\n",
    "        \n",
    "        # Crear encoding numérico optimizado para las 35 combinaciones\n",
    "        def calculate_weather_score(weather_details):\n",
    "            \"\"\"\n",
    "            Calcula score numérico basado en la favorabilidad esperada para turismo\n",
    "            \"\"\"\n",
    "            if not isinstance(weather_details, dict):\n",
    "                return 0.5\n",
    "            \n",
    "            score = 0.5  # Base neutral\n",
    "            \n",
    "            # Factor cielo (40% del score)\n",
    "            sky_scores = {\n",
    "                'Soleado': 0.4,\n",
    "                'Parcialmente nublado': 0.25,\n",
    "                'Nublado': 0.1,\n",
    "                'Indefinido': 0.2\n",
    "            }\n",
    "            score += sky_scores.get(weather_details.get('sky_condition', ''), 0.2)\n",
    "            \n",
    "            # Factor precipitación (35% del score) - más importante\n",
    "            precip_scores = {\n",
    "                'Sin lluvia': 0.35,\n",
    "                'Lloviznas': 0.1,\n",
    "                'Lluvia moderada': -0.1,\n",
    "                'Lluvia intensa': -0.25,\n",
    "                'Sin especificar': 0.15\n",
    "            }\n",
    "            score += precip_scores.get(weather_details.get('precipitation', ''), 0.15)\n",
    "            \n",
    "            # Factor temperatura (20% del score)\n",
    "            temp_scores = {\n",
    "                'Fresco y agradable': 0.2,\n",
    "                'Cálido': 0.15,\n",
    "                'Temperatura normal': 0.1,\n",
    "                'Frío': 0.05,\n",
    "                'Muy caluroso': 0.05\n",
    "            }\n",
    "            score += temp_scores.get(weather_details.get('temperature', ''), 0.1)\n",
    "            \n",
    "            # Factor viento (5% del score)\n",
    "            if weather_details.get('wind', '') == 'Con viento':\n",
    "                score += 0.02  # Viento moderado puede ser agradable\n",
    "            \n",
    "            return max(0, min(1, score))  # Mantener en rango [0,1]\n",
    "        \n",
    "        city_reviews['weather_numeric'] = city_reviews['weather_details'].apply(calculate_weather_score)\n",
    "        \n",
    "        # Calcular correlación usando valores numéricos\n",
    "        correlation = city_reviews[['sentimiento_numerico', 'weather_numeric']].corr().iloc[0, 1]\n",
    "        correlation = correlation if not pd.isna(correlation) else 0\n",
    "        \n",
    "        # Análisis detallado por cada combinación específica\n",
    "        specific_combinations = {}\n",
    "        unique_descriptions = city_reviews['descripcion_sencilla'].value_counts()\n",
    "        \n",
    "        for description, count in unique_descriptions.items():\n",
    "            if count >= 3:  # Solo analizar combinaciones con suficientes datos\n",
    "                desc_reviews = city_reviews[city_reviews['descripcion_sencilla'] == description]\n",
    "                sentiment_dist = desc_reviews['sentimiento'].value_counts(normalize=True)\n",
    "                \n",
    "                specific_combinations[description] = {\n",
    "                    'total_reviews': count,\n",
    "                    'sentiment_distribution': sentiment_dist.to_dict(),\n",
    "                    'avg_confidence': desc_reviews['confianza'].mean(),\n",
    "                    'positive_ratio': sentiment_dist.get('positivo', 0),\n",
    "                    'negative_ratio': sentiment_dist.get('negativo', 0),\n",
    "                    'neutral_ratio': sentiment_dist.get('neutro', 0),\n",
    "                    'weather_score': desc_reviews['weather_numeric'].iloc[0],\n",
    "                    'dominant_sentiment': sentiment_dist.idxmax() if not sentiment_dist.empty else 'neutro'\n",
    "                }\n",
    "        \n",
    "        # Análisis por categorías principales\n",
    "        climate_factors = {}\n",
    "        \n",
    "        # Por condición del cielo\n",
    "        for sky_condition in city_reviews['sky_condition'].unique():\n",
    "            if sky_condition != 'Sin datos':\n",
    "                sky_reviews = city_reviews[city_reviews['sky_condition'] == sky_condition]\n",
    "                if not sky_reviews.empty:\n",
    "                    sentiment_dist = sky_reviews['sentimiento'].value_counts(normalize=True)\n",
    "                    climate_factors[f'sky_{sky_condition.lower().replace(\" \", \"_\")}'] = {\n",
    "                        'total_reviews': len(sky_reviews),\n",
    "                        'sentiment_distribution': sentiment_dist.to_dict(),\n",
    "                        'avg_confidence': sky_reviews['confianza'].mean(),\n",
    "                        'positive_ratio': sentiment_dist.get('positivo', 0),\n",
    "                        'negative_ratio': sentiment_dist.get('negativo', 0)\n",
    "                    }\n",
    "        \n",
    "        # Por precipitación\n",
    "        for precipitation in city_reviews['precipitation'].unique():\n",
    "            if precipitation != 'Sin datos':\n",
    "                precip_reviews = city_reviews[city_reviews['precipitation'] == precipitation]\n",
    "                if not precip_reviews.empty:\n",
    "                    sentiment_dist = precip_reviews['sentimiento'].value_counts(normalize=True)\n",
    "                    climate_factors[f'precip_{precipitation.lower().replace(\" \", \"_\")}'] = {\n",
    "                        'total_reviews': len(precip_reviews),\n",
    "                        'sentiment_distribution': sentiment_dist.to_dict(),\n",
    "                        'avg_confidence': precip_reviews['confianza'].mean(),\n",
    "                        'positive_ratio': sentiment_dist.get('positivo', 0),\n",
    "                        'negative_ratio': sentiment_dist.get('negativo', 0)\n",
    "                    }\n",
    "        \n",
    "        # Por temperatura\n",
    "        for temperature in city_reviews['temperature'].unique():\n",
    "            if temperature != 'Sin datos':\n",
    "                temp_reviews = city_reviews[city_reviews['temperature'] == temperature]\n",
    "                if not temp_reviews.empty:\n",
    "                    sentiment_dist = temp_reviews['sentimiento'].value_counts(normalize=True)\n",
    "                    climate_factors[f'temp_{temperature.lower().replace(\" \", \"_\")}'] = {\n",
    "                        'total_reviews': len(temp_reviews),\n",
    "                        'sentiment_distribution': sentiment_dist.to_dict(),\n",
    "                        'avg_confidence': temp_reviews['confianza'].mean(),\n",
    "                        'positive_ratio': sentiment_dist.get('positivo', 0),\n",
    "                        'negative_ratio': sentiment_dist.get('negativo', 0)\n",
    "                    }\n",
    "        \n",
    "        # Identificar combinaciones extremas (muy positivas o muy negativas)\n",
    "        extreme_combinations = {}\n",
    "        for desc, data in specific_combinations.items():\n",
    "            if data['total_reviews'] >= 5:  # Solo combinaciones con datos suficientes\n",
    "                pos_ratio = data['positive_ratio']\n",
    "                neg_ratio = data['negative_ratio']\n",
    "                \n",
    "                if pos_ratio > 0.7:\n",
    "                    extreme_combinations[desc] = {'type': 'very_positive', 'ratio': pos_ratio}\n",
    "                elif neg_ratio > 0.5:\n",
    "                    extreme_combinations[desc] = {'type': 'very_negative', 'ratio': neg_ratio}\n",
    "                elif pos_ratio > 0.5 and 'lluvia' in desc.lower():\n",
    "                    extreme_combinations[desc] = {'type': 'rain_resilient', 'ratio': pos_ratio}\n",
    "        \n",
    "        return {\n",
    "            'correlation': correlation,\n",
    "            'climate_factors': climate_factors,\n",
    "            'specific_combinations': specific_combinations,\n",
    "            'extreme_combinations': extreme_combinations,\n",
    "            'weather_impact_strength': abs(correlation),\n",
    "            'sentiment_distribution_overall': city_reviews['sentimiento'].value_counts(normalize=True).to_dict(),\n",
    "            'total_unique_weather_conditions': len(unique_descriptions),\n",
    "            'weather_diversity_score': len(unique_descriptions) / len(city_reviews) if len(city_reviews) > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def save_processed_data(self, processed_data, output_path):\n",
    "        \"\"\"\n",
    "        Guarda datos procesados\n",
    "        \"\"\"\n",
    "        print(f\"Guardando datos procesados en {output_path}\")\n",
    "        \n",
    "        with open(output_path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'processed_data': processed_data,\n",
    "                'scalers': self.scalers,\n",
    "                'encoders': self.encoders\n",
    "            }, f)\n",
    "        \n",
    "        print(\"Datos guardados exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6aca538-7935-4b24-ade9-77bd37761228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INICIANDO PROCESAMIENTO OPTIMIZADO PARA DATASETS GRANDES\n",
      "============================================================\n",
      "\n",
      "1. Cargando datasets...\n",
      "Cargando datasets principales...\n",
      "\n",
      "2. Procesando features y emparejando reviews...\n",
      "Preparando features para deep learning...\n",
      "Validando datasets...\n",
      "  Activities dataset: 32669 registros\n",
      "  Reviews dataset: 855745 registros\n",
      "Emparejando reviews con actividades...\n",
      "Normalizando nombres de ciudades...\n",
      "Ciudades después de normalización:\n",
      "  Activities: ['Barcelona', 'Gran Canaria', 'Madrid', 'Malaga', 'Mallorca', 'Sevilla', 'Tenerife', 'Valencia']\n",
      "  Sentiment: ['Barcelona', 'Gran Canaria', 'Madrid', 'Malaga', 'Mallorca', 'Sevilla', 'Tenerife', 'Valencia']\n",
      "Procesando 32669 actividades...\n",
      "Progreso: 0/32669 (0.0%) - Matches: 0\n",
      "Progreso: 1000/32669 (3.1%) - Matches: 4118\n",
      "Progreso: 2000/32669 (6.1%) - Matches: 9329\n",
      "Progreso: 3000/32669 (9.2%) - Matches: 13625\n",
      "Progreso: 4000/32669 (12.2%) - Matches: 22823\n",
      "Progreso: 5000/32669 (15.3%) - Matches: 26761\n",
      "Progreso: 6000/32669 (18.4%) - Matches: 28235\n",
      "Progreso: 7000/32669 (21.4%) - Matches: 31851\n",
      "Progreso: 8000/32669 (24.5%) - Matches: 37226\n",
      "Progreso: 9000/32669 (27.5%) - Matches: 53802\n",
      "Progreso: 10000/32669 (30.6%) - Matches: 56727\n",
      "Progreso: 11000/32669 (33.7%) - Matches: 63426\n",
      "Progreso: 12000/32669 (36.7%) - Matches: 63607\n",
      "Progreso: 13000/32669 (39.8%) - Matches: 179950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 30\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m2. Procesando features y emparejando reviews...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m processed_features, matched_reviews \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mprepare_deep_learning_features(datasets)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m matched_reviews\u001b[38;5;241m.\u001b[39mempty:\n",
      "Cell \u001b[1;32mIn[2], line 1183\u001b[0m, in \u001b[0;36mTourismDataPreprocessor.prepare_deep_learning_features\u001b[1;34m(self, datasets)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1183\u001b[0m     matched_reviews \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatch_reviews_to_activities(\n\u001b[0;32m   1184\u001b[0m         datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivities\u001b[39m\u001b[38;5;124m'\u001b[39m], datasets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m   1185\u001b[0m     )\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[2], line 136\u001b[0m, in \u001b[0;36mTourismDataPreprocessor.match_reviews_to_activities\u001b[1;34m(self, activities_df, sentiment_df)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Buscar reviews que contengan el título (búsqueda simple)\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     matching_reviews_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 136\u001b[0m         sentiment_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtexto\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(activity_title_clean, na\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m    137\u001b[0m         (sentiment_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mciudad\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m activity_city)\n\u001b[0;32m    138\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     matching_reviews_subset \u001b[38;5;241m=\u001b[39m sentiment_df[matching_reviews_mask]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:137\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\accessor.py:3194\u001b[0m, in \u001b[0;36mStringMethods.lower\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3191\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcasemethods\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m%\u001b[39m _doc_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   3192\u001b[0m \u001b[38;5;129m@forbid_nonstring_types\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   3193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlower\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 3194\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39m_str_lower()\n\u001b[0;32m   3195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_result(result)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:444\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_lower\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_str_lower\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_str_map(\u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mlower)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\strings\\object_array.py:76\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     75\u001b[0m mask \u001b[38;5;241m=\u001b[39m isna(arr)\n\u001b[1;32m---> 76\u001b[0m map_convert \u001b[38;5;241m=\u001b[39m convert \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(mask)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2416\u001b[0m, in \u001b[0;36m_all_dispatcher\u001b[1;34m(a, axis, out, keepdims, where)\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapreduction(a, np\u001b[38;5;241m.\u001b[39mlogical_or, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m'\u001b[39m, axis, \u001b[38;5;28;01mNone\u001b[39;00m, out,\n\u001b[0;32m   2413\u001b[0m                           keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m-> 2416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_all_dispatcher\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   2417\u001b[0m                     where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:2091\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2088\u001b[0m     msg \u001b[38;5;241m=\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exception_only(etype, value)\n\u001b[0;32m   2089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(msg)\n\u001b[1;32m-> 2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshowtraceback\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tb_offset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2092\u001b[0m                   exception_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, running_compiled_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display the exception that just occurred.\u001b[39;00m\n\u001b[0;32m   2094\u001b[0m \n\u001b[0;32m   2095\u001b[0m \u001b[38;5;124;03m    If nothing is known about the exception, this is the method which\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;124;03m    SyntaxError exception, don't try to analyze the stack manually and\u001b[39;00m\n\u001b[0;32m   2102\u001b[0m \u001b[38;5;124;03m    simply call this method.\"\"\"\u001b[39;00m\n\u001b[0;32m   2104\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Definir rutas de archivos\n",
    "    file_paths = {\n",
    "        'activities': ['atracciones_civitatis_procesado.csv', 'booking_atracciones_limpios.csv', 'booking_hoteles_limpio.csv', \n",
    "                      'detripadvisor_procesado.csv', 'getyourguide_procesado.csv'],\n",
    "        'reviews': 'comentarios_final_definitivo_con_descripcion.csv',\n",
    "        'un_tourism': ['inbound_arrivals_by_region.csv', 'inbound_arrivals.csv', 'inbound_expenditure.csv',\n",
    "                      'outbound_departures.csv', 'outbound_expenditure.csv', 'tourism_domestic_trips.csv'],\n",
    "        'commuting_zones': 'data-for-good-at-meta-commuting-zones-march-2023-ciudades.csv',\n",
    "        'movement_data': 'movements_spain_cities_all.csv',\n",
    "        'search_trends': 'busquedas_relacionadas_actuales.csv',\n",
    "        'monthly_interest': 'interes_turistico_mensual_por_ciudad.csv'\n",
    "    }\n",
    "    \n",
    "    # Configuración para datasets grandes\n",
    "    STORAGE_PATH = \"E:\\AI Recomendador\"\n",
    "    \n",
    "    # Inicializar preprocessor\n",
    "    preprocessor = TourismDataPreprocessor()\n",
    "    \n",
    "    print(\"INICIANDO PROCESAMIENTO OPTIMIZADO PARA DATASETS GRANDES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Cargar datasets\n",
    "    print(\"\\n1. Cargando datasets...\")\n",
    "    datasets = preprocessor.load_and_clean_datasets(file_paths)\n",
    "    \n",
    "    # Procesar features (incluye emparejamiento optimizado)\n",
    "    print(\"\\n2. Procesando features y emparejando reviews...\")\n",
    "    processed_features, matched_reviews = preprocessor.prepare_deep_learning_features(datasets)\n",
    "    \n",
    "    if not matched_reviews.empty:\n",
    "        print(f\"\\n3. Emparejamiento exitoso: {len(matched_reviews):,} reviews emparejadas\")\n",
    "        \n",
    "        # Crear matrices usuario-item usando storage en disco\n",
    "        print(\"\\n4. Creando matrices usuario-item (usando storage en disco)...\")\n",
    "        sample_matrices = preprocessor.create_user_item_matrix(\n",
    "            matched_reviews, datasets['activities'], storage_path=STORAGE_PATH\n",
    "        )\n",
    "        \n",
    "        # Preparar datos de entrenamiento eficientemente\n",
    "        print(\"\\n5. Preparando datos de entrenamiento...\")\n",
    "        training_sample, training_mappings = preprocessor.prepare_training_data_for_deep_learning(\n",
    "            matched_reviews, storage_path=STORAGE_PATH, sample_size=100000\n",
    "        )\n",
    "        \n",
    "        # Guardar todo de manera eficiente\n",
    "        print(\"\\n6. Guardando datos procesados...\")\n",
    "        all_processed_data = {\n",
    "            'city_features': processed_features,\n",
    "            'user_item_matrix': sample_matrices[0],  # Solo la muestra en memoria\n",
    "            'user_sentiment_matrix': sample_matrices[1],\n",
    "            'user_confidence_matrix': sample_matrices[2],\n",
    "            'matched_reviews': matched_reviews,\n",
    "            'training_sample': training_sample,\n",
    "            'training_mappings': training_mappings,\n",
    "            'raw_datasets': datasets\n",
    "        }\n",
    "        \n",
    "        efficient_data = preprocessor.save_processed_data_efficiently(\n",
    "            all_processed_data, 'processed_tourism_data_efficient.pkl', STORAGE_PATH\n",
    "        )\n",
    "        \n",
    "        print(\"\\nPROCESAMIENTO COMPLETADO EXITOSAMENTE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Archivos principales:\")\n",
    "        print(f\"   • processed_tourism_data_efficient.pkl (metadatos y referencias)\")\n",
    "        print(f\"   • {STORAGE_PATH}/ (matrices completas en HDF5)\")\n",
    "        print(f\"   • matched_reviews_sample.pkl (muestra para análisis rápido)\")\n",
    "        \n",
    "        print(f\"\\nEstadísticas finales:\")\n",
    "        print(f\"   • Reviews emparejadas: {len(matched_reviews):,}\")\n",
    "        print(f\"   • Usuarios sintéticos: {matched_reviews['user_id'].nunique():,}\")\n",
    "        print(f\"   • Actividades cubiertas: {matched_reviews['item_id'].nunique():,}\")\n",
    "        print(f\"   • Ciudades procesadas: {len(processed_features)}\")\n",
    "        print(f\"   • Storage utilizado: ~{preprocessor._estimate_storage_size(STORAGE_PATH)} GB\")\n",
    "        \n",
    "        print(f\"\\nPara cargar matrices específicas posteriormente:\")\n",
    "        print(f\"   subset_matrix = preprocessor.load_user_item_matrices_from_storage(\")\n",
    "        print(f\"       storage_path='{STORAGE_PATH}',\")\n",
    "        print(f\"       user_subset=['user_1', 'user_2', ...],\")\n",
    "        print(f\"       item_subset=['item_1', 'item_2', ...]\")\n",
    "        print(f\"   )\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNo se encontraron emparejamientos. Revisar datos de entrada.\")\n",
    "        \n",
    "        # Guardar datos básicos sin matrices\n",
    "        basic_processed_data = {\n",
    "            'city_features': processed_features,\n",
    "            'user_item_matrix': pd.DataFrame(),\n",
    "            'user_sentiment_matrix': pd.DataFrame(),\n",
    "            'user_confidence_matrix': pd.DataFrame(),\n",
    "            'matched_reviews': pd.DataFrame(),\n",
    "            'raw_datasets': datasets\n",
    "        }\n",
    "        \n",
    "        preprocessor.save_processed_data(basic_processed_data, 'processed_tourism_data_basic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf935b-44c2-4c43-9e00-c592dcff9989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
