{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e93673b6-b555-4afe-abf3-3cfc54858bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df62801c-030f-4b11-b207-2e88e5165aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        physical_devices[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=3584)]  # Limitar a 3.5GB\n",
    "    )\n",
    "    print(\"GPU configurada para RTX 3050 Ti (3.5GB VRAM limit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64b8e4c4-c1e1-4a9b-b51e-4bdc4098b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourismRecommenderModel:\n",
    "    def __init__(self, embedding_dim=32, dense_units=64):\n",
    "        \"\"\"\n",
    "        Modelo optimizado para RTX 3050 Ti - dimensiones reducidas\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dense_units = dense_units\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "    def build_hybrid_model(self, num_users, num_items, num_cities, \n",
    "                          contextual_features_dim=10, weather_features_dim=15):\n",
    "        \"\"\"\n",
    "        Modelo híbrido compacto optimizado para 4GB VRAM\n",
    "        \"\"\"\n",
    "        print(\"Construyendo modelo híbrido compacto para RTX 3050 Ti...\")\n",
    "        \n",
    "        # === ENTRADAS REDUCIDAS ===\n",
    "        user_input = layers.Input(shape=(), name='user_id')\n",
    "        item_input = layers.Input(shape=(), name='item_id')\n",
    "        city_input = layers.Input(shape=(), name='city_id')\n",
    "        \n",
    "        # Features reducidas para VRAM\n",
    "        contextual_input = layers.Input(shape=(contextual_features_dim,), name='contextual_features')\n",
    "        temporal_input = layers.Input(shape=(4,), name='temporal_features')  # Solo estación del año\n",
    "        weather_sentiment_input = layers.Input(shape=(weather_features_dim,), name='weather_sentiment')\n",
    "        \n",
    "        # === EMBEDDINGS COMPACTOS ===\n",
    "        user_embedding = layers.Embedding(\n",
    "            num_users + 1, self.embedding_dim, \n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)  # Regularización reducida\n",
    "        )(user_input)\n",
    "        user_vec = layers.Flatten()(user_embedding)\n",
    "        \n",
    "        item_embedding = layers.Embedding(\n",
    "            num_items + 1, self.embedding_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)\n",
    "        )(item_input)\n",
    "        item_vec = layers.Flatten()(item_embedding)\n",
    "        \n",
    "        city_embedding = layers.Embedding(\n",
    "            num_cities + 1, self.embedding_dim // 2,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6)\n",
    "        )(city_input)\n",
    "        city_vec = layers.Flatten()(city_embedding)\n",
    "        \n",
    "        # === PROCESAMIENTO COMPACTO ===\n",
    "        # Red contextual simplificada\n",
    "        contextual_dense = layers.Dense(self.dense_units//2, activation='relu')(contextual_input)\n",
    "        contextual_dense = layers.Dropout(0.2)(contextual_dense)\n",
    "        \n",
    "        # Red temporal simplificada\n",
    "        temporal_dense = layers.Dense(16, activation='relu')(temporal_input)\n",
    "        \n",
    "        # Red climática compacta pero especializada\n",
    "        weather_dense = layers.Dense(32, activation='relu', name='climate_layer')(weather_sentiment_input)\n",
    "        weather_dense = layers.Dropout(0.2)(weather_dense)\n",
    "        \n",
    "        # === COMBINACIÓN EFICIENTE ===\n",
    "        combined = layers.Concatenate(name='feature_fusion')([\n",
    "            user_vec, item_vec, city_vec,\n",
    "            contextual_dense, temporal_dense, weather_dense\n",
    "        ])\n",
    "        \n",
    "        # === RED COMPACTA ===\n",
    "        x = layers.Dense(self.dense_units, activation='relu', name='main_layer')(combined)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(self.dense_units//2, activation='relu', name='output_layer')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        # === SALIDAS SIMPLIFICADAS ===\n",
    "        rating_output = layers.Dense(1, activation='sigmoid', name='rating')(x)\n",
    "        sentiment_output = layers.Dense(1, activation='sigmoid', name='sentiment')(x)\n",
    "        interaction_output = layers.Dense(1, activation='sigmoid', name='interaction')(x)\n",
    "        \n",
    "        # === MODELO FINAL ===\n",
    "        self.model = Model(\n",
    "            inputs=[user_input, item_input, city_input, \n",
    "                   contextual_input, temporal_input, weather_sentiment_input],\n",
    "            outputs=[rating_output, sentiment_output, interaction_output],\n",
    "            name='TourismRecommenderCompact'\n",
    "        )\n",
    "        \n",
    "        # Optimizer optimizado para GPU\n",
    "        optimizer = keras.optimizers.Adam(\n",
    "            learning_rate=0.002,  # Learning rate más alto para convergencia rápida\n",
    "            epsilon=1e-7\n",
    "        )\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss={\n",
    "                'rating': 'mse',\n",
    "                'sentiment': 'mse', \n",
    "                'interaction': 'binary_crossentropy'\n",
    "            },\n",
    "            loss_weights={\n",
    "                'rating': 1.0,\n",
    "                'sentiment': 0.7,\n",
    "                'interaction': 0.3\n",
    "            },\n",
    "            metrics={\n",
    "                'rating': ['mae'],\n",
    "                'sentiment': ['mae'],\n",
    "                'interaction': ['accuracy']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        total_params = self.model.count_params()\n",
    "        print(f\"Modelo compacto construido: {total_params:,} parámetros\")\n",
    "        print(f\"Memoria estimada: ~{total_params * 4 / (1024**2):.1f}MB\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def prepare_training_data(self, processed_data):\n",
    "        \"\"\"\n",
    "        Preparación optimizada para GPU rápida\n",
    "        \"\"\"\n",
    "        print(\"Preparando datos de entrenamiento optimizados para GPU...\")\n",
    "        \n",
    "        # Usar datos ya optimizados del preprocessor\n",
    "        matched_reviews_df = processed_data.get('training_sample', pd.DataFrame())\n",
    "        training_mappings = processed_data.get('training_mappings', {})\n",
    "        \n",
    "        if matched_reviews_df.empty:\n",
    "            raise ValueError(\"No hay datos de entrenamiento preparados\")\n",
    "        \n",
    "        self.user_to_idx = training_mappings['user_to_idx']\n",
    "        self.item_to_idx = training_mappings['item_to_idx'] \n",
    "        self.city_to_idx = training_mappings['city_to_idx']\n",
    "        \n",
    "        # Crear dataset de entrenamiento compacto\n",
    "        training_data = []\n",
    "        \n",
    "        print(f\"Procesando {len(matched_reviews_df)} reviews para entrenamiento...\")\n",
    "        \n",
    "        for idx, row in matched_reviews_df.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"Progreso: {idx}/{len(matched_reviews_df)}\")\n",
    "            \n",
    "            user_idx = self.user_to_idx.get(row['user_id'], 0)\n",
    "            item_idx = self.item_to_idx.get(row['item_id'], 0)\n",
    "            city_idx = self.city_to_idx.get(row['ciudad'], 0)\n",
    "            \n",
    "            # Features simplificadas para velocidad\n",
    "            contextual_features = self._extract_contextual_features_fast(row)\n",
    "            temporal_features = self._extract_temporal_features_fast(row)\n",
    "            weather_sentiment_features = self._extract_weather_sentiment_features_fast(row)\n",
    "            \n",
    "            # Targets optimizados\n",
    "            sentiment_cat = row['sentimiento']\n",
    "            if sentiment_cat == 'negativo':\n",
    "                rating_target = 0.25\n",
    "                sentiment_target = 0.0\n",
    "            elif sentiment_cat == 'positivo':\n",
    "                rating_target = 0.875\n",
    "                sentiment_target = 1.0\n",
    "            else:  # neutro\n",
    "                rating_target = 0.6\n",
    "                sentiment_target = 0.5\n",
    "            \n",
    "            training_data.append({\n",
    "                'user_id': user_idx,\n",
    "                'item_id': item_idx,\n",
    "                'city_id': city_idx,\n",
    "                'contextual_features': contextual_features,\n",
    "                'temporal_features': temporal_features,\n",
    "                'weather_sentiment': weather_sentiment_features,\n",
    "                'rating': rating_target,\n",
    "                'sentiment': sentiment_target,\n",
    "                'interaction': 1.0,\n",
    "                'sample_weight': row['confianza']\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(training_data)\n",
    "    \n",
    "    def _extract_contextual_features_fast(self, review_row):\n",
    "        \"\"\"\n",
    "        Extracción rápida de features contextuales (solo 10 features)\n",
    "        \"\"\"\n",
    "        return [\n",
    "            0.5,  # Precio normalizado (placeholder)\n",
    "            0.7,  # Rating normalizado (placeholder)\n",
    "            0.0,  # Sentiment promedio (placeholder)\n",
    "            1.0,  # Review count normalizado (placeholder)\n",
    "            0.8,  # Confianza promedio (placeholder)\n",
    "            0.5,  # Factor geográfico (placeholder)\n",
    "            1.0,  # Factor turístico (placeholder)\n",
    "            0.6,  # Correlación clima (placeholder)\n",
    "            0.8,  # Factor fuente (placeholder)\n",
    "            0.5   # Factor general (placeholder)\n",
    "        ]\n",
    "    \n",
    "    def _extract_temporal_features_fast(self, review_row):\n",
    "        \"\"\"\n",
    "        Features temporales mínimas (solo estación)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            date = pd.to_datetime(review_row.get('fecha'), errors='coerce')\n",
    "            if pd.isna(date):\n",
    "                return [0.25, 0.25, 0.25, 0.25]  # Distribución uniforme\n",
    "            \n",
    "            month = date.month\n",
    "            if month in [12, 1, 2]:\n",
    "                return [1, 0, 0, 0]  # Invierno\n",
    "            elif month in [3, 4, 5]:\n",
    "                return [0, 1, 0, 0]  # Primavera\n",
    "            elif month in [6, 7, 8]:\n",
    "                return [0, 0, 1, 0]  # Verano\n",
    "            else:\n",
    "                return [0, 0, 0, 1]  # Otoño\n",
    "        except:\n",
    "            return [0.25, 0.25, 0.25, 0.25]\n",
    "    \n",
    "    def _extract_weather_sentiment_features_fast(self, review_row):\n",
    "        \"\"\"\n",
    "        Features climáticas compactas (15 features)\n",
    "        \"\"\"\n",
    "        sentiment_numeric = {'negativo': -1, 'neutro': 0, 'positivo': 1}.get(\n",
    "            review_row.get('sentimiento', 'neutro'), 0\n",
    "        )\n",
    "        \n",
    "        weather_desc = str(review_row.get('descripcion_sencilla', '')).lower()\n",
    "        \n",
    "        features = [\n",
    "            sentiment_numeric,  # Sentimiento\n",
    "            review_row.get('confianza', 0.5),  # Confianza\n",
    "            1 if 'soleado' in weather_desc else 0,  # Soleado\n",
    "            1 if 'nublado' in weather_desc else 0,  # Nublado\n",
    "            1 if 'lluvia' in weather_desc else 0,  # Lluvia\n",
    "            1 if 'calido' in weather_desc or 'cálido' in weather_desc else 0,  # Cálido\n",
    "            1 if 'frio' in weather_desc or 'frío' in weather_desc else 0,  # Frío\n",
    "            1 if 'agradable' in weather_desc else 0,  # Agradable\n",
    "            0.8 if 'soleado' in weather_desc else 0.5,  # Score climático\n",
    "            1 if sentiment_numeric > 0 and 'lluvia' in weather_desc else 0,  # Resistencia lluvia\n",
    "            0.7,  # Placeholder 1\n",
    "            0.5,  # Placeholder 2\n",
    "            0.6,  # Placeholder 3\n",
    "            0.4,  # Placeholder 4\n",
    "            0.8   # Placeholder 5\n",
    "        ]\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train_model(self, training_data, validation_split=0.2, epochs=30, batch_size=128):\n",
    "        \"\"\"\n",
    "        Entrenamiento optimizado para RTX 3050 Ti\n",
    "        \"\"\"\n",
    "        print(\"Iniciando entrenamiento optimizado para RTX 3050 Ti...\")\n",
    "        \n",
    "        # Preparar datos\n",
    "        X = {\n",
    "            'user_id': training_data['user_id'].values.astype('int32'),\n",
    "            'item_id': training_data['item_id'].values.astype('int32'),\n",
    "            'city_id': training_data['city_id'].values.astype('int32'),\n",
    "            'contextual_features': np.array(training_data['contextual_features'].tolist(), dtype='float32'),\n",
    "            'temporal_features': np.array(training_data['temporal_features'].tolist(), dtype='float32'),\n",
    "            'weather_sentiment': np.array(training_data['weather_sentiment'].tolist(), dtype='float32')\n",
    "        }\n",
    "        \n",
    "        y = {\n",
    "            'rating': training_data['rating'].values.astype('float32'),\n",
    "            'sentiment': training_data['sentiment'].values.astype('float32'),\n",
    "            'interaction': training_data['interaction'].values.astype('float32')\n",
    "        }\n",
    "        \n",
    "        sample_weights = training_data['sample_weight'].values.astype('float32')\n",
    "        \n",
    "        # Callbacks optimizados para velocidad\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=5, restore_best_weights=True, verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.7, patience=3, min_lr=1e-6, verbose=1\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                'best_tourism_model_compact.h5', save_best_only=True, monitor='val_loss', verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"Configuración de entrenamiento:\")\n",
    "        print(f\"  - Batch size: {batch_size} (optimizado para 4GB VRAM)\")\n",
    "        print(f\"  - Epochs máximos: {epochs}\")\n",
    "        print(f\"  - Samples: {len(training_data):,}\")\n",
    "        print(f\"  - Validation split: {validation_split}\")\n",
    "        \n",
    "        # Entrenar con configuración optimizada\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            sample_weight=sample_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1,\n",
    "            workers=4,  # Usar múltiples cores del i5-12500H\n",
    "            use_multiprocessing=True\n",
    "        )\n",
    "        \n",
    "        print(\"Entrenamiento completado!\")\n",
    "        \n",
    "        # Mostrar mejor performance\n",
    "        best_epoch = np.argmin(self.history.history['val_loss'])\n",
    "        best_val_loss = min(self.history.history['val_loss'])\n",
    "        print(f\"Mejor época: {best_epoch + 1}\")\n",
    "        print(f\"Mejor val_loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "        \n",
    "    def build_hybrid_model(self, num_users, num_items, num_cities, \n",
    "                          contextual_features_dim, weather_features_dim=25):\n",
    "        \"\"\"\n",
    "        Construye modelo híbrido con embeddings contextuales y usuario-item\n",
    "        Actualizado para 25 features de clima específicas\n",
    "        \"\"\"\n",
    "        print(\"Construyendo modelo híbrido...\")\n",
    "        \n",
    "        # === ENTRADAS ===\n",
    "        # Usuario e Item embeddings\n",
    "        user_input = layers.Input(shape=(), name='user_id')\n",
    "        item_input = layers.Input(shape=(), name='item_id')\n",
    "        city_input = layers.Input(shape=(), name='city_id')\n",
    "        \n",
    "        # Features contextuales\n",
    "        contextual_input = layers.Input(shape=(contextual_features_dim,), name='contextual_features')\n",
    "        \n",
    "        # Features temporales\n",
    "        temporal_input = layers.Input(shape=(12,), name='temporal_features')  # 12 meses\n",
    "        \n",
    "        # Features de clima y sentimiento (expandido a 25 features)\n",
    "        weather_sentiment_input = layers.Input(shape=(weather_features_dim,), name='weather_sentiment')\n",
    "        \n",
    "        # === EMBEDDINGS ===\n",
    "        # User embeddings\n",
    "        user_embedding = layers.Embedding(\n",
    "            num_users + 1, self.embedding_dim, \n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-5)\n",
    "        )(user_input)\n",
    "        user_vec = layers.Flatten()(user_embedding)\n",
    "        \n",
    "        # Item embeddings\n",
    "        item_embedding = layers.Embedding(\n",
    "            num_items + 1, self.embedding_dim,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-5)\n",
    "        )(item_input)\n",
    "        item_vec = layers.Flatten()(item_embedding)\n",
    "        \n",
    "        # City embeddings\n",
    "        city_embedding = layers.Embedding(\n",
    "            num_cities + 1, self.embedding_dim // 2,\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-5)\n",
    "        )(city_input)\n",
    "        city_vec = layers.Flatten()(city_embedding)\n",
    "        \n",
    "        # === PROCESAMIENTO DE FEATURES CONTEXTUALES ===\n",
    "        # Red para features contextuales (turismo, geo, etc.)\n",
    "        contextual_dense = layers.Dense(self.dense_units, activation='relu')(contextual_input)\n",
    "        contextual_dense = layers.Dropout(0.3)(contextual_dense)\n",
    "        contextual_dense = layers.Dense(self.dense_units // 2, activation='relu')(contextual_dense)\n",
    "        \n",
    "        # Red para features temporales\n",
    "        temporal_dense = layers.Dense(64, activation='relu')(temporal_input)\n",
    "        temporal_dense = layers.Dropout(0.2)(temporal_dense)\n",
    "        temporal_dense = layers.Dense(32, activation='relu')(temporal_dense)\n",
    "        \n",
    "        # Red especializada para las 25 features de clima-sentimiento\n",
    "        # Esta es tu ventaja competitiva - red dedicada para climate intelligence\n",
    "        weather_dense_1 = layers.Dense(128, activation='relu', name='climate_layer_1')(weather_sentiment_input)\n",
    "        weather_dense_1 = layers.BatchNormalization()(weather_dense_1)\n",
    "        weather_dense_1 = layers.Dropout(0.3)(weather_dense_1)\n",
    "        \n",
    "        weather_dense_2 = layers.Dense(64, activation='relu', name='climate_layer_2')(weather_dense_1)\n",
    "        weather_dense_2 = layers.Dropout(0.2)(weather_dense_2)\n",
    "        \n",
    "        # Capa especializada para patrones climáticos complejos\n",
    "        weather_patterns = layers.Dense(32, activation='relu', name='climate_patterns')(weather_dense_2)\n",
    "        \n",
    "        # === COMBINACIÓN DE FEATURES ===\n",
    "        # Concatenar todas las representaciones\n",
    "        combined = layers.Concatenate(name='feature_fusion')([\n",
    "            user_vec, item_vec, city_vec,\n",
    "            contextual_dense, temporal_dense, weather_patterns\n",
    "        ])\n",
    "        \n",
    "        # === RED PROFUNDA ESPECIALIZADA ===\n",
    "        # Capas densas para aprendizaje de patrones complejos\n",
    "        x = layers.Dense(self.dense_units * 2, activation='relu', name='deep_layer_1')(combined)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.4)(x)\n",
    "        \n",
    "        # Capa de atención para features climáticas (tu innovación)\n",
    "        weather_attention = layers.Dense(weather_patterns.shape[-1], activation='sigmoid', name='weather_attention')(x)\n",
    "        attended_weather = layers.Multiply(name='climate_attention')([weather_patterns, weather_attention])\n",
    "        \n",
    "        # Combinar con el resto de features\n",
    "        x_with_climate = layers.Concatenate(name='climate_enhanced_features')([x, attended_weather])\n",
    "        \n",
    "        x = layers.Dense(self.dense_units, activation='relu', name='deep_layer_2')(x_with_climate)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Dense(self.dense_units // 2, activation='relu', name='deep_layer_3')(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        \n",
    "        x = layers.Dense(64, activation='relu', name='final_dense')(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        \n",
    "        # === SALIDAS MÚLTIPLES ===\n",
    "        # Predicción de rating\n",
    "        rating_output = layers.Dense(1, activation='sigmoid', name='rating')(x)\n",
    "        \n",
    "        # Predicción de sentimiento\n",
    "        sentiment_output = layers.Dense(1, activation='sigmoid', name='sentiment')(x)  # Cambiado a sigmoid para [0,1]\n",
    "        \n",
    "        # Probabilidad de interacción\n",
    "        interaction_output = layers.Dense(1, activation='sigmoid', name='interaction')(x)\n",
    "        \n",
    "        # Predicción especializada de resistencia climática (tu feature única)\n",
    "        climate_resilience_output = layers.Dense(1, activation='sigmoid', name='climate_resilience')(weather_patterns)\n",
    "        \n",
    "        # === MODELO FINAL ===\n",
    "        self.model = Model(\n",
    "            inputs=[user_input, item_input, city_input, \n",
    "                   contextual_input, temporal_input, weather_sentiment_input],\n",
    "            outputs=[rating_output, sentiment_output, interaction_output, climate_resilience_output],\n",
    "            name='TourismRecommenderWithClimateIntelligence'\n",
    "        )\n",
    "        \n",
    "        # Compilar modelo con múltiples losses\n",
    "        self.model.compile(\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "            loss={\n",
    "                'rating': 'mse',\n",
    "                'sentiment': 'mse',\n",
    "                'interaction': 'binary_crossentropy',\n",
    "                'climate_resilience': 'binary_crossentropy'\n",
    "            },\n",
    "            loss_weights={\n",
    "                'rating': 1.0,\n",
    "                'sentiment': 0.6,\n",
    "                'interaction': 0.3,\n",
    "                'climate_resilience': 0.4  # Tu feature especializada\n",
    "            },\n",
    "            metrics={\n",
    "                'rating': ['mae'],\n",
    "                'sentiment': ['mae'],\n",
    "                'interaction': ['accuracy'],\n",
    "                'climate_resilience': ['accuracy']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"Modelo con Climate Intelligence construido exitosamente!\")\n",
    "        print(f\"Total parámetros: {self.model.count_params():,}\")\n",
    "        return self.model\n",
    "    \n",
    "    def prepare_training_data(self, processed_data):\n",
    "        \"\"\"\n",
    "        Prepara datos para entrenamiento del modelo usando sentimientos categóricos\n",
    "        \"\"\"\n",
    "        print(\"Preparando datos de entrenamiento...\")\n",
    "        \n",
    "        # Extraer datos matched\n",
    "        matched_reviews_df = processed_data.get('matched_reviews', pd.DataFrame())\n",
    "        activities_df = processed_data['raw_datasets']['activities']\n",
    "        city_features = processed_data['city_features']\n",
    "        \n",
    "        if matched_reviews_df.empty:\n",
    "            raise ValueError(\"No hay reviews emparejadas con actividades. Verificar proceso de matching.\")\n",
    "        \n",
    "        # Convertir sentimiento categórico a numérico\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        \n",
    "        matched_reviews_df['sentimiento_numerico'] = matched_reviews_df['sentimiento'].map(sentiment_to_numeric)\n",
    "        \n",
    "        # Verificar que todas las categorías están mapeadas\n",
    "        unmapped_sentiments = matched_reviews_df[matched_reviews_df['sentimiento_numerico'].isna()]\n",
    "        if not unmapped_sentiments.empty:\n",
    "            print(f\"⚠️ Encontrados {len(unmapped_sentiments)} sentimientos no mapeados:\")\n",
    "            print(unmapped_sentiments['sentimiento'].value_counts())\n",
    "            # Asignar neutro a los no mapeados\n",
    "            matched_reviews_df['sentimiento_numerico'] = matched_reviews_df['sentimiento_numerico'].fillna(0)\n",
    "        \n",
    "        # Mostrar distribución de sentimientos\n",
    "        print(\"Distribución de sentimientos en datos de entrenamiento:\")\n",
    "        sentiment_dist = matched_reviews_df['sentimiento'].value_counts()\n",
    "        for sentiment, count in sentiment_dist.items():\n",
    "            print(f\"   {sentiment}: {count} ({count/len(matched_reviews_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Crear mapeos de IDs\n",
    "        user_ids = matched_reviews_df['user_id'].unique()\n",
    "        item_ids = matched_reviews_df['item_id'].unique()\n",
    "        cities = list(city_features.keys())\n",
    "        \n",
    "        self.user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "        self.item_to_idx = {item: idx for idx, item in enumerate(item_ids)}\n",
    "        self.city_to_idx = {city: idx for idx, city in enumerate(cities)}\n",
    "        \n",
    "        # Crear dataset de entrenamiento\n",
    "        training_data = []\n",
    "        \n",
    "        for idx, row in matched_reviews_df.iterrows():\n",
    "            if idx % 10000 == 0:\n",
    "                print(f\"Procesando review {idx}/{len(matched_reviews_df)}\")\n",
    "            \n",
    "            user_idx = self.user_to_idx.get(row['user_id'], 0)\n",
    "            item_idx = self.item_to_idx.get(row['item_id'], 0)\n",
    "            city_idx = self.city_to_idx.get(row['ciudad'], 0)\n",
    "            \n",
    "            # Features contextuales usando datos reales\n",
    "            contextual_features = self._extract_contextual_features(\n",
    "                row, row['ciudad'], city_features, activities_df\n",
    "            )\n",
    "            \n",
    "            # Features temporales\n",
    "            temporal_features = self._extract_temporal_features(row.get('fecha_comentario'))\n",
    "            \n",
    "            # Features de clima y sentimiento categórico\n",
    "            weather_sentiment_features = self._extract_weather_sentiment_features(row)\n",
    "            \n",
    "            # Targets usando categorías de sentimiento\n",
    "            # Rating: convertir sentimiento categórico a escala 0-1\n",
    "            sentiment_cat = row['sentimiento']\n",
    "            if sentiment_cat == 'negativo':\n",
    "                rating_target = 0.25  # Equivalente a rating 1-2\n",
    "            elif sentiment_cat == 'positivo':\n",
    "                rating_target = 0.875  # Equivalente a rating 4-5\n",
    "            else:  # neutro\n",
    "                rating_target = 0.6  # Equivalente a rating 3\n",
    "            \n",
    "            # Sentimiento numérico normalizado [-1,1] -> [0,1]\n",
    "            sentiment_numeric = row['sentimiento_numerico']\n",
    "            sentiment_target = (sentiment_numeric + 1) / 2  # De [-1,1] a [0,1]\n",
    "            \n",
    "            # Interacción: siempre 1 porque existe la review\n",
    "            interaction_target = 1.0\n",
    "            \n",
    "            # Climate resilience target (tu feature única)\n",
    "            # 1 si es experiencia positiva con condiciones adversas, 0 otherwise\n",
    "            weather_desc = str(row.get('descripcion_corta_clima', '')).lower()\n",
    "            adverse_conditions = any([\n",
    "                'lluvia intensa' in weather_desc,\n",
    "                'lluvia moderada' in weather_desc, \n",
    "                ('lloviznas' in weather_desc and sentiment_cat == 'positivo'),\n",
    "                ('muy caluroso' in weather_desc and sentiment_cat == 'positivo'),\n",
    "                ('frío' in weather_desc and sentiment_cat == 'positivo')\n",
    "            ])\n",
    "            \n",
    "            climate_resilience_target = 1.0 if (adverse_conditions and sentiment_cat == 'positivo') else 0.0\n",
    "            \n",
    "            # Pesar por confianza del modelo de sentimiento\n",
    "            sample_weight = row['confianza']\n",
    "            \n",
    "            training_data.append({\n",
    "                'user_id': user_idx,\n",
    "                'item_id': item_idx,\n",
    "                'city_id': city_idx,\n",
    "                'contextual_features': contextual_features,\n",
    "                'temporal_features': temporal_features,\n",
    "                'weather_sentiment': weather_sentiment_features,\n",
    "                'rating': rating_target,\n",
    "                'sentiment': sentiment_target,\n",
    "                'interaction': interaction_target,\n",
    "                'climate_resilience': climate_resilience_target,\n",
    "                'sample_weight': sample_weight,\n",
    "                'sentiment_category': sentiment_cat  # Mantener categoría para análisis\n",
    "            })\n",
    "        \n",
    "        training_df = pd.DataFrame(training_data)\n",
    "        \n",
    "        # Análisis de distribución de targets\n",
    "        print(\"\\nDistribución de targets de entrenamiento:\")\n",
    "        print(f\"   Ratings - min: {training_df['rating'].min():.3f}, max: {training_df['rating'].max():.3f}, mean: {training_df['rating'].mean():.3f}\")\n",
    "        print(f\"   Sentiments - min: {training_df['sentiment'].min():.3f}, max: {training_df['sentiment'].max():.3f}, mean: {training_df['sentiment'].mean():.3f}\")\n",
    "        print(f\"   Sample weights - min: {training_df['sample_weight'].min():.3f}, max: {training_df['sample_weight'].max():.3f}, mean: {training_df['sample_weight'].mean():.3f}\")\n",
    "        \n",
    "        return training_df\n",
    "    \n",
    "    def _extract_contextual_features(self, review_row, city, city_features, activities_df):\n",
    "        \"\"\"\n",
    "        Extrae features contextuales para una review usando datos reales\n",
    "        \"\"\"\n",
    "        # Obtener actividad\n",
    "        try:\n",
    "            activity = activities_df[activities_df['item_id'] == review_row['item_id']].iloc[0]\n",
    "        except:\n",
    "            # Actividad no encontrada, usar valores por defecto\n",
    "            activity = pd.Series({\n",
    "                'precio': 100,\n",
    "                'rating': 3.5,\n",
    "                'avg_sentiment': 0,\n",
    "                'review_count': 0,\n",
    "                'avg_confidence': 0.5\n",
    "            })\n",
    "        \n",
    "        # Features básicas de la actividad\n",
    "        features = [\n",
    "            activity.get('precio', 100) / 1000,  # Normalizar precio\n",
    "            activity.get('rating', 3.5) / 5.0,   # Normalizar rating\n",
    "            activity.get('avg_sentiment', 0),     # Sentiment promedio de la actividad\n",
    "            activity.get('review_count', 0) / 100,  # Normalizar count\n",
    "            activity.get('avg_confidence', 0.5),    # Confianza promedio\n",
    "        ]\n",
    "        \n",
    "        # Features geográficas\n",
    "        city_data = city_features.get(city, {})\n",
    "        geo_data = city_data.get('geo')\n",
    "        if geo_data is not None and isinstance(geo_data, pd.Series):\n",
    "            features.extend([\n",
    "                geo_data.get('densidad_poblacion', 0),\n",
    "                geo_data.get('densidad_carreteras', 0),\n",
    "                geo_data.get('win_population', 0) / 1000000,  # Normalizar población\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])\n",
    "        \n",
    "        # Features de contexto turístico (datos ONU)\n",
    "        tourism_context = city_data.get('tourism_context', {})\n",
    "        combined_features = tourism_context.get('combined_features', {})\n",
    "        \n",
    "        features.extend([\n",
    "            combined_features.get('spending_ratio', 1.0),\n",
    "            combined_features.get('price_sensitivity_factor', 1.0),\n",
    "            combined_features.get('tourism_volume_ratio', 1.0),\n",
    "            combined_features.get('demand_pressure_factor', 1.0),\n",
    "            combined_features.get('domestic_tourism_strength', 0) / 1000000,  # Normalizar\n",
    "            combined_features.get('local_preference_factor', 1.0),\n",
    "        ])\n",
    "        \n",
    "        # Features de correlación clima-sentimiento (tu análisis único)\n",
    "        climate_correlation = city_data.get('climate_sentiment_correlation', {})\n",
    "        features.extend([\n",
    "            climate_correlation.get('correlation', 0),\n",
    "            climate_correlation.get('weather_impact_strength', 0),\n",
    "        ])\n",
    "        \n",
    "        # Features de la fuente del review\n",
    "        source_encoding = {\n",
    "            'booking': 0.8, 'airbnb': 0.7, 'tripadvisor': 0.9, \n",
    "            'google': 0.6, 'expedia': 0.7, 'otros': 0.5\n",
    "        }\n",
    "        source_value = source_encoding.get(review_row.get('fuente', 'otros').lower(), 0.5)\n",
    "        features.append(source_value)\n",
    "        \n",
    "        # Padding hasta completar dimensión fija\n",
    "        while len(features) < 20:  # contextual_features_dim\n",
    "            features.append(0)\n",
    "        \n",
    "        return features[:20]\n",
    "    \n",
    "    def _extract_temporal_features(self, date):\n",
    "        \"\"\"\n",
    "        Extrae features temporales usando fecha real del comentario\n",
    "        \"\"\"\n",
    "        if pd.isna(date):\n",
    "            date = datetime.now()\n",
    "        elif isinstance(date, str):\n",
    "            try:\n",
    "                date = pd.to_datetime(date)\n",
    "            except:\n",
    "                date = datetime.now()\n",
    "        \n",
    "        # One-hot encoding del mes\n",
    "        month_features = [0] * 12\n",
    "        month_features[date.month - 1] = 1\n",
    "        \n",
    "        return month_features\n",
    "    \n",
    "    def _extract_weather_sentiment_features(self, review_row):\n",
    "        \"\"\"\n",
    "        Extrae features de clima y sentimiento categórico\n",
    "        \"\"\"\n",
    "        # Convertir sentimiento categórico a numérico\n",
    "        sentiment_to_numeric = {\n",
    "            'negativo': -1,\n",
    "            'neutro': 0,\n",
    "            'positivo': 1\n",
    "        }\n",
    "        \n",
    "        sentiment_numeric = sentiment_to_numeric.get(review_row.get('sentimiento', 'neutro'), 0)\n",
    "        \n",
    "        # Tu feature más valiosa: correlación clima-sentimiento\n",
    "        features = [\n",
    "            sentiment_numeric,                              # Sentimiento numérico [-1, 0, 1]\n",
    "            review_row.get('confianza', 0.5),              # Confianza del modelo de sentimiento\n",
    "        ]\n",
    "        \n",
    "        # Encoding de descripción climática\n",
    "        weather_desc = str(review_row.get('descripcion_corta_clima', '')).lower()\n",
    "        \n",
    "        # Features climáticas categóricas (one-hot simplificado)\n",
    "        weather_features = {\n",
    "            'soleado': 1 if any(w in weather_desc for w in ['soleado', 'despejado', 'clear']) else 0,\n",
    "            'nublado': 1 if any(w in weather_desc for w in ['nublado', 'cloudy']) else 0,\n",
    "            'lluvioso': 1 if any(w in weather_desc for w in ['lluvia', 'rain', 'lluvioso']) else 0,\n",
    "            'tormentoso': 1 if any(w in weather_desc for w in ['tormenta', 'storm']) else 0,\n",
    "        }\n",
    "        \n",
    "        features.extend(list(weather_features.values()))\n",
    "        \n",
    "        # Features derivadas del análisis clima-sentimiento categórico\n",
    "        # Sentimiento esperado por categoría climática\n",
    "        expected_sentiment_map = {\n",
    "            'soleado': 'positivo',\n",
    "            'nublado': 'neutro', \n",
    "            'lluvioso': 'neutro',  # Puede ser neutro o ligeramente negativo\n",
    "            'tormentoso': 'negativo'\n",
    "        }\n",
    "        \n",
    "        # Determinar clima dominante\n",
    "        dominant_weather = 'neutro'  # Por defecto\n",
    "        for weather_type, is_present in weather_features.items():\n",
    "            if is_present:\n",
    "                dominant_weather = weather_type\n",
    "                break\n",
    "        \n",
    "        # Sentimiento esperado vs real\n",
    "        expected_sentiment = expected_sentiment_map.get(dominant_weather, 'neutro')\n",
    "        actual_sentiment = review_row.get('sentimiento', 'neutro')\n",
    "        \n",
    "        # Feature de coincidencia expectativa-realidad\n",
    "        expectation_match = 1 if expected_sentiment == actual_sentiment else 0\n",
    "        features.append(expectation_match)\n",
    "        \n",
    "        # Feature de sorpresa positiva (mejor de lo esperado)\n",
    "        positive_surprise = 0\n",
    "        if expected_sentiment in ['negativo', 'neutro'] and actual_sentiment == 'positivo':\n",
    "            positive_surprise = 1\n",
    "        features.append(positive_surprise)\n",
    "        \n",
    "        # Feature de sorpresa negativa (peor de lo esperado)\n",
    "        negative_surprise = 0\n",
    "        if expected_sentiment in ['positivo', 'neutro'] and actual_sentiment == 'negativo':\n",
    "            negative_surprise = 1\n",
    "        features.append(negative_surprise)\n",
    "        \n",
    "        # Feature de resistencia climática (positivo a pesar del mal clima)\n",
    "        climate_resilience = 0\n",
    "        if weather_features['lluvioso'] or weather_features['tormentoso']:\n",
    "            if actual_sentiment == 'positivo':\n",
    "                climate_resilience = 1\n",
    "        features.append(climate_resilience)\n",
    "        \n",
    "        # Padding hasta 10 features\n",
    "        while len(features) < 10:\n",
    "            features.append(0)\n",
    "        \n",
    "        return features[:10]\n",
    "    \n",
    "    def train_model(self, training_data, validation_split=0.2, epochs=100, batch_size=256):\n",
    "        \"\"\"\n",
    "        Entrena el modelo con sample weights para considerar confianza\n",
    "        \"\"\"\n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "        \n",
    "        # Separar features y targets\n",
    "        X = {\n",
    "            'user_id': training_data['user_id'].values,\n",
    "            'item_id': training_data['item_id'].values,\n",
    "            'city_id': training_data['city_id'].values,\n",
    "            'contextual_features': np.array(training_data['contextual_features'].tolist()),\n",
    "            'temporal_features': np.array(training_data['temporal_features'].tolist()),\n",
    "            'weather_sentiment': np.array(training_data['weather_sentiment'].tolist())\n",
    "        }\n",
    "        \n",
    "        y = {\n",
    "            'rating': training_data['rating'].values,\n",
    "            'sentiment': training_data['sentiment'].values,\n",
    "            'interaction': training_data['interaction'].values,\n",
    "            'climate_resilience': training_data['climate_resilience'].values\n",
    "        }\n",
    "        \n",
    "        # Sample weights basados en confianza del modelo de sentimiento\n",
    "        sample_weights = training_data['sample_weight'].values\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=15, restore_best_weights=True\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=8, min_lr=1e-7\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                'best_tourism_model.h5', save_best_only=True, monitor='val_loss'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entrenar modelo con sample weights\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            sample_weight=sample_weights,  # Pesar samples por confianza\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Entrenamiento completado!\")\n",
    "        print(f\"Mejor val_loss: {min(self.history.history['val_loss']):.4f}\")\n",
    "        print(f\"Mejor val_rating_mae: {min(self.history.history['val_rating_mae']):.4f}\")\n",
    "        print(f\"Mejor val_sentiment_mae: {min(self.history.history['val_sentiment_mae']):.4f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def generate_recommendations(self, user_id, city, num_recommendations=10, \n",
    "                               current_date=None, weather_conditions=None):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones para un usuario usando el modelo entrenado\n",
    "        \"\"\"\n",
    "        if current_date is None:\n",
    "            current_date = datetime.now()\n",
    "        \n",
    "        # Obtener todas las actividades de la ciudad\n",
    "        city_activities = self._get_city_activities(city)  # Implementar según tu estructura\n",
    "        \n",
    "        if not city_activities:\n",
    "            return []\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for item_id in city_activities:\n",
    "            try:\n",
    "                # Preparar inputs\n",
    "                user_idx = self.user_to_idx.get(user_id, 0)\n",
    "                item_idx = self.item_to_idx.get(item_id, 0)\n",
    "                city_idx = self.city_to_idx.get(city, 0)\n",
    "                \n",
    "                # Crear features para predicción\n",
    "                contextual_features = self._create_prediction_features(\n",
    "                    item_id, city, current_date, weather_conditions\n",
    "                )\n",
    "                \n",
    "                # Hacer predicción\n",
    "                pred_input = {\n",
    "                    'user_id': np.array([user_idx]),\n",
    "                    'item_id': np.array([item_idx]),\n",
    "                    'city_id': np.array([city_idx]),\n",
    "                    'contextual_features': np.array([contextual_features['contextual']]),\n",
    "                    'temporal_features': np.array([contextual_features['temporal']]),\n",
    "                    'weather_sentiment': np.array([contextual_features['weather_sentiment']])\n",
    "                }\n",
    "                \n",
    "                pred = self.model.predict(pred_input, verbose=0)\n",
    "                \n",
    "                predictions.append({\n",
    "                    'item_id': item_id,\n",
    "                    'predicted_rating': float(pred[0][0][0]),\n",
    "                    'predicted_sentiment': float(pred[1][0][0]),\n",
    "                    'interaction_probability': float(pred[2][0][0])\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error prediciendo para item {item_id}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calcular score combinado con énfasis en tus features únicas\n",
    "        for pred in predictions:\n",
    "            # Dar más peso al sentiment porque es tu feature más confiable\n",
    "            pred['combined_score'] = (\n",
    "                pred['predicted_rating'] * 0.3 +\n",
    "                pred['predicted_sentiment'] * 0.5 +  # Mayor peso al sentimiento\n",
    "                pred['interaction_probability'] * 0.2\n",
    "            )\n",
    "        \n",
    "        # Ordenar por score combinado\n",
    "        recommendations = sorted(predictions, key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        return recommendations[:num_recommendations]\n",
    "    \n",
    "    def _get_city_activities(self, city):\n",
    "        \"\"\"\n",
    "        Obtiene actividades disponibles para una ciudad\n",
    "        \"\"\"\n",
    "        # Esta función debería conectar con tu dataset de actividades\n",
    "        # Por ahora retorno una lista ejemplo\n",
    "        return list(self.item_to_idx.keys())[:50]  # Limitar para eficiencia\n",
    "    \n",
    "    def _create_prediction_features(self, item_id, city, current_date, weather_conditions):\n",
    "        \"\"\"\n",
    "        Crea features para predicción en tiempo real\n",
    "        \"\"\"\n",
    "        # Features contextuales simplificadas para predicción\n",
    "        contextual_features = [0.5] * 20  # Valores por defecto\n",
    "        \n",
    "        # Features temporales basadas en fecha actual\n",
    "        temporal_features = [0] * 12\n",
    "        temporal_features[current_date.month - 1] = 1\n",
    "        \n",
    "        # Features de clima actual (si se proporcionan)\n",
    "        weather_sentiment_features = [0] * 10\n",
    "        if weather_conditions:\n",
    "            # Procesar condiciones climáticas actuales\n",
    "            weather_sentiment_features[0] = 0  # Placeholder para sentimiento\n",
    "            weather_sentiment_features[1] = 0.8  # Confianza alta para predicción\n",
    "            # Encoding de condiciones actuales...\n",
    "        \n",
    "        return {\n",
    "            'contextual': contextual_features,\n",
    "            'temporal': temporal_features,\n",
    "            'weather_sentiment': weather_sentiment_features\n",
    "        }\n",
    "    \n",
    "    def _extract_contextual_features(self, review_row, city, city_features, activities_df):\n",
    "        \"\"\"\n",
    "        Extrae features contextuales para una review\n",
    "        \"\"\"\n",
    "        # Obtener actividad\n",
    "        activity = activities_df[activities_df['item_id'] == review_row['item_id']].iloc[0]\n",
    "        \n",
    "        # Features básicas de la actividad\n",
    "        features = [\n",
    "            activity.get('precio', 0) / 1000,  # Normalizar precio\n",
    "            activity.get('rating', 0) / 5.0,   # Normalizar rating\n",
    "            activity.get('avg_sentiment', 0),   # Sentiment promedio\n",
    "            activity.get('review_count', 0) / 100,  # Normalizar count\n",
    "        ]\n",
    "        \n",
    "        # Features geográficas si están disponibles\n",
    "        city_data = city_features.get(city, {})\n",
    "        geo_data = city_data.get('geo')\n",
    "        if geo_data is not None:\n",
    "            features.extend([\n",
    "                geo_data.get('densidad_poblacion', 0),\n",
    "                geo_data.get('densidad_carreteras', 0),\n",
    "                geo_data.get('win_population', 0) / 1000000,  # Normalizar\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0, 0, 0])\n",
    "        \n",
    "        # Padding hasta completar dimensión fija\n",
    "        while len(features) < 20:  # contextual_features_dim\n",
    "            features.append(0)\n",
    "        \n",
    "        return features[:20]\n",
    "    \n",
    "    def _extract_temporal_features(self, date):\n",
    "        \"\"\"\n",
    "        Extrae features temporales (estacionalidad)\n",
    "        \"\"\"\n",
    "        if isinstance(date, str):\n",
    "            date = datetime.strptime(date, '%Y-%m-%d')\n",
    "        elif pd.isna(date):\n",
    "            date = datetime.now()\n",
    "        \n",
    "        # One-hot encoding del mes\n",
    "        month_features = [0] * 12\n",
    "        month_features[date.month - 1] = 1\n",
    "        \n",
    "        return month_features\n",
    "    \n",
    "    def _extract_weather_sentiment_features(self, review_row):\n",
    "        \"\"\"\n",
    "        Extrae features de clima y sentimiento\n",
    "        \"\"\"\n",
    "        features = [\n",
    "            review_row.get('sentiment_score', 0),\n",
    "            review_row.get('temperature', 20) / 40,  # Normalizar temperatura\n",
    "            review_row.get('humidity', 50) / 100,    # Normalizar humedad\n",
    "            review_row.get('wind_speed', 5) / 20,    # Normalizar viento\n",
    "            review_row.get('precipitation', 0) / 10, # Normalizar precipitación\n",
    "        ]\n",
    "        \n",
    "        # Padding hasta 10 features\n",
    "        while len(features) < 10:\n",
    "            features.append(0)\n",
    "        \n",
    "        return features[:10]\n",
    "    \n",
    "    def train_model(self, training_data, validation_split=0.2, epochs=100, batch_size=256):\n",
    "        \"\"\"\n",
    "        Entrena el modelo\n",
    "        \"\"\"\n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "        \n",
    "        # Separar features y targets\n",
    "        X = {\n",
    "            'user_id': training_data['user_id'].values,\n",
    "            'item_id': training_data['item_id'].values,\n",
    "            'city_id': training_data['city_id'].values,\n",
    "            'contextual_features': np.array(training_data['contextual_features'].tolist()),\n",
    "            'temporal_features': np.array(training_data['temporal_features'].tolist()),\n",
    "            'weather_sentiment': np.array(training_data['weather_sentiment'].tolist())\n",
    "        }\n",
    "        \n",
    "        y = {\n",
    "            'rating': training_data['rating'].values,\n",
    "            'sentiment': training_data['sentiment'].values,\n",
    "            'interaction': training_data['interaction'].values\n",
    "        }\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss', patience=10, restore_best_weights=True\n",
    "            ),\n",
    "            keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7\n",
    "            ),\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                'best_tourism_model.h5', save_best_only=True, monitor='val_loss'\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        self.history = self.model.fit(\n",
    "            X, y,\n",
    "            validation_split=validation_split,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        print(\"Entrenamiento completado!\")\n",
    "        return self.history\n",
    "    \n",
    "    def generate_recommendations(self, user_id, city, num_recommendations=10, \n",
    "                               current_date=None, weather_conditions=None):\n",
    "        \"\"\"\n",
    "        Genera recomendaciones para un usuario\n",
    "        \"\"\"\n",
    "        if current_date is None:\n",
    "            current_date = datetime.now()\n",
    "        \n",
    "        # Obtener todas las actividades de la ciudad\n",
    "        city_items = []  # Implementar según tu estructura de datos\n",
    "        \n",
    "        # Preparar features para predicción\n",
    "        predictions = []\n",
    "        \n",
    "        for item_id in city_items:\n",
    "            # Preparar inputs\n",
    "            user_idx = self.user_to_idx.get(user_id, 0)\n",
    "            item_idx = self.item_to_idx.get(item_id, 0)\n",
    "            city_idx = self.city_to_idx.get(city, 0)\n",
    "            \n",
    "            # Crear features contextuales, temporales y de clima\n",
    "            contextual_features = self._create_prediction_features(\n",
    "                item_id, city, current_date, weather_conditions\n",
    "            )\n",
    "            \n",
    "            # Hacer predicción\n",
    "            pred_input = {\n",
    "                'user_id': np.array([user_idx]),\n",
    "                'item_id': np.array([item_idx]),\n",
    "                'city_id': np.array([city_idx]),\n",
    "                'contextual_features': np.array([contextual_features['contextual']]),\n",
    "                'temporal_features': np.array([contextual_features['temporal']]),\n",
    "                'weather_sentiment': np.array([contextual_features['weather_sentiment']])\n",
    "            }\n",
    "            \n",
    "            pred = self.model.predict(pred_input, verbose=0)\n",
    "            \n",
    "            predictions.append({\n",
    "                'item_id': item_id,\n",
    "                'predicted_rating': pred[0][0][0],\n",
    "                'predicted_sentiment': pred[1][0][0],\n",
    "                'interaction_probability': pred[2][0][0]\n",
    "            })\n",
    "        \n",
    "        # Ordenar por score combinado\n",
    "        for pred in predictions:\n",
    "            pred['combined_score'] = (\n",
    "                pred['predicted_rating'] * 0.4 +\n",
    "                pred['predicted_sentiment'] * 0.3 +\n",
    "                pred['interaction_probability'] * 0.3\n",
    "            )\n",
    "        \n",
    "        recommendations = sorted(predictions, key=lambda x: x['combined_score'], reverse=True)\n",
    "        \n",
    "        return recommendations[:num_recommendations]\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        \"\"\"\n",
    "        Guarda el modelo entrenado\n",
    "        \"\"\"\n",
    "        self.model.save(model_path)\n",
    "        \n",
    "        # Guardar mapeos\n",
    "        mappings = {\n",
    "            'user_to_idx': self.user_to_idx,\n",
    "            'item_to_idx': self.item_to_idx,\n",
    "            'city_to_idx': self.city_to_idx\n",
    "        }\n",
    "        \n",
    "        with open(model_path.replace('.h5', '_mappings.pkl'), 'wb') as f:\n",
    "            pickle.dump(mappings, f)\n",
    "        \n",
    "        print(f\"Modelo guardado en {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6add6a1-664d-4ebe-a4db-e02a3316dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainabilityEngine:\n",
    "    def __init__(self, reranking_system):\n",
    "        self.reranking_system = reranking_system\n",
    "    \n",
    "    def generate_explanation_fast(self, recommendation, user_context=None):\n",
    "        \"\"\"\n",
    "        Explicación rápida y simple\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        base_score = recommendation.get('original_score', 0)\n",
    "        \n",
    "        explanations.append(f\"Puntuación base: {base_score:.2f}\")\n",
    "        \n",
    "        # Explicaciones simplificadas\n",
    "        if recommendation.get('weather_boost', False):\n",
    "            explanations.append(\"Recomendado por buen clima actual\")\n",
    "        \n",
    "        if recommendation.get('temporal_boost', False):\n",
    "            explanations.append(\"Horario ideal para esta actividad\")\n",
    "        \n",
    "        if recommendation.get('trending_boost', False):\n",
    "            explanations.append(\"Actividad popular últimamente\")\n",
    "        \n",
    "        if recommendation.get('seasonal_boost', False):\n",
    "            explanations.append(\"Temporada alta - experiencia óptima\")\n",
    "        \n",
    "        return {\n",
    "            'final_score': recommendation.get('final_score', base_score),\n",
    "            'explanations': explanations,\n",
    "            'confidence': min(1.0, recommendation.get('final_score', base_score)),\n",
    "            'boost_factors': {\n",
    "                'weather': recommendation.get('weather_boost', False),\n",
    "                'temporal': recommendation.get('temporal_boost', False),\n",
    "                'trending': recommendation.get('trending_boost', False),\n",
    "                'seasonal': recommendation.get('seasonal_boost', False)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca183b-8437-41da-8c07-3dc7ffa3385a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
