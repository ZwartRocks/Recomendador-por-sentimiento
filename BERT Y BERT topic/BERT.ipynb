{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "051c7f01",
   "metadata": {},
   "source": [
    "# 🧠 BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14aba5",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: transformers, torch, sklearn, pandas, numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9825c3a3-67bb-4d53-9487-5bf7da1b9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification,TrainerCallback,Trainer, TrainingArguments\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import warnings\n",
    "import time\n",
    "import psutil\n",
    "import threading\n",
    "from tqdm.auto import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2a01e",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04487175-a612-47df-ae34-461dc5ae3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "from tqdm.auto import tqdm\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d942ff4",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e738bda6-09a7-46e4-86d3-3c120d9b17b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Usando dispositivo:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f749fd8",
   "metadata": {},
   "source": [
    "## 🏷️ Preparación de etiquetas\n",
    "- Mapea/convierte etiquetas y define `num_labels` si aplica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdcad11a-d8f6-4e0a-8b75-509b5dcf8501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer_config.json\n",
      "loading file tokenizer.json from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\tokenizer.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at C:\\Users\\swart\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased\\snapshots\\3f076fdb1ab68d5b2880cb87a0886f315b8146f8\\model.safetensors\n",
      "A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
      "* `cls.predictions.transform.LayerNorm.beta` -> `cls.predictions.transform.LayerNorm.bias`\n",
      "* `cls.predictions.transform.LayerNorm.gamma` -> `cls.predictions.transform.LayerNorm.weight`\n",
      "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723afa54",
   "metadata": {},
   "source": [
    "## 📥 Carga de datos\n",
    "- Lee dataset: dataset_multilingue_10idiomas_balanceado.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3771ceba-5833-4f5e-a017-7c656e343649",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_multilingue_10idiomas_balanceado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce009167",
   "metadata": {},
   "source": [
    "## 🧼 Limpieza de texto\n",
    "- Normaliza texto (lower/regex), quita URLs/símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9ecefc6-1fd5-4baa-ba04-bf59d6725263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_safe(text):\n",
    "    \"\"\"Detecta idioma de forma segura\"\"\"\n",
    "    try:\n",
    "        if len(str(text).strip()) < 3:\n",
    "            return 'unknown'\n",
    "        lang = detect(str(text)[:100])  # Solo primeros 100 chars\n",
    "        return lang\n",
    "    except (LangDetectException, Exception):\n",
    "        return 'unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5191a29",
   "metadata": {},
   "source": [
    "## 🛠️ Helpers / funciones auxiliares\n",
    "- Funciones de apoyo (tokenización, métricas, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae97cf0-4f4d-487e-abc1-6237244e133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_multilingual_distribution(df, sample_size=1000):\n",
    "    \"\"\"Analiza distribución de idiomas en el dataset\"\"\"\n",
    "    print(f\"\\nANÁLISIS MULTILINGÜE (muestra de {sample_size})...\")\n",
    "    \n",
    "    # Tomar muestra para análisis de idiomas\n",
    "    sample_df = df.sample(n=min(sample_size, len(df)), random_state=42)\n",
    "    sample_df['language'] = sample_df['Texto'].apply(detect_language_safe)\n",
    "    \n",
    "    # Mostrar distribución por idioma\n",
    "    lang_dist = sample_df['language'].value_counts()\n",
    "    print(f\"\\nDISTRIBUCIÓN POR IDIOMAS:\")\n",
    "    \n",
    "    lang_names = {\n",
    "        'es': 'Español', 'en': 'Inglés', 'pt': 'Portugués', \n",
    "        'fr': 'Francés', 'it': 'Italiano', 'de': 'Alemán',\n",
    "        'ca': 'Catalán', 'ro': 'Rumano', 'pl': 'Polaco',\n",
    "        'nl': 'Neerlandés', 'unknown': 'Desconocido'\n",
    "    }\n",
    "    \n",
    "    for lang, count in lang_dist.head(10).items():\n",
    "        pct = (count / len(sample_df)) * 100\n",
    "        lang_name = lang_names.get(lang, lang.upper())\n",
    "        print(f\"  {lang_name:12} ({lang}): {count:3d} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Análisis por sentimiento e idioma\n",
    "    print(f\"\\nDISTRIBUCIÓN POR SENTIMIENTO:\")\n",
    "    sentiment_dist = df[\"Label\"].value_counts().sort_index()\n",
    "    label_names = [\"Malo\", \"Neutro\", \"Bueno\"]\n",
    "    total = len(df)\n",
    "    \n",
    "    for label, count in sentiment_dist.items():\n",
    "        pct = (count / total) * 100\n",
    "        print(f\"  {label_names[int(label)]:8}: {count:6,} ({pct:.1f}%)\")\n",
    "    \n",
    "    return sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6711d5",
   "metadata": {},
   "source": [
    "- Salida de un script de diagnóstico que imprime: Una muestra para la distribución por idiomas, sdemás, los rótulos “Malo/Neutro/Bueno” son solo alias de la columna sentimiento (probablemente negativo/neutro/positivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66d5e133-23f1-473a-9764-eda5335d9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISIS MULTILINGÜE (muestra de 1000)...\n",
      "\n",
      "DISTRIBUCIÓN POR IDIOMAS:\n",
      "  Inglés       (en): 684 (68.4%)\n",
      "  Español      (es): 208 (20.8%)\n",
      "  Francés      (fr):  45 (4.5%)\n",
      "  Italiano     (it):  13 (1.3%)\n",
      "  Catalán      (ca):  11 (1.1%)\n",
      "  Alemán       (de):   8 (0.8%)\n",
      "  Portugués    (pt):   8 (0.8%)\n",
      "  Polaco       (pl):   7 (0.7%)\n",
      "  Neerlandés   (nl):   5 (0.5%)\n",
      "  Rumano       (ro):   3 (0.3%)\n",
      "\n",
      "DISTRIBUCIÓN POR SENTIMIENTO:\n",
      "  Malo    : 101,310 (33.3%)\n",
      "  Neutro  : 101,308 (33.3%)\n",
      "  Bueno   : 101,307 (33.3%)\n"
     ]
    }
   ],
   "source": [
    "sample_with_langs = analyze_multilingual_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff6a1e",
   "metadata": {},
   "source": [
    "## 🧼 Limpieza de texto\n",
    "- Normaliza texto (lower/regex), quita URLs/símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aa2200a-403d-49ef-a6f7-875009160f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_multilingual_text(text):\n",
    "    \n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    \n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = ' '.join(text.split())  # Normalizar espacios\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3ebba26-88a1-454c-8a57-681eb91e696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Texto'] = df['Texto'].apply(preprocess_multilingual_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c8b45",
   "metadata": {},
   "source": [
    "## 🧼 Limpieza de texto\n",
    "- Normaliza texto (lower/regex), quita URLs/símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b3026a-a648-4205-8126-a1f22ff8497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestras después del preprocessamiento: 303,925\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Texto'].str.len() > 0]\n",
    "print(f\"Muestras después del preprocessamiento: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0cb2c",
   "metadata": {},
   "source": [
    "## 🧭 División train/valid/test\n",
    "- Separa datos para entrenamiento y validación/prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d4324ca-1c2b-41c8-acf2-05f323b3b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multilingual_splits(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Crea splits considerando tanto sentimiento como diversidad lingüística\n",
    "    \"\"\"\n",
    "    print(f\"\\nCREANDO SPLITS ESTRATIFICADOS MULTILINGÜES...\")\n",
    "    \n",
    "    # Detectar idioma en una muestra más grande para splits\n",
    "    df_lang_sample = df.sample(n=min(2000, len(df)), random_state=42)\n",
    "    df_lang_sample['language'] = df_lang_sample['Texto'].apply(detect_language_safe)\n",
    "    \n",
    "    # Crear estratificación combinada (sentimiento + idioma principal)\n",
    "    # Identificar idiomas principales (>5% del dataset)\n",
    "    lang_dist = df_lang_sample['language'].value_counts()\n",
    "    main_languages = lang_dist[lang_dist >= len(df_lang_sample) * 0.05].index.tolist()\n",
    "    \n",
    "    print(f\"Idiomas principales detectados: {main_languages}\")\n",
    "    \n",
    "    # Para el dataset completo, usar solo estratificación por sentimiento\n",
    "    # (más estable que intentar estratificar por idioma en datasets grandes)\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df[\"Texto\"],\n",
    "        df[\"Label\"],\n",
    "        test_size=test_size,\n",
    "        stratify=df[\"Label\"],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    return train_texts, val_texts, train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6772b705-9fb7-4d81-82be-ab6f14a13c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREANDO SPLITS ESTRATIFICADOS MULTILINGÜES...\n",
      "Idiomas principales detectados: ['en', 'es']\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = create_multilingual_splits(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7be2dc-a52e-483b-8c6f-2a6c31fb9429",
   "metadata": {},
   "source": [
    "- Imprime cuántas muestras hay en cada conjunto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6833ff53-229b-41e4-b1e8-efdfe2e196e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Entrenamiento: 243,140 muestras\n",
      "  Validación: 60,785 muestras\n"
     ]
    }
   ],
   "source": [
    "print(f\"  Entrenamiento: {len(train_texts):,} muestras\")\n",
    "print(f\"  Validación: {len(val_texts):,} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9fb62",
   "metadata": {},
   "source": [
    "## Balance de clases:\n",
    "- Hace tres cosas sencillas con tus etiquetas (pandas):\n",
    "- Cuenta cuántas veces aparece cada clase\n",
    "- val_dist = val_labels.value_counts().sort_index(): Lo mismo pero para el conjunto de validación.\n",
    "- label_names = [\"Malo\", \"Neutro\", \"Bueno\"], Una lista de nombres bonitos para mostrar en gráficos/tablas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c88b21-fb30-4a10-b018-199edf664db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist = train_labels.value_counts().sort_index()\n",
    "val_dist = val_labels.value_counts().sort_index()\n",
    "label_names = [\"Malo\", \"Neutro\", \"Bueno\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8cd0cd",
   "metadata": {},
   "source": [
    "## Balance de clases para train y validación\n",
    "- Muestra clases para train y validación, con conteos y porcentajes, formateada en columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caa33f9e-ef64-4efe-98d2-2db6cbc7659b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase    Train    Val      Train%   Val%    \n",
      "---------------------------------------------\n",
      "Malo     81,048   20,262   33.3   % 33.3   %\n",
      "Neutro   81,046   20,262   33.3   % 33.3   %\n",
      "Bueno    81,046   20,261   33.3   % 33.3   %\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Clase':<8} {'Train':<8} {'Val':<8} {'Train%':<8} {'Val%':<8}\")\n",
    "print(\"-\" * 45)\n",
    "for i, (train_count, val_count) in enumerate(zip(train_dist, val_dist)):\n",
    "    train_pct = (train_count / len(train_texts)) * 100\n",
    "    val_pct = (val_count / len(val_texts)) * 100\n",
    "    print(f\"{label_names[i]:<8} {train_count:<8,} {val_count:<8,} {train_pct:<7.1f}% {val_pct:<7.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d5584a",
   "metadata": {},
   "source": [
    "## 🔤 Tokenizador BERT\n",
    "- Configura el tokenizador (padding/truncation/max_len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05f3f1ca-7512-4aea-b8ae-80f2d08abb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(\n",
    "    list(train_texts), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128,  # Suficiente para la mayoría de reviews\n",
    "    return_tensors=None  # Evitar tensores grandes en memoria\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a017ef5",
   "metadata": {},
   "source": [
    "## 🔤 Tokenizador BERT\n",
    "- Configura el tokenizador (padding/truncation/max_len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b43d05b5-a8e5-4140-994b-1bfcb0923ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(\n",
    "    list(val_texts), \n",
    "    truncation=True, \n",
    "    padding=True, \n",
    "    max_length=128,\n",
    "    return_tensors=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef17949",
   "metadata": {},
   "source": [
    "- Calcula pesos de clase para lidiar con desbalance y los prepara para usarlos en PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72366410-c922-4800-b606-263ea2a014c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_labels),\n",
    "    y=train_labels\n",
    ")\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b08a73",
   "metadata": {},
   "source": [
    "- Calcular pesos por clase (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c78ad2bc-c42d-4a70-878a-ec929325fa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Malo: 1.000\n",
      "  Neutro: 1.000\n",
      "  Bueno: 1.000\n"
     ]
    }
   ],
   "source": [
    "for i, weight in enumerate(class_weights):\n",
    "    label_name = label_names[i]\n",
    "    print(f\"  {label_name}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb9eb3",
   "metadata": {},
   "source": [
    "- Resume los pesos de clase\n",
    "- Calcula el promedio\n",
    "- Calcula la varianza\n",
    "- Prints con formato a 3 decimales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e61c81f4-2ced-4f1d-8d36-6c0023f9bbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Peso promedio: 1.000\n",
      "  Varianza de pesos: 0.000\n"
     ]
    }
   ],
   "source": [
    "avg_weight = np.mean(class_weights)\n",
    "weight_variance = np.var(class_weights)\n",
    "print(f\"  Peso promedio: {avg_weight:.3f}\")\n",
    "print(f\"  Varianza de pesos: {weight_variance:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74370b2f-d87a-49a1-adff-079fbe0bc334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset bien balanceado (varianza < 0.1)\n"
     ]
    }
   ],
   "source": [
    "if weight_variance < 0.1:\n",
    "    print(\"Dataset bien balanceado (varianza < 0.1)\")\n",
    "    USE_CLASS_WEIGHTS = False  # No necesario usar pesos\n",
    "else:\n",
    "    print(\"Dataset con desbalance residual\")\n",
    "    USE_CLASS_WEIGHTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc40a61",
   "metadata": {},
   "source": [
    "## 🛠️ Helpers / funciones auxiliares\n",
    "- Funciones de apoyo (tokenización, métricas, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af212f0-8d46-4df5-af23-65b5039173cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualReviewDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(int(self.labels.iloc[idx]))\n",
    "        return item\n",
    "\n",
    "train_dataset = MultilingualReviewDataset(train_encodings, train_labels)\n",
    "val_dataset = MultilingualReviewDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bac834",
   "metadata": {},
   "source": [
    "## 🏷️ Preparación de etiquetas\n",
    "- Mapea/convierte etiquetas y define `num_labels` si aplica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ef650e1-c479-4c6d-b92b-7aa2036e1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualWeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, use_weights=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        self.use_weights = use_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Usar pesos solo si es necesario\n",
    "        if self.use_weights and self.class_weights is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9511902c",
   "metadata": {},
   "source": [
    "## 📊 Evaluación\n",
    "- Calcula métricas (accuracy/F1/precision/recall) y reportes/confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "099c1545-5fb0-44fb-88fd-21fc879de677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_multilingual_metrics(eval_pred):\n",
    "    \"\"\"Métricas optimizadas para evaluación multilingüe\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Calcular métricas detalladas\n",
    "    report = classification_report(\n",
    "        labels, predictions, \n",
    "        target_names=[\"Malo\", \"Neutro\", \"Bueno\"], \n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'macro_f1': report['macro avg']['f1-score'],\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        'malo_f1': report['Malo']['f1-score'],\n",
    "        'neutro_f1': report['Neutro']['f1-score'],\n",
    "        'bueno_f1': report['Bueno']['f1-score'],\n",
    "        'malo_precision': report['Malo']['precision'],\n",
    "        'malo_recall': report['Malo']['recall'],\n",
    "        'neutro_precision': report['Neutro']['precision'],\n",
    "        'neutro_recall': report['Neutro']['recall'],\n",
    "        'bueno_precision': report['Bueno']['precision'],\n",
    "        'bueno_recall': report['Bueno']['recall']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41fa66",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f3addfb3-d012-4c64-aecb-bebc59fd5cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitor:\n",
    "    def __init__(self):\n",
    "        self.monitoring = False\n",
    "        self.thread = None\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        self.monitoring = True\n",
    "        self.thread = threading.Thread(target=self._monitor_resources, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        self.monitoring = False\n",
    "        \n",
    "    def _monitor_resources(self):\n",
    "        while self.monitoring:\n",
    "            cpu_percent = psutil.cpu_percent(interval=1)\n",
    "            memory = psutil.virtual_memory()\n",
    "            memory_percent = memory.percent\n",
    "            \n",
    "            gpu_info = \"\"\n",
    "            if torch.cuda.is_available():\n",
    "                try:\n",
    "                    gpu_mem_used = torch.cuda.memory_allocated() / 1e9\n",
    "                    gpu_mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "                    gpu_mem_percent = (gpu_mem_used / gpu_mem_total) * 100\n",
    "                    gpu_info = f\" | GPU: {gpu_mem_percent:.1f}% ({gpu_mem_used:.1f}/{gpu_mem_total:.1f}GB)\"\n",
    "                except:\n",
    "                    gpu_info = \" | GPU: Monitoring\"\n",
    "            \n",
    "            print(f\"\\r[RECURSOS] CPU: {cpu_percent:.1f}% | RAM: {memory_percent:.1f}%{gpu_info}\", end=\"\", flush=True)\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ef0cf",
   "metadata": {},
   "source": [
    "## 📊 Evaluación\n",
    "- Calcula métricas (accuracy/F1/precision/recall) y reportes/confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90d00276-d0ba-435e-baa9-ae65f3966d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetailedProgressCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.epoch_start_time = None\n",
    "        self.last_log_time = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(f\"\\nENTRENAMIENTO INICIADO\")\n",
    "        print(f\"Total pasos: {state.max_steps:,}\")\n",
    "        print(f\"Épocas: {args.num_train_epochs}\")\n",
    "        print(f\"Batch size efectivo: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
    "        print(f\"Usando FP16: {args.fp16}\")\n",
    "        print(f\"Métrica objetivo: {args.metric_for_best_model}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        self.epoch_start_time = time.time()\n",
    "        current_epoch = int(state.epoch) + 1\n",
    "        print(f\"\\n📈 ÉPOCA {current_epoch}/{args.num_train_epochs} - Paso {state.global_step:,}/{state.max_steps:,}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    def on_step_end(self, args, state, control, logs=None, **kwargs):\n",
    "        # Mostrar progreso cada 100 pasos\n",
    "        if state.global_step % 100 == 0 or state.global_step <= 50:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Calcular ETA\n",
    "            if self.start_time:\n",
    "                elapsed = current_time - self.start_time\n",
    "                if state.global_step > 0:\n",
    "                    avg_time_per_step = elapsed / state.global_step\n",
    "                    remaining_steps = state.max_steps - state.global_step\n",
    "                    eta_seconds = remaining_steps * avg_time_per_step\n",
    "                    eta_formatted = time.strftime('%H:%M:%S', time.gmtime(eta_seconds))\n",
    "                else:\n",
    "                    eta_formatted = \"Calculando...\"\n",
    "            else:\n",
    "                eta_formatted = \"Calculando...\"\n",
    "            \n",
    "            # Progreso global\n",
    "            progress_percent = (state.global_step / state.max_steps) * 100\n",
    "            progress_bar = \"█\" * int(progress_percent // 2.5) + \"░\" * (40 - int(progress_percent // 2.5))\n",
    "            \n",
    "            print(f\"\\n[{progress_bar}] {progress_percent:.1f}% - Paso {state.global_step:,}/{state.max_steps:,} | ETA: {eta_formatted}\")\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            current_time = time.time()\n",
    "            \n",
    "            # Evitar spam de logs\n",
    "            if self.last_log_time is None or current_time - self.last_log_time > 30:  # Cada 30 segundos max\n",
    "                print(f\"Paso {state.global_step:,}: Loss = {logs['loss']:.4f}\")\n",
    "                \n",
    "                # Mostrar learning rate si está disponible\n",
    "                if 'learning_rate' in logs:\n",
    "                    print(f\"Learning Rate: {logs['learning_rate']:.2e}\")\n",
    "                \n",
    "                # Mostrar velocidad de entrenamiento\n",
    "                if self.start_time and state.global_step > 0:\n",
    "                    elapsed = current_time - self.start_time\n",
    "                    steps_per_second = state.global_step / elapsed\n",
    "                    print(f\"Velocidad: {steps_per_second:.2f} pasos/seg\")\n",
    "                \n",
    "                self.last_log_time = current_time\n",
    "                    \n",
    "    def on_evaluate(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            print(f\"\\n🔍 EVALUACIÓN - Época {int(state.epoch)}\")\n",
    "            print(\"-\" * 30)\n",
    "            for key, value in logs.items():\n",
    "                if isinstance(value, (int, float)) and not key.startswith('eval_runtime'):\n",
    "                    print(f\"   {key}: {value:.4f}\")\n",
    "            print(\"-\" * 30)\n",
    "    \n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        if self.epoch_start_time:\n",
    "            epoch_time = time.time() - self.epoch_start_time\n",
    "            current_epoch = int(state.epoch)\n",
    "            print(f\"\\nÉpoca {current_epoch} completada en {epoch_time/60:.1f} minutos\")\n",
    "            \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.start_time:\n",
    "            total_time = time.time() - self.start_time\n",
    "            print(f\"\\nENTRENAMIENTO COMPLETADO!\")\n",
    "            print(f\"Tiempo total: {total_time/3600:.2f} horas ({total_time/60:.1f} minutos)\")\n",
    "            print(f\"Pasos completados: {state.global_step:,}\")\n",
    "            print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bbd36d",
   "metadata": {},
   "source": [
    "## 🚂 Entrenamiento\n",
    "- Épocas: 3.\n",
    "- Batch size: 16.\n",
    "- Configura y ejecuta el **entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8b67ecc-9f77-45e1-a16c-f608e6794669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_multilingual_optimized\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=18,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  \n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=0,  \n",
    "    dataloader_pin_memory=False,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    logging_strategy=\"steps\",\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b57624",
   "metadata": {},
   "source": [
    "## 🚂 Entrenamiento\n",
    "- Configura y ejecuta el **entrenamiento**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2a7cdf03-36a3-4458-814e-49b5a3c07d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = MultilingualWeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    class_weights=class_weights_tensor if USE_CLASS_WEIGHTS else None,\n",
    "    use_weights=USE_CLASS_WEIGHTS,\n",
    "    compute_metrics=compute_multilingual_metrics,\n",
    "    callbacks=[DetailedProgressCallback()]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6986f6d4-9034-4bdd-a569-44b0a879a3bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 243,140\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 45,591\n",
      "  Number of trainable parameters = 177,855,747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENTRENAMIENTO INICIADO\n",
      "Total pasos: 45,591\n",
      "Épocas: 3\n",
      "Batch size efectivo: 16\n",
      "Usando FP16: True\n",
      "Métrica objetivo: macro_f1\n",
      "======================================================================\n",
      "\n",
      "📈 ÉPOCA 1/3 - Paso 0/45,591\n",
      "--------------------------------------------------\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 1/45,591 | ETA: 00:12:04\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45591' max='45591' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45591/45591 33:56:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Malo F1</th>\n",
       "      <th>Neutro F1</th>\n",
       "      <th>Bueno F1</th>\n",
       "      <th>Malo Precision</th>\n",
       "      <th>Malo Recall</th>\n",
       "      <th>Neutro Precision</th>\n",
       "      <th>Neutro Recall</th>\n",
       "      <th>Bueno Precision</th>\n",
       "      <th>Bueno Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.528436</td>\n",
       "      <td>0.740479</td>\n",
       "      <td>0.706063</td>\n",
       "      <td>0.706066</td>\n",
       "      <td>0.902672</td>\n",
       "      <td>0.739781</td>\n",
       "      <td>0.475735</td>\n",
       "      <td>0.826899</td>\n",
       "      <td>0.993732</td>\n",
       "      <td>0.623983</td>\n",
       "      <td>0.908351</td>\n",
       "      <td>0.932411</td>\n",
       "      <td>0.319333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.508545</td>\n",
       "      <td>0.745315</td>\n",
       "      <td>0.708174</td>\n",
       "      <td>0.708178</td>\n",
       "      <td>0.911697</td>\n",
       "      <td>0.746892</td>\n",
       "      <td>0.465934</td>\n",
       "      <td>0.840058</td>\n",
       "      <td>0.996693</td>\n",
       "      <td>0.622870</td>\n",
       "      <td>0.932583</td>\n",
       "      <td>0.969569</td>\n",
       "      <td>0.306648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.507309</td>\n",
       "      <td>0.750679</td>\n",
       "      <td>0.714934</td>\n",
       "      <td>0.714937</td>\n",
       "      <td>0.917414</td>\n",
       "      <td>0.751833</td>\n",
       "      <td>0.475554</td>\n",
       "      <td>0.849208</td>\n",
       "      <td>0.997532</td>\n",
       "      <td>0.627057</td>\n",
       "      <td>0.938604</td>\n",
       "      <td>0.961683</td>\n",
       "      <td>0.315878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 2/45,591 | ETA: 11:47:29\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 3/45,591 | ETA: 10:35:42\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 4/45,591 | ETA: 10:25:35\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 5/45,591 | ETA: 08:16:56\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 6/45,591 | ETA: 08:16:12\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 7/45,591 | ETA: 08:17:50\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 8/45,591 | ETA: 08:22:51\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 9/45,591 | ETA: 07:23:14\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 10/45,591 | ETA: 07:39:41\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 11/45,591 | ETA: 07:46:30\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 12/45,591 | ETA: 07:49:36\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 13/45,591 | ETA: 07:51:07\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 14/45,591 | ETA: 07:51:58\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 15/45,591 | ETA: 07:52:59\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 16/45,591 | ETA: 07:54:24\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 17/45,591 | ETA: 07:56:41\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 18/45,591 | ETA: 07:59:22\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 19/45,591 | ETA: 08:01:00\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 20/45,591 | ETA: 08:02:53\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 21/45,591 | ETA: 08:03:10\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.0% - Paso 22/45,591 | ETA: 07:38:54\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 23/45,591 | ETA: 07:40:28\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 24/45,591 | ETA: 07:43:41\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 25/45,591 | ETA: 07:45:18\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 26/45,591 | ETA: 07:46:33\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 27/45,591 | ETA: 07:47:12\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 28/45,591 | ETA: 07:48:04\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 29/45,591 | ETA: 07:48:31\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 30/45,591 | ETA: 07:51:02\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 31/45,591 | ETA: 07:51:57\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 32/45,591 | ETA: 07:53:31\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 33/45,591 | ETA: 07:54:06\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 34/45,591 | ETA: 07:54:19\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 35/45,591 | ETA: 07:57:01\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 36/45,591 | ETA: 07:58:17\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 37/45,591 | ETA: 07:59:24\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 38/45,591 | ETA: 07:59:30\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 39/45,591 | ETA: 07:59:51\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 40/45,591 | ETA: 07:59:51\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 41/45,591 | ETA: 08:00:04\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 42/45,591 | ETA: 08:00:15\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 43/45,591 | ETA: 08:02:19\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 44/45,591 | ETA: 08:02:17\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 45/45,591 | ETA: 08:02:08\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 46/45,591 | ETA: 08:01:59\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 47/45,591 | ETA: 08:01:53\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 48/45,591 | ETA: 08:05:35\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 49/45,591 | ETA: 08:06:39\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% - Paso 50/45,591 | ETA: 08:06:49\n",
      "Paso 50: Loss = 0.9781\n",
      "Learning Rate: 1.84e-06\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.2% - Paso 100/45,591 | ETA: 08:07:03\n",
      "Paso 100: Loss = 0.8577\n",
      "Learning Rate: 3.84e-06\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 150: Loss = 0.7600\n",
      "Learning Rate: 5.84e-06\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.4% - Paso 200/45,591 | ETA: 08:15:46\n",
      "Paso 200: Loss = 0.6913\n",
      "Learning Rate: 7.80e-06\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 250: Loss = 0.6556\n",
      "Learning Rate: 9.80e-06\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.7% - Paso 300/45,591 | ETA: 08:39:50\n",
      "Paso 300: Loss = 0.6479\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 350: Loss = 0.6280\n",
      "Learning Rate: 1.38e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.9% - Paso 400/45,591 | ETA: 09:13:39\n",
      "Paso 400: Loss = 0.6451\n",
      "Learning Rate: 1.58e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 450: Loss = 0.6359\n",
      "Learning Rate: 1.78e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.1% - Paso 500/45,591 | ETA: 09:13:57\n",
      "Paso 500: Loss = 0.5745\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 550: Loss = 0.6152\n",
      "Learning Rate: 2.00e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.3% - Paso 600/45,591 | ETA: 09:00:29\n",
      "Paso 600: Loss = 0.6305\n",
      "Learning Rate: 2.00e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 650: Loss = 0.6305\n",
      "Learning Rate: 1.99e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.5% - Paso 700/45,591 | ETA: 08:45:54\n",
      "Paso 700: Loss = 0.6085\n",
      "Learning Rate: 1.99e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 750: Loss = 0.5868\n",
      "Learning Rate: 1.99e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 1.8% - Paso 800/45,591 | ETA: 08:33:31\n",
      "Paso 800: Loss = 0.5882\n",
      "Learning Rate: 1.99e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 850: Loss = 0.6337\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.0% - Paso 900/45,591 | ETA: 08:22:49\n",
      "Paso 900: Loss = 0.6081\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 950: Loss = 0.5483\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.2% - Paso 1,000/45,591 | ETA: 08:13:30\n",
      "Paso 1,000: Loss = 0.5472\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 1,050: Loss = 0.5937\n",
      "Learning Rate: 1.98e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.4% - Paso 1,100/45,591 | ETA: 08:05:02\n",
      "Paso 1,100: Loss = 0.5261\n",
      "Learning Rate: 1.97e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,150: Loss = 0.5942\n",
      "Learning Rate: 1.97e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.6% - Paso 1,200/45,591 | ETA: 07:57:42\n",
      "Paso 1,200: Loss = 0.5745\n",
      "Learning Rate: 1.97e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,250: Loss = 0.6069\n",
      "Learning Rate: 1.97e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 2.9% - Paso 1,300/45,591 | ETA: 07:50:46\n",
      "Paso 1,300: Loss = 0.5888\n",
      "Learning Rate: 1.96e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,350: Loss = 0.5856\n",
      "Learning Rate: 1.96e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.1% - Paso 1,400/45,591 | ETA: 07:43:57\n",
      "Paso 1,400: Loss = 0.5567\n",
      "Learning Rate: 1.96e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,450: Loss = 0.6079\n",
      "Learning Rate: 1.96e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.3% - Paso 1,500/45,591 | ETA: 07:37:36\n",
      "Paso 1,500: Loss = 0.5353\n",
      "Learning Rate: 1.96e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,550: Loss = 0.5774\n",
      "Learning Rate: 1.95e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.5% - Paso 1,600/45,591 | ETA: 07:33:17\n",
      "Paso 1,600: Loss = 0.5638\n",
      "Learning Rate: 1.95e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,650: Loss = 0.5892\n",
      "Learning Rate: 1.95e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.7% - Paso 1,700/45,591 | ETA: 07:27:11\n",
      "Paso 1,700: Loss = 0.5890\n",
      "Learning Rate: 1.95e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,750: Loss = 0.5709\n",
      "Learning Rate: 1.94e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 3.9% - Paso 1,800/45,591 | ETA: 07:21:08\n",
      "Paso 1,800: Loss = 0.5566\n",
      "Learning Rate: 1.94e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,850: Loss = 0.5516\n",
      "Learning Rate: 1.94e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.2% - Paso 1,900/45,591 | ETA: 07:15:16\n",
      "Paso 1,900: Loss = 0.5609\n",
      "Learning Rate: 1.94e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 1,950: Loss = 0.5870\n",
      "Learning Rate: 1.94e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.4% - Paso 2,000/45,591 | ETA: 07:09:29\n",
      "Paso 2,000: Loss = 0.5666\n",
      "Learning Rate: 1.93e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,050: Loss = 0.5857\n",
      "Learning Rate: 1.93e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.6% - Paso 2,100/45,591 | ETA: 07:03:57\n",
      "Paso 2,100: Loss = 0.5609\n",
      "Learning Rate: 1.93e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,150: Loss = 0.5696\n",
      "Learning Rate: 1.93e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 4.8% - Paso 2,200/45,591 | ETA: 06:58:33\n",
      "Paso 2,200: Loss = 0.5280\n",
      "Learning Rate: 1.92e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,250: Loss = 0.6076\n",
      "Learning Rate: 1.92e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.0% - Paso 2,300/45,591 | ETA: 06:53:15\n",
      "Paso 2,300: Loss = 0.5498\n",
      "Learning Rate: 1.92e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,350: Loss = 0.5699\n",
      "Learning Rate: 1.92e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.3% - Paso 2,400/45,591 | ETA: 06:48:04\n",
      "Paso 2,400: Loss = 0.5322\n",
      "Learning Rate: 1.92e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,450: Loss = 0.5575\n",
      "Learning Rate: 1.91e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.5% - Paso 2,500/45,591 | ETA: 06:42:56\n",
      "Paso 2,500: Loss = 0.5668\n",
      "Learning Rate: 1.91e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,550: Loss = 0.5599\n",
      "Learning Rate: 1.91e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.7% - Paso 2,600/45,591 | ETA: 06:38:09\n",
      "Paso 2,600: Loss = 0.5697\n",
      "Learning Rate: 1.91e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,650: Loss = 0.5819\n",
      "Learning Rate: 1.90e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 5.9% - Paso 2,700/45,591 | ETA: 06:33:15\n",
      "Paso 2,700: Loss = 0.5634\n",
      "Learning Rate: 1.90e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,750: Loss = 0.5442\n",
      "Learning Rate: 1.90e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6.1% - Paso 2,800/45,591 | ETA: 06:28:24\n",
      "Paso 2,800: Loss = 0.5731\n",
      "Learning Rate: 1.90e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,850: Loss = 0.5618\n",
      "Learning Rate: 1.90e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6.4% - Paso 2,900/45,591 | ETA: 06:23:35\n",
      "Paso 2,900: Loss = 0.5718\n",
      "Learning Rate: 1.89e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 2,950: Loss = 0.5795\n",
      "Learning Rate: 1.89e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6.6% - Paso 3,000/45,591 | ETA: 06:18:47\n",
      "Paso 3,000: Loss = 0.5679\n",
      "Learning Rate: 1.89e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,050: Loss = 0.5991\n",
      "Learning Rate: 1.89e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 6.8% - Paso 3,100/45,591 | ETA: 06:13:59\n",
      "Paso 3,100: Loss = 0.5455\n",
      "Learning Rate: 1.88e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,150: Loss = 0.5001\n",
      "Learning Rate: 1.88e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.0% - Paso 3,200/45,591 | ETA: 06:09:11\n",
      "Paso 3,200: Loss = 0.5707\n",
      "Learning Rate: 1.88e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,250: Loss = 0.5726\n",
      "Learning Rate: 1.88e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.2% - Paso 3,300/45,591 | ETA: 06:04:28\n",
      "Paso 3,300: Loss = 0.5635\n",
      "Learning Rate: 1.88e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,350: Loss = 0.5520\n",
      "Learning Rate: 1.87e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.5% - Paso 3,400/45,591 | ETA: 05:59:45\n",
      "Paso 3,400: Loss = 0.5469\n",
      "Learning Rate: 1.87e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,450: Loss = 0.5861\n",
      "Learning Rate: 1.87e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.7% - Paso 3,500/45,591 | ETA: 05:55:08\n",
      "Paso 3,500: Loss = 0.5527\n",
      "Learning Rate: 1.87e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,550: Loss = 0.5981\n",
      "Learning Rate: 1.86e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 7.9% - Paso 3,600/45,591 | ETA: 05:50:28\n",
      "Paso 3,600: Loss = 0.5516\n",
      "Learning Rate: 1.86e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,650: Loss = 0.5531\n",
      "Learning Rate: 1.86e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.1% - Paso 3,700/45,591 | ETA: 05:45:49\n",
      "Paso 3,700: Loss = 0.5250\n",
      "Learning Rate: 1.86e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,750: Loss = 0.5324\n",
      "Learning Rate: 1.86e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.3% - Paso 3,800/45,591 | ETA: 05:41:11\n",
      "Paso 3,800: Loss = 0.5536\n",
      "Learning Rate: 1.85e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,850: Loss = 0.5592\n",
      "Learning Rate: 1.85e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.6% - Paso 3,900/45,591 | ETA: 05:36:36\n",
      "Paso 3,900: Loss = 0.5541\n",
      "Learning Rate: 1.85e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 3,950: Loss = 0.5567\n",
      "Learning Rate: 1.85e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 8.8% - Paso 4,000/45,591 | ETA: 05:32:03\n",
      "Paso 4,000: Loss = 0.5208\n",
      "Learning Rate: 1.84e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,050: Loss = 0.5784\n",
      "Learning Rate: 1.84e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.0% - Paso 4,100/45,591 | ETA: 05:27:32\n",
      "Paso 4,100: Loss = 0.5569\n",
      "Learning Rate: 1.84e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,150: Loss = 0.5405\n",
      "Learning Rate: 1.84e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.2% - Paso 4,200/45,591 | ETA: 05:23:01\n",
      "Paso 4,200: Loss = 0.5468\n",
      "Learning Rate: 1.84e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,250: Loss = 0.5584\n",
      "Learning Rate: 1.83e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.4% - Paso 4,300/45,591 | ETA: 05:18:29\n",
      "Paso 4,300: Loss = 0.5248\n",
      "Learning Rate: 1.83e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,350: Loss = 0.5937\n",
      "Learning Rate: 1.83e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.7% - Paso 4,400/45,591 | ETA: 05:14:02\n",
      "Paso 4,400: Loss = 0.5303\n",
      "Learning Rate: 1.83e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,450: Loss = 0.5996\n",
      "Learning Rate: 1.83e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 9.9% - Paso 4,500/45,591 | ETA: 05:09:34\n",
      "Paso 4,500: Loss = 0.5243\n",
      "Learning Rate: 1.82e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,550: Loss = 0.5716\n",
      "Learning Rate: 1.82e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10.1% - Paso 4,600/45,591 | ETA: 05:05:07\n",
      "Paso 4,600: Loss = 0.5231\n",
      "Learning Rate: 1.82e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,650: Loss = 0.5412\n",
      "Learning Rate: 1.82e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10.3% - Paso 4,700/45,591 | ETA: 05:00:40\n",
      "Paso 4,700: Loss = 0.5290\n",
      "Learning Rate: 1.81e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,750: Loss = 0.5593\n",
      "Learning Rate: 1.81e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10.5% - Paso 4,800/45,591 | ETA: 04:56:14\n",
      "Paso 4,800: Loss = 0.5604\n",
      "Learning Rate: 1.81e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,850: Loss = 0.5486\n",
      "Learning Rate: 1.81e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 10.7% - Paso 4,900/45,591 | ETA: 04:51:48\n",
      "Paso 4,900: Loss = 0.5398\n",
      "Learning Rate: 1.81e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 4,950: Loss = 0.5385\n",
      "Learning Rate: 1.80e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.0% - Paso 5,000/45,591 | ETA: 04:47:23\n",
      "Paso 5,000: Loss = 0.5855\n",
      "Learning Rate: 1.80e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,050: Loss = 0.5871\n",
      "Learning Rate: 1.80e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.2% - Paso 5,100/45,591 | ETA: 04:42:57\n",
      "Paso 5,100: Loss = 0.5858\n",
      "Learning Rate: 1.80e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,150: Loss = 0.5376\n",
      "Learning Rate: 1.79e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.4% - Paso 5,200/45,591 | ETA: 04:38:32\n",
      "Paso 5,200: Loss = 0.5405\n",
      "Learning Rate: 1.79e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,250: Loss = 0.5728\n",
      "Learning Rate: 1.79e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.6% - Paso 5,300/45,591 | ETA: 04:34:07\n",
      "Paso 5,300: Loss = 0.5769\n",
      "Learning Rate: 1.79e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,350: Loss = 0.5153\n",
      "Learning Rate: 1.79e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 11.8% - Paso 5,400/45,591 | ETA: 04:29:45\n",
      "Paso 5,400: Loss = 0.5769\n",
      "Learning Rate: 1.78e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,450: Loss = 0.5265\n",
      "Learning Rate: 1.78e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.1% - Paso 5,500/45,591 | ETA: 04:25:25\n",
      "Paso 5,500: Loss = 0.5552\n",
      "Learning Rate: 1.78e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,550: Loss = 0.5249\n",
      "Learning Rate: 1.78e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.3% - Paso 5,600/45,591 | ETA: 04:21:01\n",
      "Paso 5,600: Loss = 0.5811\n",
      "Learning Rate: 1.77e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,650: Loss = 0.5834\n",
      "Learning Rate: 1.77e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.5% - Paso 5,700/45,591 | ETA: 04:16:38\n",
      "Paso 5,700: Loss = 0.5494\n",
      "Learning Rate: 1.77e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,750: Loss = 0.5291\n",
      "Learning Rate: 1.77e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.7% - Paso 5,800/45,591 | ETA: 04:12:14\n",
      "Paso 5,800: Loss = 0.5787\n",
      "Learning Rate: 1.77e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,850: Loss = 0.5606\n",
      "Learning Rate: 1.76e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 12.9% - Paso 5,900/45,591 | ETA: 04:07:50\n",
      "Paso 5,900: Loss = 0.5174\n",
      "Learning Rate: 1.76e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 5,950: Loss = 0.5541\n",
      "Learning Rate: 1.76e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13.2% - Paso 6,000/45,591 | ETA: 04:03:27\n",
      "Paso 6,000: Loss = 0.5359\n",
      "Learning Rate: 1.76e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,050: Loss = 0.5527\n",
      "Learning Rate: 1.75e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13.4% - Paso 6,100/45,591 | ETA: 03:59:04\n",
      "Paso 6,100: Loss = 0.5412\n",
      "Learning Rate: 1.75e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,150: Loss = 0.5688\n",
      "Learning Rate: 1.75e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13.6% - Paso 6,200/45,591 | ETA: 03:54:44\n",
      "Paso 6,200: Loss = 0.5912\n",
      "Learning Rate: 1.75e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,250: Loss = 0.5434\n",
      "Learning Rate: 1.75e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 13.8% - Paso 6,300/45,591 | ETA: 03:50:24\n",
      "Paso 6,300: Loss = 0.5593\n",
      "Learning Rate: 1.74e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,350: Loss = 0.5195\n",
      "Learning Rate: 1.74e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.0% - Paso 6,400/45,591 | ETA: 03:46:03\n",
      "Paso 6,400: Loss = 0.5369\n",
      "Learning Rate: 1.74e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,450: Loss = 0.5543\n",
      "Learning Rate: 1.74e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.3% - Paso 6,500/45,591 | ETA: 03:41:43\n",
      "Paso 6,500: Loss = 0.5501\n",
      "Learning Rate: 1.73e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,550: Loss = 0.5272\n",
      "Learning Rate: 1.73e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.5% - Paso 6,600/45,591 | ETA: 03:37:21\n",
      "Paso 6,600: Loss = 0.5660\n",
      "Learning Rate: 1.73e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,650: Loss = 0.5644\n",
      "Learning Rate: 1.73e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.7% - Paso 6,700/45,591 | ETA: 03:33:01\n",
      "Paso 6,700: Loss = 0.5220\n",
      "Learning Rate: 1.73e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,750: Loss = 0.5514\n",
      "Learning Rate: 1.72e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 14.9% - Paso 6,800/45,591 | ETA: 03:28:42\n",
      "Paso 6,800: Loss = 0.5368\n",
      "Learning Rate: 1.72e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,850: Loss = 0.5081\n",
      "Learning Rate: 1.72e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.1% - Paso 6,900/45,591 | ETA: 03:24:24\n",
      "Paso 6,900: Loss = 0.5676\n",
      "Learning Rate: 1.72e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 6,950: Loss = 0.5524\n",
      "Learning Rate: 1.71e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.4% - Paso 7,000/45,591 | ETA: 03:20:06\n",
      "Paso 7,000: Loss = 0.5582\n",
      "Learning Rate: 1.71e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,050: Loss = 0.5340\n",
      "Learning Rate: 1.71e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.6% - Paso 7,100/45,591 | ETA: 03:15:46\n",
      "Paso 7,100: Loss = 0.5372\n",
      "Learning Rate: 1.71e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,150: Loss = 0.5496\n",
      "Learning Rate: 1.71e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 15.8% - Paso 7,200/45,591 | ETA: 03:11:27\n",
      "Paso 7,200: Loss = 0.5558\n",
      "Learning Rate: 1.70e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,250: Loss = 0.5221\n",
      "Learning Rate: 1.70e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.0% - Paso 7,300/45,591 | ETA: 03:07:07\n",
      "Paso 7,300: Loss = 0.5355\n",
      "Learning Rate: 1.70e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,350: Loss = 0.5513\n",
      "Learning Rate: 1.70e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.2% - Paso 7,400/45,591 | ETA: 03:02:48\n",
      "Paso 7,400: Loss = 0.4959\n",
      "Learning Rate: 1.69e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,450: Loss = 0.5617\n",
      "Learning Rate: 1.69e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.5% - Paso 7,500/45,591 | ETA: 02:58:28\n",
      "Paso 7,500: Loss = 0.5356\n",
      "Learning Rate: 1.69e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,550: Loss = 0.5611\n",
      "Learning Rate: 1.69e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.7% - Paso 7,600/45,591 | ETA: 02:54:09\n",
      "Paso 7,600: Loss = 0.5394\n",
      "Learning Rate: 1.69e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,650: Loss = 0.5520\n",
      "Learning Rate: 1.68e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 16.9% - Paso 7,700/45,591 | ETA: 02:49:50\n",
      "Paso 7,700: Loss = 0.5280\n",
      "Learning Rate: 1.68e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,750: Loss = 0.5547\n",
      "Learning Rate: 1.68e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17.1% - Paso 7,800/45,591 | ETA: 02:45:32\n",
      "Paso 7,800: Loss = 0.5508\n",
      "Learning Rate: 1.68e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,850: Loss = 0.5447\n",
      "Learning Rate: 1.67e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17.3% - Paso 7,900/45,591 | ETA: 02:41:13\n",
      "Paso 7,900: Loss = 0.5679\n",
      "Learning Rate: 1.67e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 7,950: Loss = 0.5278\n",
      "Learning Rate: 1.67e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17.5% - Paso 8,000/45,591 | ETA: 02:36:54\n",
      "Paso 8,000: Loss = 0.5546\n",
      "Learning Rate: 1.67e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,050: Loss = 0.5302\n",
      "Learning Rate: 1.67e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 17.8% - Paso 8,100/45,591 | ETA: 02:32:35\n",
      "Paso 8,100: Loss = 0.5639\n",
      "Learning Rate: 1.66e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,150: Loss = 0.5253\n",
      "Learning Rate: 1.66e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.0% - Paso 8,200/45,591 | ETA: 02:28:17\n",
      "Paso 8,200: Loss = 0.5501\n",
      "Learning Rate: 1.66e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,250: Loss = 0.5379\n",
      "Learning Rate: 1.66e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.2% - Paso 8,300/45,591 | ETA: 02:23:57\n",
      "Paso 8,300: Loss = 0.5683\n",
      "Learning Rate: 1.65e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,350: Loss = 0.5136\n",
      "Learning Rate: 1.65e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.4% - Paso 8,400/45,591 | ETA: 02:19:39\n",
      "Paso 8,400: Loss = 0.5218\n",
      "Learning Rate: 1.65e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,450: Loss = 0.5855\n",
      "Learning Rate: 1.65e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.6% - Paso 8,500/45,591 | ETA: 02:15:25\n",
      "Paso 8,500: Loss = 0.5280\n",
      "Learning Rate: 1.65e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,550: Loss = 0.5307\n",
      "Learning Rate: 1.64e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 18.9% - Paso 8,600/45,591 | ETA: 02:11:01\n",
      "Paso 8,600: Loss = 0.5741\n",
      "Learning Rate: 1.64e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,650: Loss = 0.5591\n",
      "Learning Rate: 1.64e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.1% - Paso 8,700/45,591 | ETA: 02:06:43\n",
      "Paso 8,700: Loss = 0.5346\n",
      "Learning Rate: 1.64e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,750: Loss = 0.5323\n",
      "Learning Rate: 1.63e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.3% - Paso 8,800/45,591 | ETA: 02:02:25\n",
      "Paso 8,800: Loss = 0.5058\n",
      "Learning Rate: 1.63e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,850: Loss = 0.5804\n",
      "Learning Rate: 1.63e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.5% - Paso 8,900/45,591 | ETA: 01:58:07\n",
      "Paso 8,900: Loss = 0.5227\n",
      "Learning Rate: 1.63e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 8,950: Loss = 0.5429\n",
      "Learning Rate: 1.63e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 19.7% - Paso 9,000/45,591 | ETA: 01:53:50\n",
      "Paso 9,000: Loss = 0.5144\n",
      "Learning Rate: 1.62e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,050: Loss = 0.5112\n",
      "Learning Rate: 1.62e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.0% - Paso 9,100/45,591 | ETA: 01:49:31\n",
      "Paso 9,100: Loss = 0.4829\n",
      "Learning Rate: 1.62e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,150: Loss = 0.5453\n",
      "Learning Rate: 1.62e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.2% - Paso 9,200/45,591 | ETA: 01:45:14\n",
      "Paso 9,200: Loss = 0.5472\n",
      "Learning Rate: 1.61e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,250: Loss = 0.5583\n",
      "Learning Rate: 1.61e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.4% - Paso 9,300/45,591 | ETA: 01:40:58\n",
      "Paso 9,300: Loss = 0.5426\n",
      "Learning Rate: 1.61e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,350: Loss = 0.5324\n",
      "Learning Rate: 1.61e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.6% - Paso 9,400/45,591 | ETA: 01:36:41\n",
      "Paso 9,400: Loss = 0.5238\n",
      "Learning Rate: 1.61e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,450: Loss = 0.5495\n",
      "Learning Rate: 1.60e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 20.8% - Paso 9,500/45,591 | ETA: 01:32:24\n",
      "Paso 9,500: Loss = 0.5185\n",
      "Learning Rate: 1.60e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,550: Loss = 0.5286\n",
      "Learning Rate: 1.60e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.1% - Paso 9,600/45,591 | ETA: 01:28:06\n",
      "Paso 9,600: Loss = 0.5192\n",
      "Learning Rate: 1.60e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,650: Loss = 0.5888\n",
      "Learning Rate: 1.59e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.3% - Paso 9,700/45,591 | ETA: 01:23:50\n",
      "Paso 9,700: Loss = 0.5451\n",
      "Learning Rate: 1.59e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,750: Loss = 0.5080\n",
      "Learning Rate: 1.59e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.5% - Paso 9,800/45,591 | ETA: 01:19:35\n",
      "Paso 9,800: Loss = 0.5309\n",
      "Learning Rate: 1.59e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,850: Loss = 0.5192\n",
      "Learning Rate: 1.59e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.7% - Paso 9,900/45,591 | ETA: 01:15:19\n",
      "Paso 9,900: Loss = 0.5528\n",
      "Learning Rate: 1.58e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 9,950: Loss = 0.5293\n",
      "Learning Rate: 1.58e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 21.9% - Paso 10,000/45,591 | ETA: 01:11:03\n",
      "Paso 10,000: Loss = 0.5171\n",
      "Learning Rate: 1.58e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,050: Loss = 0.5241\n",
      "Learning Rate: 1.58e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.2% - Paso 10,100/45,591 | ETA: 01:06:47\n",
      "Paso 10,100: Loss = 0.5127\n",
      "Learning Rate: 1.57e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,150: Loss = 0.5664\n",
      "Learning Rate: 1.57e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.4% - Paso 10,200/45,591 | ETA: 01:02:31\n",
      "Paso 10,200: Loss = 0.5412\n",
      "Learning Rate: 1.57e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,250: Loss = 0.5232\n",
      "Learning Rate: 1.57e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.6% - Paso 10,300/45,591 | ETA: 00:58:15\n",
      "Paso 10,300: Loss = 0.5204\n",
      "Learning Rate: 1.57e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,350: Loss = 0.5404\n",
      "Learning Rate: 1.56e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 22.8% - Paso 10,400/45,591 | ETA: 00:53:58\n",
      "Paso 10,400: Loss = 0.5120\n",
      "Learning Rate: 1.56e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,450: Loss = 0.5381\n",
      "Learning Rate: 1.56e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.0% - Paso 10,500/45,591 | ETA: 00:49:41\n",
      "Paso 10,500: Loss = 0.5567\n",
      "Learning Rate: 1.56e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,550: Loss = 0.5178\n",
      "Learning Rate: 1.55e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.3% - Paso 10,600/45,591 | ETA: 00:45:25\n",
      "Paso 10,600: Loss = 0.5352\n",
      "Learning Rate: 1.55e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,650: Loss = 0.5657\n",
      "Learning Rate: 1.55e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.5% - Paso 10,700/45,591 | ETA: 00:41:09\n",
      "Paso 10,700: Loss = 0.5434\n",
      "Learning Rate: 1.55e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,750: Loss = 0.5492\n",
      "Learning Rate: 1.55e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.7% - Paso 10,800/45,591 | ETA: 00:36:53\n",
      "Paso 10,800: Loss = 0.4972\n",
      "Learning Rate: 1.54e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,850: Loss = 0.5155\n",
      "Learning Rate: 1.54e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 23.9% - Paso 10,900/45,591 | ETA: 00:32:36\n",
      "Paso 10,900: Loss = 0.5185\n",
      "Learning Rate: 1.54e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 10,950: Loss = 0.5529\n",
      "Learning Rate: 1.54e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24.1% - Paso 11,000/45,591 | ETA: 00:28:20\n",
      "Paso 11,000: Loss = 0.5303\n",
      "Learning Rate: 1.53e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,050: Loss = 0.5196\n",
      "Learning Rate: 1.53e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24.3% - Paso 11,100/45,591 | ETA: 00:24:04\n",
      "Paso 11,100: Loss = 0.5386\n",
      "Learning Rate: 1.53e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,150: Loss = 0.5542\n",
      "Learning Rate: 1.53e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24.6% - Paso 11,200/45,591 | ETA: 00:19:50\n",
      "Paso 11,200: Loss = 0.5125\n",
      "Learning Rate: 1.53e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,250: Loss = 0.5287\n",
      "Learning Rate: 1.52e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 24.8% - Paso 11,300/45,591 | ETA: 00:15:34\n",
      "Paso 11,300: Loss = 0.5307\n",
      "Learning Rate: 1.52e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,350: Loss = 0.5063\n",
      "Learning Rate: 1.52e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.0% - Paso 11,400/45,591 | ETA: 00:11:19\n",
      "Paso 11,400: Loss = 0.4889\n",
      "Learning Rate: 1.52e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,450: Loss = 0.5289\n",
      "Learning Rate: 1.51e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.2% - Paso 11,500/45,591 | ETA: 00:07:03\n",
      "Paso 11,500: Loss = 0.5044\n",
      "Learning Rate: 1.51e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,550: Loss = 0.5271\n",
      "Learning Rate: 1.51e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.4% - Paso 11,600/45,591 | ETA: 00:02:45\n",
      "Paso 11,600: Loss = 0.5702\n",
      "Learning Rate: 1.51e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,650: Loss = 0.4807\n",
      "Learning Rate: 1.51e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.7% - Paso 11,700/45,591 | ETA: 23:58:29\n",
      "Paso 11,700: Loss = 0.5287\n",
      "Learning Rate: 1.50e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,750: Loss = 0.5315\n",
      "Learning Rate: 1.50e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 25.9% - Paso 11,800/45,591 | ETA: 23:54:12\n",
      "Paso 11,800: Loss = 0.5392\n",
      "Learning Rate: 1.50e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,850: Loss = 0.5579\n",
      "Learning Rate: 1.50e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.1% - Paso 11,900/45,591 | ETA: 23:49:56\n",
      "Paso 11,900: Loss = 0.5487\n",
      "Learning Rate: 1.49e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 11,950: Loss = 0.5265\n",
      "Learning Rate: 1.49e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.3% - Paso 12,000/45,591 | ETA: 23:45:40\n",
      "Paso 12,000: Loss = 0.5343\n",
      "Learning Rate: 1.49e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,050: Loss = 0.5439\n",
      "Learning Rate: 1.49e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.5% - Paso 12,100/45,591 | ETA: 23:41:24\n",
      "Paso 12,100: Loss = 0.5494\n",
      "Learning Rate: 1.49e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,150: Loss = 0.5312\n",
      "Learning Rate: 1.48e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 26.8% - Paso 12,200/45,591 | ETA: 23:37:08\n",
      "Paso 12,200: Loss = 0.5361\n",
      "Learning Rate: 1.48e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,250: Loss = 0.5215\n",
      "Learning Rate: 1.48e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.0% - Paso 12,300/45,591 | ETA: 23:32:52\n",
      "Paso 12,300: Loss = 0.4989\n",
      "Learning Rate: 1.48e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,350: Loss = 0.5129\n",
      "Learning Rate: 1.47e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.2% - Paso 12,400/45,591 | ETA: 23:28:35\n",
      "Paso 12,400: Loss = 0.4937\n",
      "Learning Rate: 1.47e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,450: Loss = 0.5355\n",
      "Learning Rate: 1.47e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.4% - Paso 12,500/45,591 | ETA: 23:24:20\n",
      "Paso 12,500: Loss = 0.5265\n",
      "Learning Rate: 1.47e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,550: Loss = 0.5763\n",
      "Learning Rate: 1.47e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.6% - Paso 12,600/45,591 | ETA: 23:20:05\n",
      "Paso 12,600: Loss = 0.5127\n",
      "Learning Rate: 1.46e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,650: Loss = 0.5744\n",
      "Learning Rate: 1.46e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 27.9% - Paso 12,700/45,591 | ETA: 23:15:51\n",
      "Paso 12,700: Loss = 0.5254\n",
      "Learning Rate: 1.46e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,750: Loss = 0.5616\n",
      "Learning Rate: 1.46e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.1% - Paso 12,800/45,591 | ETA: 23:11:33\n",
      "Paso 12,800: Loss = 0.5401\n",
      "Learning Rate: 1.45e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,850: Loss = 0.5233\n",
      "Learning Rate: 1.45e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.3% - Paso 12,900/45,591 | ETA: 23:07:18\n",
      "Paso 12,900: Loss = 0.5278\n",
      "Learning Rate: 1.45e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 12,950: Loss = 0.5229\n",
      "Learning Rate: 1.45e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.5% - Paso 13,000/45,591 | ETA: 23:03:05\n",
      "Paso 13,000: Loss = 0.5332\n",
      "Learning Rate: 1.45e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,050: Loss = 0.5176\n",
      "Learning Rate: 1.44e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 28.7% - Paso 13,100/45,591 | ETA: 22:58:49\n",
      "Paso 13,100: Loss = 0.5126\n",
      "Learning Rate: 1.44e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,150: Loss = 0.5274\n",
      "Learning Rate: 1.44e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.0% - Paso 13,200/45,591 | ETA: 22:54:33\n",
      "Paso 13,200: Loss = 0.5630\n",
      "Learning Rate: 1.44e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,250: Loss = 0.5536\n",
      "Learning Rate: 1.43e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.2% - Paso 13,300/45,591 | ETA: 22:50:17\n",
      "Paso 13,300: Loss = 0.4762\n",
      "Learning Rate: 1.43e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,350: Loss = 0.5165\n",
      "Learning Rate: 1.43e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.4% - Paso 13,400/45,591 | ETA: 22:46:01\n",
      "Paso 13,400: Loss = 0.5045\n",
      "Learning Rate: 1.43e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,450: Loss = 0.5598\n",
      "Learning Rate: 1.43e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.6% - Paso 13,500/45,591 | ETA: 22:41:45\n",
      "Paso 13,500: Loss = 0.4895\n",
      "Learning Rate: 1.42e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,550: Loss = 0.5254\n",
      "Learning Rate: 1.42e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 29.8% - Paso 13,600/45,591 | ETA: 22:37:30\n",
      "Paso 13,600: Loss = 0.5330\n",
      "Learning Rate: 1.42e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,650: Loss = 0.5086\n",
      "Learning Rate: 1.42e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.0% - Paso 13,700/45,591 | ETA: 22:33:15\n",
      "Paso 13,700: Loss = 0.5196\n",
      "Learning Rate: 1.41e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,750: Loss = 0.4926\n",
      "Learning Rate: 1.41e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.3% - Paso 13,800/45,591 | ETA: 22:28:59\n",
      "Paso 13,800: Loss = 0.5163\n",
      "Learning Rate: 1.41e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,850: Loss = 0.5241\n",
      "Learning Rate: 1.41e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.5% - Paso 13,900/45,591 | ETA: 22:24:44\n",
      "Paso 13,900: Loss = 0.5471\n",
      "Learning Rate: 1.41e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 13,950: Loss = 0.5115\n",
      "Learning Rate: 1.40e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.7% - Paso 14,000/45,591 | ETA: 22:20:30\n",
      "Paso 14,000: Loss = 0.4979\n",
      "Learning Rate: 1.40e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,050: Loss = 0.5031\n",
      "Learning Rate: 1.40e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 30.9% - Paso 14,100/45,591 | ETA: 22:16:20\n",
      "Paso 14,100: Loss = 0.5264\n",
      "Learning Rate: 1.40e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,150: Loss = 0.5708\n",
      "Learning Rate: 1.39e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31.1% - Paso 14,200/45,591 | ETA: 22:12:09\n",
      "Paso 14,200: Loss = 0.5331\n",
      "Learning Rate: 1.39e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,250: Loss = 0.5311\n",
      "Learning Rate: 1.39e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31.4% - Paso 14,300/45,591 | ETA: 22:07:54\n",
      "Paso 14,300: Loss = 0.5250\n",
      "Learning Rate: 1.39e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,350: Loss = 0.5484\n",
      "Learning Rate: 1.39e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31.6% - Paso 14,400/45,591 | ETA: 22:03:39\n",
      "Paso 14,400: Loss = 0.5106\n",
      "Learning Rate: 1.38e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,450: Loss = 0.5178\n",
      "Learning Rate: 1.38e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 31.8% - Paso 14,500/45,591 | ETA: 21:59:23\n",
      "Paso 14,500: Loss = 0.5189\n",
      "Learning Rate: 1.38e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,550: Loss = 0.5115\n",
      "Learning Rate: 1.38e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.0% - Paso 14,600/45,591 | ETA: 21:55:08\n",
      "Paso 14,600: Loss = 0.4960\n",
      "Learning Rate: 1.37e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,650: Loss = 0.5315\n",
      "Learning Rate: 1.37e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.2% - Paso 14,700/45,591 | ETA: 21:50:53\n",
      "Paso 14,700: Loss = 0.5439\n",
      "Learning Rate: 1.37e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,750: Loss = 0.5546\n",
      "Learning Rate: 1.37e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.5% - Paso 14,800/45,591 | ETA: 21:46:45\n",
      "Paso 14,800: Loss = 0.5215\n",
      "Learning Rate: 1.37e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,850: Loss = 0.5069\n",
      "Learning Rate: 1.36e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.7% - Paso 14,900/45,591 | ETA: 21:42:29\n",
      "Paso 14,900: Loss = 0.5441\n",
      "Learning Rate: 1.36e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 14,950: Loss = 0.5067\n",
      "Learning Rate: 1.36e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 32.9% - Paso 15,000/45,591 | ETA: 21:38:14\n",
      "Paso 15,000: Loss = 0.5205\n",
      "Learning Rate: 1.36e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 15,050: Loss = 0.4994\n",
      "Learning Rate: 1.36e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.1% - Paso 15,100/45,591 | ETA: 21:33:59\n",
      "Paso 15,100: Loss = 0.5217\n",
      "Learning Rate: 1.35e-05\n",
      "Velocidad: 0.39 pasos/seg\n",
      "Paso 15,150: Loss = 0.5147\n",
      "Learning Rate: 1.35e-05\n",
      "Velocidad: 0.39 pasos/seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60785\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Época 1 completada en 644.9 minutos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_multilingual_optimized\\checkpoint-15197\n",
      "Configuration saved in ./results_multilingual_optimized\\checkpoint-15197\\config.json\n",
      "Model weights saved in ./results_multilingual_optimized\\checkpoint-15197\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 ÉPOCA 2/3 - Paso 15,197/45,591\n",
      "--------------------------------------------------\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.3% - Paso 15,200/45,591 | ETA: 22:31:44\n",
      "Paso 15,200: Loss = 0.4960\n",
      "Learning Rate: 1.35e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 15,250: Loss = 0.4846\n",
      "Learning Rate: 1.35e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.6% - Paso 15,300/45,591 | ETA: 22:26:53\n",
      "Paso 15,300: Loss = 0.4881\n",
      "Learning Rate: 1.34e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 15,350: Loss = 0.5204\n",
      "Learning Rate: 1.34e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 33.8% - Paso 15,400/45,591 | ETA: 22:22:05\n",
      "Paso 15,400: Loss = 0.4995\n",
      "Learning Rate: 1.34e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 15,450: Loss = 0.5168\n",
      "Learning Rate: 1.34e-05\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.0% - Paso 15,500/45,591 | ETA: 22:17:14\n",
      "Paso 15,500: Loss = 0.4843\n",
      "Learning Rate: 1.34e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 15,550: Loss = 0.5171\n",
      "Learning Rate: 1.33e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.2% - Paso 15,600/45,591 | ETA: 22:12:25\n",
      "Paso 15,600: Loss = 0.4962\n",
      "Learning Rate: 1.33e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 15,650: Loss = 0.4872\n",
      "Learning Rate: 1.33e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.4% - Paso 15,700/45,591 | ETA: 22:07:38\n",
      "Paso 15,700: Loss = 0.4693\n",
      "Learning Rate: 1.33e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 15,750: Loss = 0.4641\n",
      "Learning Rate: 1.32e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.7% - Paso 15,800/45,591 | ETA: 22:02:57\n",
      "Paso 15,800: Loss = 0.5208\n",
      "Learning Rate: 1.32e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 15,850: Loss = 0.5040\n",
      "Learning Rate: 1.32e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░] 34.9% - Paso 15,900/45,591 | ETA: 21:58:09\n",
      "Paso 15,900: Loss = 0.4819\n",
      "Learning Rate: 1.32e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 15,950: Loss = 0.5229\n",
      "Learning Rate: 1.32e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.1% - Paso 16,000/45,591 | ETA: 21:53:21\n",
      "Paso 16,000: Loss = 0.4907\n",
      "Learning Rate: 1.31e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,050: Loss = 0.4948\n",
      "Learning Rate: 1.31e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.3% - Paso 16,100/45,591 | ETA: 21:48:32\n",
      "Paso 16,100: Loss = 0.5098\n",
      "Learning Rate: 1.31e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,150: Loss = 0.5465\n",
      "Learning Rate: 1.31e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.5% - Paso 16,200/45,591 | ETA: 21:43:44\n",
      "Paso 16,200: Loss = 0.4742\n",
      "Learning Rate: 1.30e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,250: Loss = 0.5185\n",
      "Learning Rate: 1.30e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 35.8% - Paso 16,300/45,591 | ETA: 21:38:57\n",
      "Paso 16,300: Loss = 0.5203\n",
      "Learning Rate: 1.30e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,350: Loss = 0.4947\n",
      "Learning Rate: 1.30e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.0% - Paso 16,400/45,591 | ETA: 21:34:11\n",
      "Paso 16,400: Loss = 0.5252\n",
      "Learning Rate: 1.30e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,450: Loss = 0.5110\n",
      "Learning Rate: 1.29e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.2% - Paso 16,500/45,591 | ETA: 21:29:23\n",
      "Paso 16,500: Loss = 0.4832\n",
      "Learning Rate: 1.29e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,550: Loss = 0.5104\n",
      "Learning Rate: 1.29e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.4% - Paso 16,600/45,591 | ETA: 21:24:38\n",
      "Paso 16,600: Loss = 0.5410\n",
      "Learning Rate: 1.29e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,650: Loss = 0.5025\n",
      "Learning Rate: 1.28e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.6% - Paso 16,700/45,591 | ETA: 21:19:54\n",
      "Paso 16,700: Loss = 0.4730\n",
      "Learning Rate: 1.28e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,750: Loss = 0.5036\n",
      "Learning Rate: 1.28e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 36.8% - Paso 16,800/45,591 | ETA: 21:15:08\n",
      "Paso 16,800: Loss = 0.4505\n",
      "Learning Rate: 1.28e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,850: Loss = 0.5201\n",
      "Learning Rate: 1.28e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 37.1% - Paso 16,900/45,591 | ETA: 21:10:24\n",
      "Paso 16,900: Loss = 0.4989\n",
      "Learning Rate: 1.27e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 16,950: Loss = 0.4930\n",
      "Learning Rate: 1.27e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░] 37.3% - Paso 17,000/45,591 | ETA: 21:05:39\n",
      "Paso 17,000: Loss = 0.4509\n",
      "Learning Rate: 1.27e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,050: Loss = 0.5036\n",
      "Learning Rate: 1.27e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 37.5% - Paso 17,100/45,591 | ETA: 21:00:56\n",
      "Paso 17,100: Loss = 0.5020\n",
      "Learning Rate: 1.26e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,150: Loss = 0.4861\n",
      "Learning Rate: 1.26e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 37.7% - Paso 17,200/45,591 | ETA: 20:56:12\n",
      "Paso 17,200: Loss = 0.5167\n",
      "Learning Rate: 1.26e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,250: Loss = 0.4989\n",
      "Learning Rate: 1.26e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 37.9% - Paso 17,300/45,591 | ETA: 20:51:28\n",
      "Paso 17,300: Loss = 0.4958\n",
      "Learning Rate: 1.26e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,350: Loss = 0.5128\n",
      "Learning Rate: 1.25e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38.2% - Paso 17,400/45,591 | ETA: 20:46:46\n",
      "Paso 17,400: Loss = 0.5372\n",
      "Learning Rate: 1.25e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,450: Loss = 0.4966\n",
      "Learning Rate: 1.25e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38.4% - Paso 17,500/45,591 | ETA: 20:42:03\n",
      "Paso 17,500: Loss = 0.5240\n",
      "Learning Rate: 1.25e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,550: Loss = 0.4883\n",
      "Learning Rate: 1.24e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38.6% - Paso 17,600/45,591 | ETA: 20:37:21\n",
      "Paso 17,600: Loss = 0.4534\n",
      "Learning Rate: 1.24e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,650: Loss = 0.5104\n",
      "Learning Rate: 1.24e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 38.8% - Paso 17,700/45,591 | ETA: 20:32:39\n",
      "Paso 17,700: Loss = 0.4825\n",
      "Learning Rate: 1.24e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,750: Loss = 0.5323\n",
      "Learning Rate: 1.24e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.0% - Paso 17,800/45,591 | ETA: 20:27:57\n",
      "Paso 17,800: Loss = 0.4752\n",
      "Learning Rate: 1.23e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,850: Loss = 0.5050\n",
      "Learning Rate: 1.23e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.3% - Paso 17,900/45,591 | ETA: 20:23:18\n",
      "Paso 17,900: Loss = 0.4832\n",
      "Learning Rate: 1.23e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 17,950: Loss = 0.4753\n",
      "Learning Rate: 1.23e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.5% - Paso 18,000/45,591 | ETA: 20:18:46\n",
      "Paso 18,000: Loss = 0.5349\n",
      "Learning Rate: 1.22e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,050: Loss = 0.5165\n",
      "Learning Rate: 1.22e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.7% - Paso 18,100/45,591 | ETA: 20:14:06\n",
      "Paso 18,100: Loss = 0.4817\n",
      "Learning Rate: 1.22e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,150: Loss = 0.5088\n",
      "Learning Rate: 1.22e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████░░░░░░░░░░░░░░░░░░░░░░░░░] 39.9% - Paso 18,200/45,591 | ETA: 20:09:26\n",
      "Paso 18,200: Loss = 0.4852\n",
      "Learning Rate: 1.22e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,250: Loss = 0.5182\n",
      "Learning Rate: 1.21e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.1% - Paso 18,300/45,591 | ETA: 20:04:45\n",
      "Paso 18,300: Loss = 0.4932\n",
      "Learning Rate: 1.21e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,350: Loss = 0.5194\n",
      "Learning Rate: 1.21e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.4% - Paso 18,400/45,591 | ETA: 20:00:05\n",
      "Paso 18,400: Loss = 0.4879\n",
      "Learning Rate: 1.21e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,450: Loss = 0.4983\n",
      "Learning Rate: 1.20e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.6% - Paso 18,500/45,591 | ETA: 19:55:26\n",
      "Paso 18,500: Loss = 0.5213\n",
      "Learning Rate: 1.20e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,550: Loss = 0.5185\n",
      "Learning Rate: 1.20e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 40.8% - Paso 18,600/45,591 | ETA: 19:50:47\n",
      "Paso 18,600: Loss = 0.5026\n",
      "Learning Rate: 1.20e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,650: Loss = 0.5072\n",
      "Learning Rate: 1.20e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.0% - Paso 18,700/45,591 | ETA: 19:46:08\n",
      "Paso 18,700: Loss = 0.4734\n",
      "Learning Rate: 1.19e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,750: Loss = 0.5273\n",
      "Learning Rate: 1.19e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.2% - Paso 18,800/45,591 | ETA: 19:41:28\n",
      "Paso 18,800: Loss = 0.5011\n",
      "Learning Rate: 1.19e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,850: Loss = 0.4854\n",
      "Learning Rate: 1.19e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.5% - Paso 18,900/45,591 | ETA: 19:36:50\n",
      "Paso 18,900: Loss = 0.4861\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 18,950: Loss = 0.5337\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.7% - Paso 19,000/45,591 | ETA: 19:32:11\n",
      "Paso 19,000: Loss = 0.5063\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,050: Loss = 0.4685\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 41.9% - Paso 19,100/45,591 | ETA: 19:27:33\n",
      "Paso 19,100: Loss = 0.4969\n",
      "Learning Rate: 1.18e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,150: Loss = 0.5032\n",
      "Learning Rate: 1.17e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 42.1% - Paso 19,200/45,591 | ETA: 19:22:55\n",
      "Paso 19,200: Loss = 0.5206\n",
      "Learning Rate: 1.17e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,250: Loss = 0.5136\n",
      "Learning Rate: 1.17e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████░░░░░░░░░░░░░░░░░░░░░░░░] 42.3% - Paso 19,300/45,591 | ETA: 19:18:17\n",
      "Paso 19,300: Loss = 0.4851\n",
      "Learning Rate: 1.17e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,350: Loss = 0.4641\n",
      "Learning Rate: 1.16e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 42.6% - Paso 19,400/45,591 | ETA: 19:13:40\n",
      "Paso 19,400: Loss = 0.5166\n",
      "Learning Rate: 1.16e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,450: Loss = 0.5149\n",
      "Learning Rate: 1.16e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 42.8% - Paso 19,500/45,591 | ETA: 19:09:03\n",
      "Paso 19,500: Loss = 0.4962\n",
      "Learning Rate: 1.16e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,550: Loss = 0.4838\n",
      "Learning Rate: 1.16e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.0% - Paso 19,600/45,591 | ETA: 19:05:34\n",
      "Paso 19,600: Loss = 0.5289\n",
      "Learning Rate: 1.15e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,650: Loss = 0.5025\n",
      "Learning Rate: 1.15e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.2% - Paso 19,700/45,591 | ETA: 19:01:41\n",
      "Paso 19,700: Loss = 0.5130\n",
      "Learning Rate: 1.15e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,750: Loss = 0.4852\n",
      "Learning Rate: 1.15e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.4% - Paso 19,800/45,591 | ETA: 18:57:06\n",
      "Paso 19,800: Loss = 0.4912\n",
      "Learning Rate: 1.14e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,850: Loss = 0.5048\n",
      "Learning Rate: 1.14e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.6% - Paso 19,900/45,591 | ETA: 18:52:51\n",
      "Paso 19,900: Loss = 0.4748\n",
      "Learning Rate: 1.14e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 19,950: Loss = 0.5088\n",
      "Learning Rate: 1.14e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 43.9% - Paso 20,000/45,591 | ETA: 18:48:19\n",
      "Paso 20,000: Loss = 0.5144\n",
      "Learning Rate: 1.14e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,050: Loss = 0.4786\n",
      "Learning Rate: 1.13e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.1% - Paso 20,100/45,591 | ETA: 18:43:42\n",
      "Paso 20,100: Loss = 0.5024\n",
      "Learning Rate: 1.13e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,150: Loss = 0.4780\n",
      "Learning Rate: 1.13e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.3% - Paso 20,200/45,591 | ETA: 18:39:07\n",
      "Paso 20,200: Loss = 0.5108\n",
      "Learning Rate: 1.13e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,250: Loss = 0.4985\n",
      "Learning Rate: 1.12e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.5% - Paso 20,300/45,591 | ETA: 18:34:31\n",
      "Paso 20,300: Loss = 0.4731\n",
      "Learning Rate: 1.12e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,350: Loss = 0.5358\n",
      "Learning Rate: 1.12e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 44.7% - Paso 20,400/45,591 | ETA: 18:29:53\n",
      "Paso 20,400: Loss = 0.5272\n",
      "Learning Rate: 1.12e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,450: Loss = 0.4908\n",
      "Learning Rate: 1.12e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████░░░░░░░░░░░░░░░░░░░░░░░] 45.0% - Paso 20,500/45,591 | ETA: 18:25:16\n",
      "Paso 20,500: Loss = 0.4834\n",
      "Learning Rate: 1.11e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,550: Loss = 0.5124\n",
      "Learning Rate: 1.11e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45.2% - Paso 20,600/45,591 | ETA: 18:20:40\n",
      "Paso 20,600: Loss = 0.4782\n",
      "Learning Rate: 1.11e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,650: Loss = 0.4738\n",
      "Learning Rate: 1.11e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45.4% - Paso 20,700/45,591 | ETA: 18:16:03\n",
      "Paso 20,700: Loss = 0.4831\n",
      "Learning Rate: 1.10e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,750: Loss = 0.4874\n",
      "Learning Rate: 1.10e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45.6% - Paso 20,800/45,591 | ETA: 18:11:27\n",
      "Paso 20,800: Loss = 0.5092\n",
      "Learning Rate: 1.10e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,850: Loss = 0.5142\n",
      "Learning Rate: 1.10e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 45.8% - Paso 20,900/45,591 | ETA: 18:06:51\n",
      "Paso 20,900: Loss = 0.5109\n",
      "Learning Rate: 1.10e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 20,950: Loss = 0.4495\n",
      "Learning Rate: 1.09e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.1% - Paso 21,000/45,591 | ETA: 18:02:15\n",
      "Paso 21,000: Loss = 0.5079\n",
      "Learning Rate: 1.09e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,050: Loss = 0.5083\n",
      "Learning Rate: 1.09e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.3% - Paso 21,100/45,591 | ETA: 17:57:39\n",
      "Paso 21,100: Loss = 0.5260\n",
      "Learning Rate: 1.09e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,150: Loss = 0.5062\n",
      "Learning Rate: 1.08e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.5% - Paso 21,200/45,591 | ETA: 17:53:04\n",
      "Paso 21,200: Loss = 0.4975\n",
      "Learning Rate: 1.08e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,250: Loss = 0.4839\n",
      "Learning Rate: 1.08e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.7% - Paso 21,300/45,591 | ETA: 17:48:28\n",
      "Paso 21,300: Loss = 0.5136\n",
      "Learning Rate: 1.08e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,350: Loss = 0.5236\n",
      "Learning Rate: 1.08e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 46.9% - Paso 21,400/45,591 | ETA: 17:43:53\n",
      "Paso 21,400: Loss = 0.5214\n",
      "Learning Rate: 1.07e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,450: Loss = 0.5219\n",
      "Learning Rate: 1.07e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 47.2% - Paso 21,500/45,591 | ETA: 17:39:18\n",
      "Paso 21,500: Loss = 0.4673\n",
      "Learning Rate: 1.07e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,550: Loss = 0.4980\n",
      "Learning Rate: 1.07e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████░░░░░░░░░░░░░░░░░░░░░░] 47.4% - Paso 21,600/45,591 | ETA: 17:34:44\n",
      "Paso 21,600: Loss = 0.4884\n",
      "Learning Rate: 1.06e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,650: Loss = 0.4891\n",
      "Learning Rate: 1.06e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 47.6% - Paso 21,700/45,591 | ETA: 17:30:09\n",
      "Paso 21,700: Loss = 0.5013\n",
      "Learning Rate: 1.06e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,750: Loss = 0.4813\n",
      "Learning Rate: 1.06e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 47.8% - Paso 21,800/45,591 | ETA: 17:25:35\n",
      "Paso 21,800: Loss = 0.5299\n",
      "Learning Rate: 1.06e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,850: Loss = 0.5373\n",
      "Learning Rate: 1.05e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.0% - Paso 21,900/45,591 | ETA: 17:21:01\n",
      "Paso 21,900: Loss = 0.5084\n",
      "Learning Rate: 1.05e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 21,950: Loss = 0.4837\n",
      "Learning Rate: 1.05e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.3% - Paso 22,000/45,591 | ETA: 17:16:27\n",
      "Paso 22,000: Loss = 0.4980\n",
      "Learning Rate: 1.05e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,050: Loss = 0.4852\n",
      "Learning Rate: 1.04e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.5% - Paso 22,100/45,591 | ETA: 17:11:53\n",
      "Paso 22,100: Loss = 0.4748\n",
      "Learning Rate: 1.04e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,150: Loss = 0.5175\n",
      "Learning Rate: 1.04e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.7% - Paso 22,200/45,591 | ETA: 17:07:19\n",
      "Paso 22,200: Loss = 0.4998\n",
      "Learning Rate: 1.04e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,250: Loss = 0.5140\n",
      "Learning Rate: 1.04e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 48.9% - Paso 22,300/45,591 | ETA: 17:02:46\n",
      "Paso 22,300: Loss = 0.4860\n",
      "Learning Rate: 1.03e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,350: Loss = 0.4966\n",
      "Learning Rate: 1.03e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.1% - Paso 22,400/45,591 | ETA: 16:58:13\n",
      "Paso 22,400: Loss = 0.4853\n",
      "Learning Rate: 1.03e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,450: Loss = 0.5015\n",
      "Learning Rate: 1.03e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.4% - Paso 22,500/45,591 | ETA: 16:53:40\n",
      "Paso 22,500: Loss = 0.5110\n",
      "Learning Rate: 1.02e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,550: Loss = 0.5268\n",
      "Learning Rate: 1.02e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.6% - Paso 22,600/45,591 | ETA: 16:49:07\n",
      "Paso 22,600: Loss = 0.5007\n",
      "Learning Rate: 1.02e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,650: Loss = 0.4978\n",
      "Learning Rate: 1.02e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████░░░░░░░░░░░░░░░░░░░░░] 49.8% - Paso 22,700/45,591 | ETA: 16:44:34\n",
      "Paso 22,700: Loss = 0.4642\n",
      "Learning Rate: 1.02e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,750: Loss = 0.5183\n",
      "Learning Rate: 1.01e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 50.0% - Paso 22,800/45,591 | ETA: 16:40:02\n",
      "Paso 22,800: Loss = 0.4949\n",
      "Learning Rate: 1.01e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,850: Loss = 0.4910\n",
      "Learning Rate: 1.01e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 50.2% - Paso 22,900/45,591 | ETA: 16:35:29\n",
      "Paso 22,900: Loss = 0.4841\n",
      "Learning Rate: 1.01e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 22,950: Loss = 0.4833\n",
      "Learning Rate: 1.00e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 50.4% - Paso 23,000/45,591 | ETA: 16:30:57\n",
      "Paso 23,000: Loss = 0.4882\n",
      "Learning Rate: 1.00e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,050: Loss = 0.5028\n",
      "Learning Rate: 1.00e-05\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 50.7% - Paso 23,100/45,591 | ETA: 16:26:25\n",
      "Paso 23,100: Loss = 0.5119\n",
      "Learning Rate: 9.98e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,150: Loss = 0.5014\n",
      "Learning Rate: 9.96e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 50.9% - Paso 23,200/45,591 | ETA: 16:21:53\n",
      "Paso 23,200: Loss = 0.5173\n",
      "Learning Rate: 9.94e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,250: Loss = 0.5018\n",
      "Learning Rate: 9.91e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 51.1% - Paso 23,300/45,591 | ETA: 16:17:21\n",
      "Paso 23,300: Loss = 0.4628\n",
      "Learning Rate: 9.89e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,350: Loss = 0.5131\n",
      "Learning Rate: 9.87e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 51.3% - Paso 23,400/45,591 | ETA: 16:12:49\n",
      "Paso 23,400: Loss = 0.4878\n",
      "Learning Rate: 9.85e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,450: Loss = 0.4738\n",
      "Learning Rate: 9.83e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 51.5% - Paso 23,500/45,591 | ETA: 16:08:18\n",
      "Paso 23,500: Loss = 0.4901\n",
      "Learning Rate: 9.80e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,550: Loss = 0.4901\n",
      "Learning Rate: 9.78e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 51.8% - Paso 23,600/45,591 | ETA: 16:03:46\n",
      "Paso 23,600: Loss = 0.5154\n",
      "Learning Rate: 9.76e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,650: Loss = 0.5002\n",
      "Learning Rate: 9.74e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 52.0% - Paso 23,700/45,591 | ETA: 15:59:15\n",
      "Paso 23,700: Loss = 0.5227\n",
      "Learning Rate: 9.72e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,750: Loss = 0.5165\n",
      "Learning Rate: 9.69e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 52.2% - Paso 23,800/45,591 | ETA: 15:54:44\n",
      "Paso 23,800: Loss = 0.4318\n",
      "Learning Rate: 9.67e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,850: Loss = 0.5145\n",
      "Learning Rate: 9.65e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████░░░░░░░░░░░░░░░░░░░░] 52.4% - Paso 23,900/45,591 | ETA: 15:50:12\n",
      "Paso 23,900: Loss = 0.4748\n",
      "Learning Rate: 9.63e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 23,950: Loss = 0.4876\n",
      "Learning Rate: 9.60e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 52.6% - Paso 24,000/45,591 | ETA: 15:45:41\n",
      "Paso 24,000: Loss = 0.4791\n",
      "Learning Rate: 9.58e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,050: Loss = 0.5154\n",
      "Learning Rate: 9.56e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 52.9% - Paso 24,100/45,591 | ETA: 15:41:11\n",
      "Paso 24,100: Loss = 0.4903\n",
      "Learning Rate: 9.54e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,150: Loss = 0.4491\n",
      "Learning Rate: 9.52e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 53.1% - Paso 24,200/45,591 | ETA: 15:36:40\n",
      "Paso 24,200: Loss = 0.5204\n",
      "Learning Rate: 9.49e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,250: Loss = 0.4683\n",
      "Learning Rate: 9.47e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 53.3% - Paso 24,300/45,591 | ETA: 15:32:10\n",
      "Paso 24,300: Loss = 0.5118\n",
      "Learning Rate: 9.45e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,350: Loss = 0.5342\n",
      "Learning Rate: 9.43e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 53.5% - Paso 24,400/45,591 | ETA: 15:27:40\n",
      "Paso 24,400: Loss = 0.4982\n",
      "Learning Rate: 9.40e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,450: Loss = 0.5312\n",
      "Learning Rate: 9.38e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 53.7% - Paso 24,500/45,591 | ETA: 15:23:16\n",
      "Paso 24,500: Loss = 0.5042\n",
      "Learning Rate: 9.36e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,550: Loss = 0.5027\n",
      "Learning Rate: 9.34e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 54.0% - Paso 24,600/45,591 | ETA: 15:18:50\n",
      "Paso 24,600: Loss = 0.4672\n",
      "Learning Rate: 9.32e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,650: Loss = 0.4857\n",
      "Learning Rate: 9.29e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 54.2% - Paso 24,700/45,591 | ETA: 15:14:20\n",
      "Paso 24,700: Loss = 0.5091\n",
      "Learning Rate: 9.27e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,750: Loss = 0.5364\n",
      "Learning Rate: 9.25e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 54.4% - Paso 24,800/45,591 | ETA: 15:09:50\n",
      "Paso 24,800: Loss = 0.4600\n",
      "Learning Rate: 9.23e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,850: Loss = 0.5462\n",
      "Learning Rate: 9.21e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 54.6% - Paso 24,900/45,591 | ETA: 15:05:21\n",
      "Paso 24,900: Loss = 0.4854\n",
      "Learning Rate: 9.18e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 24,950: Loss = 0.4640\n",
      "Learning Rate: 9.16e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████░░░░░░░░░░░░░░░░░░░] 54.8% - Paso 25,000/45,591 | ETA: 15:00:51\n",
      "Paso 25,000: Loss = 0.4808\n",
      "Learning Rate: 9.14e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,050: Loss = 0.4656\n",
      "Learning Rate: 9.12e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 55.1% - Paso 25,100/45,591 | ETA: 14:56:22\n",
      "Paso 25,100: Loss = 0.4837\n",
      "Learning Rate: 9.09e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,150: Loss = 0.4944\n",
      "Learning Rate: 9.07e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 55.3% - Paso 25,200/45,591 | ETA: 14:51:53\n",
      "Paso 25,200: Loss = 0.4779\n",
      "Learning Rate: 9.05e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,250: Loss = 0.5079\n",
      "Learning Rate: 9.03e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 55.5% - Paso 25,300/45,591 | ETA: 14:47:24\n",
      "Paso 25,300: Loss = 0.4847\n",
      "Learning Rate: 9.01e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,350: Loss = 0.4906\n",
      "Learning Rate: 8.98e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 55.7% - Paso 25,400/45,591 | ETA: 14:42:55\n",
      "Paso 25,400: Loss = 0.5261\n",
      "Learning Rate: 8.96e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,450: Loss = 0.4838\n",
      "Learning Rate: 8.94e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 55.9% - Paso 25,500/45,591 | ETA: 14:38:26\n",
      "Paso 25,500: Loss = 0.4998\n",
      "Learning Rate: 8.92e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,550: Loss = 0.4745\n",
      "Learning Rate: 8.90e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 56.2% - Paso 25,600/45,591 | ETA: 14:33:58\n",
      "Paso 25,600: Loss = 0.5057\n",
      "Learning Rate: 8.87e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,650: Loss = 0.5261\n",
      "Learning Rate: 8.85e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 56.4% - Paso 25,700/45,591 | ETA: 14:29:29\n",
      "Paso 25,700: Loss = 0.5238\n",
      "Learning Rate: 8.83e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,750: Loss = 0.4826\n",
      "Learning Rate: 8.81e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 56.6% - Paso 25,800/45,591 | ETA: 14:25:00\n",
      "Paso 25,800: Loss = 0.5122\n",
      "Learning Rate: 8.78e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,850: Loss = 0.5014\n",
      "Learning Rate: 8.76e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 56.8% - Paso 25,900/45,591 | ETA: 14:20:32\n",
      "Paso 25,900: Loss = 0.4979\n",
      "Learning Rate: 8.74e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 25,950: Loss = 0.5066\n",
      "Learning Rate: 8.72e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 57.0% - Paso 26,000/45,591 | ETA: 14:16:04\n",
      "Paso 26,000: Loss = 0.5044\n",
      "Learning Rate: 8.70e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,050: Loss = 0.5044\n",
      "Learning Rate: 8.67e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 57.2% - Paso 26,100/45,591 | ETA: 14:11:36\n",
      "Paso 26,100: Loss = 0.4998\n",
      "Learning Rate: 8.65e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,150: Loss = 0.5073\n",
      "Learning Rate: 8.63e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████░░░░░░░░░░░░░░░░░░] 57.5% - Paso 26,200/45,591 | ETA: 14:07:08\n",
      "Paso 26,200: Loss = 0.4282\n",
      "Learning Rate: 8.61e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,250: Loss = 0.4932\n",
      "Learning Rate: 8.58e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 57.7% - Paso 26,300/45,591 | ETA: 14:02:40\n",
      "Paso 26,300: Loss = 0.5444\n",
      "Learning Rate: 8.56e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,350: Loss = 0.5036\n",
      "Learning Rate: 8.54e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 57.9% - Paso 26,400/45,591 | ETA: 13:58:11\n",
      "Paso 26,400: Loss = 0.4807\n",
      "Learning Rate: 8.52e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,450: Loss = 0.4597\n",
      "Learning Rate: 8.50e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 58.1% - Paso 26,500/45,591 | ETA: 13:53:44\n",
      "Paso 26,500: Loss = 0.4760\n",
      "Learning Rate: 8.47e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,550: Loss = 0.4717\n",
      "Learning Rate: 8.45e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 58.3% - Paso 26,600/45,591 | ETA: 13:49:16\n",
      "Paso 26,600: Loss = 0.4794\n",
      "Learning Rate: 8.43e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,650: Loss = 0.5012\n",
      "Learning Rate: 8.41e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 58.6% - Paso 26,700/45,591 | ETA: 13:44:49\n",
      "Paso 26,700: Loss = 0.5052\n",
      "Learning Rate: 8.39e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,750: Loss = 0.5012\n",
      "Learning Rate: 8.36e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 58.8% - Paso 26,800/45,591 | ETA: 13:40:21\n",
      "Paso 26,800: Loss = 0.4822\n",
      "Learning Rate: 8.34e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,850: Loss = 0.4977\n",
      "Learning Rate: 8.32e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 59.0% - Paso 26,900/45,591 | ETA: 13:35:54\n",
      "Paso 26,900: Loss = 0.4924\n",
      "Learning Rate: 8.30e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 26,950: Loss = 0.4944\n",
      "Learning Rate: 8.27e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 59.2% - Paso 27,000/45,591 | ETA: 13:31:27\n",
      "Paso 27,000: Loss = 0.4423\n",
      "Learning Rate: 8.25e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,050: Loss = 0.4834\n",
      "Learning Rate: 8.23e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 59.4% - Paso 27,100/45,591 | ETA: 13:26:59\n",
      "Paso 27,100: Loss = 0.4343\n",
      "Learning Rate: 8.21e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,150: Loss = 0.5281\n",
      "Learning Rate: 8.19e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 59.7% - Paso 27,200/45,591 | ETA: 13:22:32\n",
      "Paso 27,200: Loss = 0.4843\n",
      "Learning Rate: 8.16e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,250: Loss = 0.4753\n",
      "Learning Rate: 8.14e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████░░░░░░░░░░░░░░░░░] 59.9% - Paso 27,300/45,591 | ETA: 13:18:07\n",
      "Paso 27,300: Loss = 0.4693\n",
      "Learning Rate: 8.12e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,350: Loss = 0.4984\n",
      "Learning Rate: 8.10e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 60.1% - Paso 27,400/45,591 | ETA: 13:13:40\n",
      "Paso 27,400: Loss = 0.4717\n",
      "Learning Rate: 8.07e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,450: Loss = 0.4683\n",
      "Learning Rate: 8.05e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 60.3% - Paso 27,500/45,591 | ETA: 13:09:13\n",
      "Paso 27,500: Loss = 0.4968\n",
      "Learning Rate: 8.03e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,550: Loss = 0.4755\n",
      "Learning Rate: 8.01e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 60.5% - Paso 27,600/45,591 | ETA: 13:04:47\n",
      "Paso 27,600: Loss = 0.4604\n",
      "Learning Rate: 7.99e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,650: Loss = 0.5180\n",
      "Learning Rate: 7.96e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 60.8% - Paso 27,700/45,591 | ETA: 13:00:20\n",
      "Paso 27,700: Loss = 0.4713\n",
      "Learning Rate: 7.94e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,750: Loss = 0.4550\n",
      "Learning Rate: 7.92e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 61.0% - Paso 27,800/45,591 | ETA: 12:55:54\n",
      "Paso 27,800: Loss = 0.4827\n",
      "Learning Rate: 7.90e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,850: Loss = 0.4796\n",
      "Learning Rate: 7.88e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 61.2% - Paso 27,900/45,591 | ETA: 12:51:27\n",
      "Paso 27,900: Loss = 0.4969\n",
      "Learning Rate: 7.85e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 27,950: Loss = 0.5119\n",
      "Learning Rate: 7.83e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 61.4% - Paso 28,000/45,591 | ETA: 12:47:01\n",
      "Paso 28,000: Loss = 0.4689\n",
      "Learning Rate: 7.81e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,050: Loss = 0.4570\n",
      "Learning Rate: 7.79e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 61.6% - Paso 28,100/45,591 | ETA: 12:42:35\n",
      "Paso 28,100: Loss = 0.5196\n",
      "Learning Rate: 7.76e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,150: Loss = 0.5034\n",
      "Learning Rate: 7.74e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 61.9% - Paso 28,200/45,591 | ETA: 12:38:09\n",
      "Paso 28,200: Loss = 0.4687\n",
      "Learning Rate: 7.72e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,250: Loss = 0.4638\n",
      "Learning Rate: 7.70e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 62.1% - Paso 28,300/45,591 | ETA: 12:33:43\n",
      "Paso 28,300: Loss = 0.5171\n",
      "Learning Rate: 7.68e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,350: Loss = 0.5209\n",
      "Learning Rate: 7.65e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████░░░░░░░░░░░░░░░░] 62.3% - Paso 28,400/45,591 | ETA: 12:29:17\n",
      "Paso 28,400: Loss = 0.4851\n",
      "Learning Rate: 7.63e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,450: Loss = 0.5096\n",
      "Learning Rate: 7.61e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 62.5% - Paso 28,500/45,591 | ETA: 12:24:52\n",
      "Paso 28,500: Loss = 0.4831\n",
      "Learning Rate: 7.59e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,550: Loss = 0.4839\n",
      "Learning Rate: 7.57e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 62.7% - Paso 28,600/45,591 | ETA: 12:20:26\n",
      "Paso 28,600: Loss = 0.4500\n",
      "Learning Rate: 7.54e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,650: Loss = 0.4983\n",
      "Learning Rate: 7.52e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 63.0% - Paso 28,700/45,591 | ETA: 12:16:00\n",
      "Paso 28,700: Loss = 0.4973\n",
      "Learning Rate: 7.50e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,750: Loss = 0.5226\n",
      "Learning Rate: 7.48e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 63.2% - Paso 28,800/45,591 | ETA: 12:11:35\n",
      "Paso 28,800: Loss = 0.4801\n",
      "Learning Rate: 7.45e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,850: Loss = 0.5109\n",
      "Learning Rate: 7.43e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 63.4% - Paso 28,900/45,591 | ETA: 12:07:09\n",
      "Paso 28,900: Loss = 0.4935\n",
      "Learning Rate: 7.41e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 28,950: Loss = 0.5045\n",
      "Learning Rate: 7.39e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 63.6% - Paso 29,000/45,591 | ETA: 12:02:44\n",
      "Paso 29,000: Loss = 0.4845\n",
      "Learning Rate: 7.37e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,050: Loss = 0.4951\n",
      "Learning Rate: 7.34e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 63.8% - Paso 29,100/45,591 | ETA: 11:58:18\n",
      "Paso 29,100: Loss = 0.4822\n",
      "Learning Rate: 7.32e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,150: Loss = 0.4671\n",
      "Learning Rate: 7.30e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 64.0% - Paso 29,200/45,591 | ETA: 11:53:53\n",
      "Paso 29,200: Loss = 0.4914\n",
      "Learning Rate: 7.28e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,250: Loss = 0.4571\n",
      "Learning Rate: 7.25e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 64.3% - Paso 29,300/45,591 | ETA: 11:49:28\n",
      "Paso 29,300: Loss = 0.4666\n",
      "Learning Rate: 7.23e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,350: Loss = 0.4835\n",
      "Learning Rate: 7.21e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 64.5% - Paso 29,400/45,591 | ETA: 11:45:03\n",
      "Paso 29,400: Loss = 0.4494\n",
      "Learning Rate: 7.19e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,450: Loss = 0.5026\n",
      "Learning Rate: 7.17e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 64.7% - Paso 29,500/45,591 | ETA: 11:40:38\n",
      "Paso 29,500: Loss = 0.4490\n",
      "Learning Rate: 7.14e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,550: Loss = 0.4962\n",
      "Learning Rate: 7.12e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████░░░░░░░░░░░░░░░] 64.9% - Paso 29,600/45,591 | ETA: 11:36:13\n",
      "Paso 29,600: Loss = 0.4544\n",
      "Learning Rate: 7.10e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,650: Loss = 0.4876\n",
      "Learning Rate: 7.08e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 65.1% - Paso 29,700/45,591 | ETA: 11:31:48\n",
      "Paso 29,700: Loss = 0.5215\n",
      "Learning Rate: 7.06e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,750: Loss = 0.5064\n",
      "Learning Rate: 7.03e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 65.4% - Paso 29,800/45,591 | ETA: 11:27:23\n",
      "Paso 29,800: Loss = 0.4635\n",
      "Learning Rate: 7.01e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,850: Loss = 0.4380\n",
      "Learning Rate: 6.99e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 65.6% - Paso 29,900/45,591 | ETA: 11:22:58\n",
      "Paso 29,900: Loss = 0.4670\n",
      "Learning Rate: 6.97e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 29,950: Loss = 0.4456\n",
      "Learning Rate: 6.94e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 65.8% - Paso 30,000/45,591 | ETA: 11:18:34\n",
      "Paso 30,000: Loss = 0.4780\n",
      "Learning Rate: 6.92e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 30,050: Loss = 0.5181\n",
      "Learning Rate: 6.90e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 66.0% - Paso 30,100/45,591 | ETA: 11:14:09\n",
      "Paso 30,100: Loss = 0.4880\n",
      "Learning Rate: 6.88e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 30,150: Loss = 0.5283\n",
      "Learning Rate: 6.86e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 66.2% - Paso 30,200/45,591 | ETA: 11:09:44\n",
      "Paso 30,200: Loss = 0.4906\n",
      "Learning Rate: 6.83e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 30,250: Loss = 0.4733\n",
      "Learning Rate: 6.81e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 66.5% - Paso 30,300/45,591 | ETA: 11:05:20\n",
      "Paso 30,300: Loss = 0.4939\n",
      "Learning Rate: 6.79e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 30,350: Loss = 0.4963\n",
      "Learning Rate: 6.77e-06\n",
      "Velocidad: 0.38 pasos/seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60785\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Época 2 completada en 646.5 minutos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_multilingual_optimized\\checkpoint-30394\n",
      "Configuration saved in ./results_multilingual_optimized\\checkpoint-30394\\config.json\n",
      "Model weights saved in ./results_multilingual_optimized\\checkpoint-30394\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 ÉPOCA 3/3 - Paso 30,394/45,591\n",
      "--------------------------------------------------\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 66.7% - Paso 30,400/45,591 | ETA: 11:16:29\n",
      "Paso 30,400: Loss = 0.5096\n",
      "Learning Rate: 6.74e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,450: Loss = 0.4504\n",
      "Learning Rate: 6.72e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 66.9% - Paso 30,500/45,591 | ETA: 11:11:56\n",
      "Paso 30,500: Loss = 0.4698\n",
      "Learning Rate: 6.70e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,550: Loss = 0.4730\n",
      "Learning Rate: 6.68e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 67.1% - Paso 30,600/45,591 | ETA: 11:07:23\n",
      "Paso 30,600: Loss = 0.4622\n",
      "Learning Rate: 6.66e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,650: Loss = 0.5179\n",
      "Learning Rate: 6.63e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[██████████████████████████░░░░░░░░░░░░░░] 67.3% - Paso 30,700/45,591 | ETA: 11:02:50\n",
      "Paso 30,700: Loss = 0.4342\n",
      "Learning Rate: 6.61e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,750: Loss = 0.4829\n",
      "Learning Rate: 6.59e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 67.6% - Paso 30,800/45,591 | ETA: 10:58:17\n",
      "Paso 30,800: Loss = 0.4809\n",
      "Learning Rate: 6.57e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,850: Loss = 0.4753\n",
      "Learning Rate: 6.54e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 67.8% - Paso 30,900/45,591 | ETA: 10:53:44\n",
      "Paso 30,900: Loss = 0.4486\n",
      "Learning Rate: 6.52e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 30,950: Loss = 0.4811\n",
      "Learning Rate: 6.50e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 68.0% - Paso 31,000/45,591 | ETA: 10:49:13\n",
      "Paso 31,000: Loss = 0.4736\n",
      "Learning Rate: 6.48e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,050: Loss = 0.4887\n",
      "Learning Rate: 6.46e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 68.2% - Paso 31,100/45,591 | ETA: 10:44:41\n",
      "Paso 31,100: Loss = 0.4377\n",
      "Learning Rate: 6.43e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,150: Loss = 0.4467\n",
      "Learning Rate: 6.41e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 68.4% - Paso 31,200/45,591 | ETA: 10:40:08\n",
      "Paso 31,200: Loss = 0.5124\n",
      "Learning Rate: 6.39e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,250: Loss = 0.4842\n",
      "Learning Rate: 6.37e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 68.7% - Paso 31,300/45,591 | ETA: 10:35:35\n",
      "Paso 31,300: Loss = 0.5049\n",
      "Learning Rate: 6.35e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,350: Loss = 0.4680\n",
      "Learning Rate: 6.32e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 68.9% - Paso 31,400/45,591 | ETA: 10:31:03\n",
      "Paso 31,400: Loss = 0.4758\n",
      "Learning Rate: 6.30e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,450: Loss = 0.4747\n",
      "Learning Rate: 6.28e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 69.1% - Paso 31,500/45,591 | ETA: 10:26:31\n",
      "Paso 31,500: Loss = 0.4441\n",
      "Learning Rate: 6.26e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,550: Loss = 0.4741\n",
      "Learning Rate: 6.23e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 69.3% - Paso 31,600/45,591 | ETA: 10:22:06\n",
      "Paso 31,600: Loss = 0.4552\n",
      "Learning Rate: 6.21e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,650: Loss = 0.4444\n",
      "Learning Rate: 6.19e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 69.5% - Paso 31,700/45,591 | ETA: 10:17:45\n",
      "Paso 31,700: Loss = 0.4767\n",
      "Learning Rate: 6.17e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,750: Loss = 0.4830\n",
      "Learning Rate: 6.15e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 69.8% - Paso 31,800/45,591 | ETA: 10:13:13\n",
      "Paso 31,800: Loss = 0.4723\n",
      "Learning Rate: 6.12e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,850: Loss = 0.4772\n",
      "Learning Rate: 6.10e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[███████████████████████████░░░░░░░░░░░░░] 70.0% - Paso 31,900/45,591 | ETA: 10:08:44\n",
      "Paso 31,900: Loss = 0.4784\n",
      "Learning Rate: 6.08e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 31,950: Loss = 0.4195\n",
      "Learning Rate: 6.06e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 70.2% - Paso 32,000/45,591 | ETA: 10:04:20\n",
      "Paso 32,000: Loss = 0.4946\n",
      "Learning Rate: 6.04e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 32,050: Loss = 0.4485\n",
      "Learning Rate: 6.01e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 70.4% - Paso 32,100/45,591 | ETA: 09:59:49\n",
      "Paso 32,100: Loss = 0.4637\n",
      "Learning Rate: 5.99e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 32,150: Loss = 0.4859\n",
      "Learning Rate: 5.97e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 70.6% - Paso 32,200/45,591 | ETA: 09:55:18\n",
      "Paso 32,200: Loss = 0.4595\n",
      "Learning Rate: 5.95e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 32,250: Loss = 0.4408\n",
      "Learning Rate: 5.92e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 70.8% - Paso 32,300/45,591 | ETA: 09:50:46\n",
      "Paso 32,300: Loss = 0.4643\n",
      "Learning Rate: 5.90e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "Paso 32,350: Loss = 0.4714\n",
      "Learning Rate: 5.88e-06\n",
      "Velocidad: 0.37 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 71.1% - Paso 32,400/45,591 | ETA: 09:46:14\n",
      "Paso 32,400: Loss = 0.4657\n",
      "Learning Rate: 5.86e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,450: Loss = 0.4608\n",
      "Learning Rate: 5.84e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 71.3% - Paso 32,500/45,591 | ETA: 09:41:43\n",
      "Paso 32,500: Loss = 0.4371\n",
      "Learning Rate: 5.81e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,550: Loss = 0.4819\n",
      "Learning Rate: 5.79e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 71.5% - Paso 32,600/45,591 | ETA: 09:37:12\n",
      "Paso 32,600: Loss = 0.4856\n",
      "Learning Rate: 5.77e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,650: Loss = 0.5128\n",
      "Learning Rate: 5.75e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 71.7% - Paso 32,700/45,591 | ETA: 09:32:40\n",
      "Paso 32,700: Loss = 0.4470\n",
      "Learning Rate: 5.73e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,750: Loss = 0.4850\n",
      "Learning Rate: 5.70e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 71.9% - Paso 32,800/45,591 | ETA: 09:28:09\n",
      "Paso 32,800: Loss = 0.4932\n",
      "Learning Rate: 5.68e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,850: Loss = 0.4346\n",
      "Learning Rate: 5.66e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 72.2% - Paso 32,900/45,591 | ETA: 09:23:38\n",
      "Paso 32,900: Loss = 0.4765\n",
      "Learning Rate: 5.64e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 32,950: Loss = 0.4523\n",
      "Learning Rate: 5.61e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████░░░░░░░░░░░░] 72.4% - Paso 33,000/45,591 | ETA: 09:19:07\n",
      "Paso 33,000: Loss = 0.4595\n",
      "Learning Rate: 5.59e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,050: Loss = 0.4384\n",
      "Learning Rate: 5.57e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 72.6% - Paso 33,100/45,591 | ETA: 09:14:36\n",
      "Paso 33,100: Loss = 0.4493\n",
      "Learning Rate: 5.55e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,150: Loss = 0.4591\n",
      "Learning Rate: 5.53e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 72.8% - Paso 33,200/45,591 | ETA: 09:10:05\n",
      "Paso 33,200: Loss = 0.4803\n",
      "Learning Rate: 5.50e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,250: Loss = 0.4685\n",
      "Learning Rate: 5.48e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 73.0% - Paso 33,300/45,591 | ETA: 09:05:34\n",
      "Paso 33,300: Loss = 0.4752\n",
      "Learning Rate: 5.46e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,350: Loss = 0.4303\n",
      "Learning Rate: 5.44e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 73.3% - Paso 33,400/45,591 | ETA: 09:01:03\n",
      "Paso 33,400: Loss = 0.4283\n",
      "Learning Rate: 5.41e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,450: Loss = 0.4356\n",
      "Learning Rate: 5.39e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 73.5% - Paso 33,500/45,591 | ETA: 08:56:33\n",
      "Paso 33,500: Loss = 0.4648\n",
      "Learning Rate: 5.37e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,550: Loss = 0.4154\n",
      "Learning Rate: 5.35e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 73.7% - Paso 33,600/45,591 | ETA: 08:52:02\n",
      "Paso 33,600: Loss = 0.4814\n",
      "Learning Rate: 5.33e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,650: Loss = 0.4423\n",
      "Learning Rate: 5.30e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 73.9% - Paso 33,700/45,591 | ETA: 08:47:32\n",
      "Paso 33,700: Loss = 0.4393\n",
      "Learning Rate: 5.28e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,750: Loss = 0.5050\n",
      "Learning Rate: 5.26e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 74.1% - Paso 33,800/45,591 | ETA: 08:43:02\n",
      "Paso 33,800: Loss = 0.5021\n",
      "Learning Rate: 5.24e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,850: Loss = 0.4730\n",
      "Learning Rate: 5.22e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 74.4% - Paso 33,900/45,591 | ETA: 08:38:31\n",
      "Paso 33,900: Loss = 0.4595\n",
      "Learning Rate: 5.19e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 33,950: Loss = 0.4747\n",
      "Learning Rate: 5.17e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 74.6% - Paso 34,000/45,591 | ETA: 08:34:01\n",
      "Paso 34,000: Loss = 0.4859\n",
      "Learning Rate: 5.15e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,050: Loss = 0.4525\n",
      "Learning Rate: 5.13e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████░░░░░░░░░░░] 74.8% - Paso 34,100/45,591 | ETA: 08:29:31\n",
      "Paso 34,100: Loss = 0.4956\n",
      "Learning Rate: 5.10e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,150: Loss = 0.4455\n",
      "Learning Rate: 5.08e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 75.0% - Paso 34,200/45,591 | ETA: 08:25:01\n",
      "Paso 34,200: Loss = 0.4645\n",
      "Learning Rate: 5.06e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,250: Loss = 0.4367\n",
      "Learning Rate: 5.04e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 75.2% - Paso 34,300/45,591 | ETA: 08:20:31\n",
      "Paso 34,300: Loss = 0.4409\n",
      "Learning Rate: 5.02e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,350: Loss = 0.4873\n",
      "Learning Rate: 4.99e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 75.5% - Paso 34,400/45,591 | ETA: 08:16:02\n",
      "Paso 34,400: Loss = 0.4327\n",
      "Learning Rate: 4.97e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,450: Loss = 0.4611\n",
      "Learning Rate: 4.95e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 75.7% - Paso 34,500/45,591 | ETA: 08:11:32\n",
      "Paso 34,500: Loss = 0.4747\n",
      "Learning Rate: 4.93e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,550: Loss = 0.4693\n",
      "Learning Rate: 4.90e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 75.9% - Paso 34,600/45,591 | ETA: 08:07:02\n",
      "Paso 34,600: Loss = 0.4512\n",
      "Learning Rate: 4.88e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,650: Loss = 0.4755\n",
      "Learning Rate: 4.86e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 76.1% - Paso 34,700/45,591 | ETA: 08:02:33\n",
      "Paso 34,700: Loss = 0.4904\n",
      "Learning Rate: 4.84e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,750: Loss = 0.4593\n",
      "Learning Rate: 4.82e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 76.3% - Paso 34,800/45,591 | ETA: 07:58:03\n",
      "Paso 34,800: Loss = 0.4505\n",
      "Learning Rate: 4.79e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,850: Loss = 0.4491\n",
      "Learning Rate: 4.77e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 76.6% - Paso 34,900/45,591 | ETA: 07:53:34\n",
      "Paso 34,900: Loss = 0.4641\n",
      "Learning Rate: 4.75e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 34,950: Loss = 0.4237\n",
      "Learning Rate: 4.73e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 76.8% - Paso 35,000/45,591 | ETA: 07:49:05\n",
      "Paso 35,000: Loss = 0.4773\n",
      "Learning Rate: 4.71e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,050: Loss = 0.4464\n",
      "Learning Rate: 4.68e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 77.0% - Paso 35,100/45,591 | ETA: 07:44:36\n",
      "Paso 35,100: Loss = 0.4705\n",
      "Learning Rate: 4.66e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,150: Loss = 0.4550\n",
      "Learning Rate: 4.64e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 77.2% - Paso 35,200/45,591 | ETA: 07:40:07\n",
      "Paso 35,200: Loss = 0.4502\n",
      "Learning Rate: 4.62e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,250: Loss = 0.4876\n",
      "Learning Rate: 4.59e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████░░░░░░░░░░] 77.4% - Paso 35,300/45,591 | ETA: 07:35:38\n",
      "Paso 35,300: Loss = 0.4376\n",
      "Learning Rate: 4.57e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,350: Loss = 0.4578\n",
      "Learning Rate: 4.55e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 77.6% - Paso 35,400/45,591 | ETA: 07:31:10\n",
      "Paso 35,400: Loss = 0.4526\n",
      "Learning Rate: 4.53e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,450: Loss = 0.4561\n",
      "Learning Rate: 4.51e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 77.9% - Paso 35,500/45,591 | ETA: 07:26:41\n",
      "Paso 35,500: Loss = 0.4650\n",
      "Learning Rate: 4.48e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,550: Loss = 0.4469\n",
      "Learning Rate: 4.46e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 78.1% - Paso 35,600/45,591 | ETA: 07:22:13\n",
      "Paso 35,600: Loss = 0.4919\n",
      "Learning Rate: 4.44e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,650: Loss = 0.4955\n",
      "Learning Rate: 4.42e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 78.3% - Paso 35,700/45,591 | ETA: 07:17:44\n",
      "Paso 35,700: Loss = 0.4744\n",
      "Learning Rate: 4.39e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,750: Loss = 0.4860\n",
      "Learning Rate: 4.37e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 78.5% - Paso 35,800/45,591 | ETA: 07:13:16\n",
      "Paso 35,800: Loss = 0.4697\n",
      "Learning Rate: 4.35e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,850: Loss = 0.4751\n",
      "Learning Rate: 4.33e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 78.7% - Paso 35,900/45,591 | ETA: 07:08:47\n",
      "Paso 35,900: Loss = 0.4880\n",
      "Learning Rate: 4.31e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 35,950: Loss = 0.4847\n",
      "Learning Rate: 4.28e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 79.0% - Paso 36,000/45,591 | ETA: 07:04:18\n",
      "Paso 36,000: Loss = 0.4526\n",
      "Learning Rate: 4.26e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,050: Loss = 0.4619\n",
      "Learning Rate: 4.24e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 79.2% - Paso 36,100/45,591 | ETA: 06:59:50\n",
      "Paso 36,100: Loss = 0.4456\n",
      "Learning Rate: 4.22e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,150: Loss = 0.4527\n",
      "Learning Rate: 4.20e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 79.4% - Paso 36,200/45,591 | ETA: 06:55:22\n",
      "Paso 36,200: Loss = 0.4260\n",
      "Learning Rate: 4.17e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,250: Loss = 0.4416\n",
      "Learning Rate: 4.15e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 79.6% - Paso 36,300/45,591 | ETA: 06:50:54\n",
      "Paso 36,300: Loss = 0.4543\n",
      "Learning Rate: 4.13e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,350: Loss = 0.4885\n",
      "Learning Rate: 4.11e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████░░░░░░░░░] 79.8% - Paso 36,400/45,591 | ETA: 06:46:25\n",
      "Paso 36,400: Loss = 0.4432\n",
      "Learning Rate: 4.08e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,450: Loss = 0.4457\n",
      "Learning Rate: 4.06e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 80.1% - Paso 36,500/45,591 | ETA: 06:41:57\n",
      "Paso 36,500: Loss = 0.4697\n",
      "Learning Rate: 4.04e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,550: Loss = 0.4872\n",
      "Learning Rate: 4.02e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 80.3% - Paso 36,600/45,591 | ETA: 06:37:29\n",
      "Paso 36,600: Loss = 0.4720\n",
      "Learning Rate: 4.00e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,650: Loss = 0.4556\n",
      "Learning Rate: 3.97e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 80.5% - Paso 36,700/45,591 | ETA: 06:33:01\n",
      "Paso 36,700: Loss = 0.4632\n",
      "Learning Rate: 3.95e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,750: Loss = 0.4406\n",
      "Learning Rate: 3.93e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 80.7% - Paso 36,800/45,591 | ETA: 06:28:34\n",
      "Paso 36,800: Loss = 0.4755\n",
      "Learning Rate: 3.91e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,850: Loss = 0.4568\n",
      "Learning Rate: 3.89e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 80.9% - Paso 36,900/45,591 | ETA: 06:24:06\n",
      "Paso 36,900: Loss = 0.4383\n",
      "Learning Rate: 3.86e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 36,950: Loss = 0.4346\n",
      "Learning Rate: 3.84e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 81.2% - Paso 37,000/45,591 | ETA: 06:19:38\n",
      "Paso 37,000: Loss = 0.4700\n",
      "Learning Rate: 3.82e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,050: Loss = 0.4539\n",
      "Learning Rate: 3.80e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 81.4% - Paso 37,100/45,591 | ETA: 06:15:10\n",
      "Paso 37,100: Loss = 0.4797\n",
      "Learning Rate: 3.77e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,150: Loss = 0.4552\n",
      "Learning Rate: 3.75e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 81.6% - Paso 37,200/45,591 | ETA: 06:10:43\n",
      "Paso 37,200: Loss = 0.4464\n",
      "Learning Rate: 3.73e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,250: Loss = 0.4557\n",
      "Learning Rate: 3.71e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 81.8% - Paso 37,300/45,591 | ETA: 06:06:15\n",
      "Paso 37,300: Loss = 0.4451\n",
      "Learning Rate: 3.69e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,350: Loss = 0.4639\n",
      "Learning Rate: 3.66e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 82.0% - Paso 37,400/45,591 | ETA: 06:01:48\n",
      "Paso 37,400: Loss = 0.4464\n",
      "Learning Rate: 3.64e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,450: Loss = 0.4558\n",
      "Learning Rate: 3.62e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 82.3% - Paso 37,500/45,591 | ETA: 05:57:21\n",
      "Paso 37,500: Loss = 0.4388\n",
      "Learning Rate: 3.60e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,550: Loss = 0.4863\n",
      "Learning Rate: 3.57e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████░░░░░░░░] 82.5% - Paso 37,600/45,591 | ETA: 05:52:53\n",
      "Paso 37,600: Loss = 0.4822\n",
      "Learning Rate: 3.55e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,650: Loss = 0.4538\n",
      "Learning Rate: 3.53e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 82.7% - Paso 37,700/45,591 | ETA: 05:48:26\n",
      "Paso 37,700: Loss = 0.4710\n",
      "Learning Rate: 3.51e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,750: Loss = 0.4573\n",
      "Learning Rate: 3.49e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 82.9% - Paso 37,800/45,591 | ETA: 05:43:59\n",
      "Paso 37,800: Loss = 0.4696\n",
      "Learning Rate: 3.46e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,850: Loss = 0.4378\n",
      "Learning Rate: 3.44e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 83.1% - Paso 37,900/45,591 | ETA: 05:39:32\n",
      "Paso 37,900: Loss = 0.4575\n",
      "Learning Rate: 3.42e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 37,950: Loss = 0.5219\n",
      "Learning Rate: 3.40e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 83.3% - Paso 38,000/45,591 | ETA: 05:35:05\n",
      "Paso 38,000: Loss = 0.4424\n",
      "Learning Rate: 3.37e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,050: Loss = 0.4726\n",
      "Learning Rate: 3.35e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 83.6% - Paso 38,100/45,591 | ETA: 05:30:38\n",
      "Paso 38,100: Loss = 0.4400\n",
      "Learning Rate: 3.33e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,150: Loss = 0.4568\n",
      "Learning Rate: 3.31e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 83.8% - Paso 38,200/45,591 | ETA: 05:26:11\n",
      "Paso 38,200: Loss = 0.4824\n",
      "Learning Rate: 3.29e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,250: Loss = 0.4637\n",
      "Learning Rate: 3.26e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 84.0% - Paso 38,300/45,591 | ETA: 05:21:44\n",
      "Paso 38,300: Loss = 0.4528\n",
      "Learning Rate: 3.24e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,350: Loss = 0.4968\n",
      "Learning Rate: 3.22e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 84.2% - Paso 38,400/45,591 | ETA: 05:17:18\n",
      "Paso 38,400: Loss = 0.4548\n",
      "Learning Rate: 3.20e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,450: Loss = 0.4249\n",
      "Learning Rate: 3.18e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 84.4% - Paso 38,500/45,591 | ETA: 05:12:51\n",
      "Paso 38,500: Loss = 0.4411\n",
      "Learning Rate: 3.15e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,550: Loss = 0.4869\n",
      "Learning Rate: 3.13e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 84.7% - Paso 38,600/45,591 | ETA: 05:08:24\n",
      "Paso 38,600: Loss = 0.4702\n",
      "Learning Rate: 3.11e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,650: Loss = 0.4684\n",
      "Learning Rate: 3.09e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████░░░░░░░] 84.9% - Paso 38,700/45,591 | ETA: 05:03:58\n",
      "Paso 38,700: Loss = 0.4755\n",
      "Learning Rate: 3.06e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,750: Loss = 0.4387\n",
      "Learning Rate: 3.04e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 85.1% - Paso 38,800/45,591 | ETA: 04:59:31\n",
      "Paso 38,800: Loss = 0.4519\n",
      "Learning Rate: 3.02e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,850: Loss = 0.4290\n",
      "Learning Rate: 3.00e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 85.3% - Paso 38,900/45,591 | ETA: 04:55:05\n",
      "Paso 38,900: Loss = 0.4644\n",
      "Learning Rate: 2.98e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 38,950: Loss = 0.4560\n",
      "Learning Rate: 2.95e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 85.5% - Paso 39,000/45,591 | ETA: 04:50:39\n",
      "Paso 39,000: Loss = 0.4549\n",
      "Learning Rate: 2.93e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,050: Loss = 0.4597\n",
      "Learning Rate: 2.91e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 85.8% - Paso 39,100/45,591 | ETA: 04:46:12\n",
      "Paso 39,100: Loss = 0.4975\n",
      "Learning Rate: 2.89e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,150: Loss = 0.4499\n",
      "Learning Rate: 2.87e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 86.0% - Paso 39,200/45,591 | ETA: 04:41:46\n",
      "Paso 39,200: Loss = 0.4915\n",
      "Learning Rate: 2.84e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,250: Loss = 0.5118\n",
      "Learning Rate: 2.82e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 86.2% - Paso 39,300/45,591 | ETA: 04:37:20\n",
      "Paso 39,300: Loss = 0.4714\n",
      "Learning Rate: 2.80e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,350: Loss = 0.4767\n",
      "Learning Rate: 2.78e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 86.4% - Paso 39,400/45,591 | ETA: 04:32:54\n",
      "Paso 39,400: Loss = 0.4901\n",
      "Learning Rate: 2.75e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,450: Loss = 0.4854\n",
      "Learning Rate: 2.73e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 86.6% - Paso 39,500/45,591 | ETA: 04:28:28\n",
      "Paso 39,500: Loss = 0.4203\n",
      "Learning Rate: 2.71e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,550: Loss = 0.4736\n",
      "Learning Rate: 2.69e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 86.9% - Paso 39,600/45,591 | ETA: 04:24:02\n",
      "Paso 39,600: Loss = 0.4505\n",
      "Learning Rate: 2.67e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,650: Loss = 0.4910\n",
      "Learning Rate: 2.64e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 87.1% - Paso 39,700/45,591 | ETA: 04:19:36\n",
      "Paso 39,700: Loss = 0.4814\n",
      "Learning Rate: 2.62e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,750: Loss = 0.5029\n",
      "Learning Rate: 2.60e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████░░░░░░] 87.3% - Paso 39,800/45,591 | ETA: 04:15:10\n",
      "Paso 39,800: Loss = 0.4644\n",
      "Learning Rate: 2.58e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,850: Loss = 0.4691\n",
      "Learning Rate: 2.55e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 87.5% - Paso 39,900/45,591 | ETA: 04:10:44\n",
      "Paso 39,900: Loss = 0.4404\n",
      "Learning Rate: 2.53e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 39,950: Loss = 0.4670\n",
      "Learning Rate: 2.51e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 87.7% - Paso 40,000/45,591 | ETA: 04:06:18\n",
      "Paso 40,000: Loss = 0.4472\n",
      "Learning Rate: 2.49e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,050: Loss = 0.4431\n",
      "Learning Rate: 2.47e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 88.0% - Paso 40,100/45,591 | ETA: 04:01:53\n",
      "Paso 40,100: Loss = 0.4772\n",
      "Learning Rate: 2.44e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,150: Loss = 0.4842\n",
      "Learning Rate: 2.42e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 88.2% - Paso 40,200/45,591 | ETA: 03:57:27\n",
      "Paso 40,200: Loss = 0.4322\n",
      "Learning Rate: 2.40e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,250: Loss = 0.4516\n",
      "Learning Rate: 2.38e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 88.4% - Paso 40,300/45,591 | ETA: 03:53:01\n",
      "Paso 40,300: Loss = 0.4445\n",
      "Learning Rate: 2.36e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,350: Loss = 0.4350\n",
      "Learning Rate: 2.33e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 88.6% - Paso 40,400/45,591 | ETA: 03:48:36\n",
      "Paso 40,400: Loss = 0.4724\n",
      "Learning Rate: 2.31e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,450: Loss = 0.4879\n",
      "Learning Rate: 2.29e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 88.8% - Paso 40,500/45,591 | ETA: 03:44:10\n",
      "Paso 40,500: Loss = 0.4465\n",
      "Learning Rate: 2.27e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,550: Loss = 0.4171\n",
      "Learning Rate: 2.24e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 89.1% - Paso 40,600/45,591 | ETA: 03:39:45\n",
      "Paso 40,600: Loss = 0.4467\n",
      "Learning Rate: 2.22e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,650: Loss = 0.4571\n",
      "Learning Rate: 2.20e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 89.3% - Paso 40,700/45,591 | ETA: 03:35:20\n",
      "Paso 40,700: Loss = 0.4339\n",
      "Learning Rate: 2.18e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,750: Loss = 0.4453\n",
      "Learning Rate: 2.16e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 89.5% - Paso 40,800/45,591 | ETA: 03:30:54\n",
      "Paso 40,800: Loss = 0.4277\n",
      "Learning Rate: 2.13e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,850: Loss = 0.4347\n",
      "Learning Rate: 2.11e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 89.7% - Paso 40,900/45,591 | ETA: 03:26:29\n",
      "Paso 40,900: Loss = 0.4522\n",
      "Learning Rate: 2.09e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 40,950: Loss = 0.4373\n",
      "Learning Rate: 2.07e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████░░░░░] 89.9% - Paso 41,000/45,591 | ETA: 03:22:04\n",
      "Paso 41,000: Loss = 0.4948\n",
      "Learning Rate: 2.05e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,050: Loss = 0.4738\n",
      "Learning Rate: 2.02e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 90.1% - Paso 41,100/45,591 | ETA: 03:17:39\n",
      "Paso 41,100: Loss = 0.4375\n",
      "Learning Rate: 2.00e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,150: Loss = 0.4439\n",
      "Learning Rate: 1.98e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 90.4% - Paso 41,200/45,591 | ETA: 03:13:13\n",
      "Paso 41,200: Loss = 0.4662\n",
      "Learning Rate: 1.96e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,250: Loss = 0.4585\n",
      "Learning Rate: 1.93e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 90.6% - Paso 41,300/45,591 | ETA: 03:08:48\n",
      "Paso 41,300: Loss = 0.4189\n",
      "Learning Rate: 1.91e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,350: Loss = 0.4503\n",
      "Learning Rate: 1.89e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 90.8% - Paso 41,400/45,591 | ETA: 03:04:23\n",
      "Paso 41,400: Loss = 0.4845\n",
      "Learning Rate: 1.87e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,450: Loss = 0.4560\n",
      "Learning Rate: 1.85e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 91.0% - Paso 41,500/45,591 | ETA: 02:59:58\n",
      "Paso 41,500: Loss = 0.4611\n",
      "Learning Rate: 1.82e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,550: Loss = 0.4464\n",
      "Learning Rate: 1.80e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 91.2% - Paso 41,600/45,591 | ETA: 02:55:34\n",
      "Paso 41,600: Loss = 0.4522\n",
      "Learning Rate: 1.78e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,650: Loss = 0.4599\n",
      "Learning Rate: 1.76e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 91.5% - Paso 41,700/45,591 | ETA: 02:51:09\n",
      "Paso 41,700: Loss = 0.4426\n",
      "Learning Rate: 1.74e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,750: Loss = 0.4412\n",
      "Learning Rate: 1.71e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 91.7% - Paso 41,800/45,591 | ETA: 02:46:44\n",
      "Paso 41,800: Loss = 0.4737\n",
      "Learning Rate: 1.69e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,850: Loss = 0.4519\n",
      "Learning Rate: 1.67e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 91.9% - Paso 41,900/45,591 | ETA: 02:42:19\n",
      "Paso 41,900: Loss = 0.4654\n",
      "Learning Rate: 1.65e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 41,950: Loss = 0.4633\n",
      "Learning Rate: 1.62e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 92.1% - Paso 42,000/45,591 | ETA: 02:37:55\n",
      "Paso 42,000: Loss = 0.4470\n",
      "Learning Rate: 1.60e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,050: Loss = 0.4573\n",
      "Learning Rate: 1.58e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[████████████████████████████████████░░░░] 92.3% - Paso 42,100/45,591 | ETA: 02:33:30\n",
      "Paso 42,100: Loss = 0.4705\n",
      "Learning Rate: 1.56e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,150: Loss = 0.4566\n",
      "Learning Rate: 1.54e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 92.6% - Paso 42,200/45,591 | ETA: 02:29:05\n",
      "Paso 42,200: Loss = 0.4681\n",
      "Learning Rate: 1.51e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,250: Loss = 0.4713\n",
      "Learning Rate: 1.49e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 92.8% - Paso 42,300/45,591 | ETA: 02:24:41\n",
      "Paso 42,300: Loss = 0.4485\n",
      "Learning Rate: 1.47e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,350: Loss = 0.4579\n",
      "Learning Rate: 1.45e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 93.0% - Paso 42,400/45,591 | ETA: 02:20:17\n",
      "Paso 42,400: Loss = 0.4580\n",
      "Learning Rate: 1.42e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,450: Loss = 0.4726\n",
      "Learning Rate: 1.40e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 93.2% - Paso 42,500/45,591 | ETA: 02:15:52\n",
      "Paso 42,500: Loss = 0.4628\n",
      "Learning Rate: 1.38e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,550: Loss = 0.4639\n",
      "Learning Rate: 1.36e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 93.4% - Paso 42,600/45,591 | ETA: 02:11:28\n",
      "Paso 42,600: Loss = 0.4840\n",
      "Learning Rate: 1.34e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,650: Loss = 0.4410\n",
      "Learning Rate: 1.31e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 93.7% - Paso 42,700/45,591 | ETA: 02:07:04\n",
      "Paso 42,700: Loss = 0.4694\n",
      "Learning Rate: 1.29e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,750: Loss = 0.4459\n",
      "Learning Rate: 1.27e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 93.9% - Paso 42,800/45,591 | ETA: 02:02:39\n",
      "Paso 42,800: Loss = 0.4677\n",
      "Learning Rate: 1.25e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,850: Loss = 0.4500\n",
      "Learning Rate: 1.23e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 94.1% - Paso 42,900/45,591 | ETA: 01:58:15\n",
      "Paso 42,900: Loss = 0.4422\n",
      "Learning Rate: 1.20e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 42,950: Loss = 0.4672\n",
      "Learning Rate: 1.18e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 94.3% - Paso 43,000/45,591 | ETA: 01:53:51\n",
      "Paso 43,000: Loss = 0.4445\n",
      "Learning Rate: 1.16e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,050: Loss = 0.4573\n",
      "Learning Rate: 1.14e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 94.5% - Paso 43,100/45,591 | ETA: 01:49:27\n",
      "Paso 43,100: Loss = 0.4417\n",
      "Learning Rate: 1.11e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,150: Loss = 0.4397\n",
      "Learning Rate: 1.09e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 94.8% - Paso 43,200/45,591 | ETA: 01:45:02\n",
      "Paso 43,200: Loss = 0.4388\n",
      "Learning Rate: 1.07e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,250: Loss = 0.4664\n",
      "Learning Rate: 1.05e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[█████████████████████████████████████░░░] 95.0% - Paso 43,300/45,591 | ETA: 01:40:38\n",
      "Paso 43,300: Loss = 0.4644\n",
      "Learning Rate: 1.03e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,350: Loss = 0.4621\n",
      "Learning Rate: 1.00e-06\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 95.2% - Paso 43,400/45,591 | ETA: 01:36:14\n",
      "Paso 43,400: Loss = 0.4375\n",
      "Learning Rate: 9.81e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,450: Loss = 0.4319\n",
      "Learning Rate: 9.59e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 95.4% - Paso 43,500/45,591 | ETA: 01:31:50\n",
      "Paso 43,500: Loss = 0.4578\n",
      "Learning Rate: 9.37e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,550: Loss = 0.4311\n",
      "Learning Rate: 9.15e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 95.6% - Paso 43,600/45,591 | ETA: 01:27:27\n",
      "Paso 43,600: Loss = 0.4390\n",
      "Learning Rate: 8.92e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,650: Loss = 0.4705\n",
      "Learning Rate: 8.70e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 95.9% - Paso 43,700/45,591 | ETA: 01:23:03\n",
      "Paso 43,700: Loss = 0.4926\n",
      "Learning Rate: 8.48e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,750: Loss = 0.4561\n",
      "Learning Rate: 8.26e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 96.1% - Paso 43,800/45,591 | ETA: 01:18:39\n",
      "Paso 43,800: Loss = 0.4558\n",
      "Learning Rate: 8.04e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,850: Loss = 0.4802\n",
      "Learning Rate: 7.82e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 96.3% - Paso 43,900/45,591 | ETA: 01:14:17\n",
      "Paso 43,900: Loss = 0.4133\n",
      "Learning Rate: 7.59e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 43,950: Loss = 0.4715\n",
      "Learning Rate: 7.37e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 96.5% - Paso 44,000/45,591 | ETA: 01:09:53\n",
      "Paso 44,000: Loss = 0.4354\n",
      "Learning Rate: 7.15e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,050: Loss = 0.4661\n",
      "Learning Rate: 6.93e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 96.7% - Paso 44,100/45,591 | ETA: 01:05:29\n",
      "Paso 44,100: Loss = 0.4542\n",
      "Learning Rate: 6.71e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,150: Loss = 0.4991\n",
      "Learning Rate: 6.48e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 96.9% - Paso 44,200/45,591 | ETA: 01:01:06\n",
      "Paso 44,200: Loss = 0.4318\n",
      "Learning Rate: 6.26e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,250: Loss = 0.5153\n",
      "Learning Rate: 6.04e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 97.2% - Paso 44,300/45,591 | ETA: 00:56:42\n",
      "Paso 44,300: Loss = 0.4037\n",
      "Learning Rate: 5.82e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,350: Loss = 0.4735\n",
      "Learning Rate: 5.60e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[██████████████████████████████████████░░] 97.4% - Paso 44,400/45,591 | ETA: 00:52:18\n",
      "Paso 44,400: Loss = 0.4670\n",
      "Learning Rate: 5.38e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,450: Loss = 0.4209\n",
      "Learning Rate: 5.15e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 97.6% - Paso 44,500/45,591 | ETA: 00:47:55\n",
      "Paso 44,500: Loss = 0.4462\n",
      "Learning Rate: 4.93e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,550: Loss = 0.4522\n",
      "Learning Rate: 4.71e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 97.8% - Paso 44,600/45,591 | ETA: 00:43:31\n",
      "Paso 44,600: Loss = 0.4195\n",
      "Learning Rate: 4.49e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,650: Loss = 0.4614\n",
      "Learning Rate: 4.27e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 98.0% - Paso 44,700/45,591 | ETA: 00:39:08\n",
      "Paso 44,700: Loss = 0.4479\n",
      "Learning Rate: 4.05e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,750: Loss = 0.4751\n",
      "Learning Rate: 3.82e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 98.3% - Paso 44,800/45,591 | ETA: 00:34:44\n",
      "Paso 44,800: Loss = 0.4424\n",
      "Learning Rate: 3.61e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,850: Loss = 0.4352\n",
      "Learning Rate: 3.38e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 98.5% - Paso 44,900/45,591 | ETA: 00:30:21\n",
      "Paso 44,900: Loss = 0.4457\n",
      "Learning Rate: 3.16e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 44,950: Loss = 0.4575\n",
      "Learning Rate: 2.94e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 98.7% - Paso 45,000/45,591 | ETA: 00:25:57\n",
      "Paso 45,000: Loss = 0.4329\n",
      "Learning Rate: 2.72e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,050: Loss = 0.4693\n",
      "Learning Rate: 2.50e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 98.9% - Paso 45,100/45,591 | ETA: 00:21:34\n",
      "Paso 45,100: Loss = 0.4579\n",
      "Learning Rate: 2.28e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,150: Loss = 0.4302\n",
      "Learning Rate: 2.05e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 99.1% - Paso 45,200/45,591 | ETA: 00:17:11\n",
      "Paso 45,200: Loss = 0.4545\n",
      "Learning Rate: 1.83e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,250: Loss = 0.4481\n",
      "Learning Rate: 1.61e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 99.4% - Paso 45,300/45,591 | ETA: 00:12:47\n",
      "Paso 45,300: Loss = 0.4597\n",
      "Learning Rate: 1.39e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,350: Loss = 0.4967\n",
      "Learning Rate: 1.17e-07\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 99.6% - Paso 45,400/45,591 | ETA: 00:08:23\n",
      "Paso 45,400: Loss = 0.4537\n",
      "Learning Rate: 9.45e-08\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,450: Loss = 0.4982\n",
      "Learning Rate: 7.23e-08\n",
      "Velocidad: 0.38 pasos/seg\n",
      "\n",
      "[███████████████████████████████████████░] 99.8% - Paso 45,500/45,591 | ETA: 00:03:59\n",
      "Paso 45,500: Loss = 0.4393\n",
      "Learning Rate: 5.01e-08\n",
      "Velocidad: 0.38 pasos/seg\n",
      "Paso 45,550: Loss = 0.4877\n",
      "Learning Rate: 2.79e-08\n",
      "Velocidad: 0.38 pasos/seg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60785\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Época 3 completada en 650.4 minutos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results_multilingual_optimized\\checkpoint-45591\n",
      "Configuration saved in ./results_multilingual_optimized\\checkpoint-45591\\config.json\n",
      "Model weights saved in ./results_multilingual_optimized\\checkpoint-45591\\model.safetensors\n",
      "Deleting older checkpoint [results_multilingual_optimized\\checkpoint-15197] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results_multilingual_optimized\\checkpoint-45591 (score: 0.7149335357427283).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENTRENAMIENTO COMPLETADO!\n",
      "Tiempo total: 33.94 horas (2036.2 minutos)\n",
      "Pasos completados: 45,591\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45591, training_loss=0.5019875658818491, metrics={'train_runtime': 122171.1049, 'train_samples_per_second': 5.97, 'train_steps_per_second': 0.373, 'total_flos': 4.798004728978944e+16, 'train_loss': 0.5019875658818491, 'epoch': 3.0})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faa5468",
   "metadata": {},
   "source": [
    "## 🧼 Limpieza de texto\n",
    "- Normaliza texto (lower/regex), quita URLs/símbolos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ced69298-1d8f-4764-98b7-343eebdb59b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 60785\n",
      "  Batch size = 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3377' max='3377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3377/3377 33:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  loss: 0.5073\n",
      "  accuracy: 0.7507\n",
      "  macro_f1: 0.7149\n",
      "  weighted_f1: 0.7149\n",
      "  malo_f1: 0.9174\n",
      "  neutro_f1: 0.7518\n",
      "  bueno_f1: 0.4756\n",
      "  malo_precision: 0.8492\n",
      "  malo_recall: 0.9975\n",
      "  neutro_precision: 0.6271\n",
      "  neutro_recall: 0.9386\n",
      "  bueno_precision: 0.9617\n",
      "  bueno_recall: 0.3159\n",
      "  runtime: 1996.1024\n",
      "  samples_per_second: 30.4520\n",
      "  steps_per_second: 1.6920\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "for key, value in metrics.items():\n",
    "    if key.startswith('eval_'):\n",
    "        clean_key = key.replace('eval_', '')\n",
    "        print(f\"  {clean_key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c8e11d",
   "metadata": {},
   "source": [
    "## 🧩 Construcción del dataset tokenizado\n",
    "- Aplica el tokenizador y construye Dataset/DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3155623-d031-49a9-a0c9-89539d8f6b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./modelo_sentimientos_multilingual_balanced\n",
      "Configuration saved in ./modelo_sentimientos_multilingual_balanced\\config.json\n",
      "Model weights saved in ./modelo_sentimientos_multilingual_balanced\\model.safetensors\n",
      "tokenizer config file saved in ./modelo_sentimientos_multilingual_balanced\\tokenizer_config.json\n",
      "Special tokens file saved in ./modelo_sentimientos_multilingual_balanced\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./modelo_sentimientos_multilingual_balanced\\\\tokenizer_config.json',\n",
       " './modelo_sentimientos_multilingual_balanced\\\\special_tokens_map.json',\n",
       " './modelo_sentimientos_multilingual_balanced\\\\vocab.txt',\n",
       " './modelo_sentimientos_multilingual_balanced\\\\added_tokens.json')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = \"./modelo_sentimientos_multilingual_balanced\"\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d40f0",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ff4cb8fe-ef86-4bb2-b2f3-6da9dd417597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4520"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0abbb9",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f9237de-0cfa-43ae-88db-78d6a873ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75d147",
   "metadata": {},
   "source": [
    "## 🔤 Tokenizador BERT\n",
    "- Configura el tokenizador (padding/truncation/max_len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f4b55f05-3bf4-4333-b863-e074f0fc065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading file chat_template.jinja\n",
      "loading configuration file ./modelo_sentimientos_multilingual_balanced\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file ./modelo_sentimientos_multilingual_balanced\\model.safetensors\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./modelo_sentimientos_multilingual_balanced.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_save_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    model_save_path = \"./results_multilingual_balanced\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795448a4",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b710a701-8492-4f44-9815-0314e2815abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_in_batches_multilingual(texts, labels, model, tokenizer, batch_size=16):\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch_texts = list(texts.iloc[i:i+batch_size])\n",
    "            \n",
    "            # Tokenizar con configuración multilingüe\n",
    "            inputs = tokenizer(\n",
    "                batch_texts, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=128, \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "            \n",
    "            # Predicción\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            \n",
    "            # Limpieza de memoria\n",
    "            del inputs, outputs, predictions, probabilities\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if (i // batch_size + 1) % 20 == 0:\n",
    "                print(f\"    Procesados {i + batch_size}/{len(texts)} muestras\")\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83e1f9",
   "metadata": {},
   "source": [
    "## 🧩 Construcción del dataset tokenizado\n",
    "- Aplica el tokenizador y construye Dataset/DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6885a3d1-2890-4b15-9297-fa4dc1084d9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Procesados 640/60785 muestras\n",
      "    Procesados 1280/60785 muestras\n",
      "    Procesados 1920/60785 muestras\n",
      "    Procesados 2560/60785 muestras\n",
      "    Procesados 3200/60785 muestras\n",
      "    Procesados 3840/60785 muestras\n",
      "    Procesados 4480/60785 muestras\n",
      "    Procesados 5120/60785 muestras\n",
      "    Procesados 5760/60785 muestras\n",
      "    Procesados 6400/60785 muestras\n",
      "    Procesados 7040/60785 muestras\n",
      "    Procesados 7680/60785 muestras\n",
      "    Procesados 8320/60785 muestras\n",
      "    Procesados 8960/60785 muestras\n",
      "    Procesados 9600/60785 muestras\n",
      "    Procesados 10240/60785 muestras\n",
      "    Procesados 10880/60785 muestras\n",
      "    Procesados 11520/60785 muestras\n",
      "    Procesados 12160/60785 muestras\n",
      "    Procesados 12800/60785 muestras\n",
      "    Procesados 13440/60785 muestras\n",
      "    Procesados 14080/60785 muestras\n",
      "    Procesados 14720/60785 muestras\n",
      "    Procesados 15360/60785 muestras\n",
      "    Procesados 16000/60785 muestras\n",
      "    Procesados 16640/60785 muestras\n",
      "    Procesados 17280/60785 muestras\n",
      "    Procesados 17920/60785 muestras\n",
      "    Procesados 18560/60785 muestras\n",
      "    Procesados 19200/60785 muestras\n",
      "    Procesados 19840/60785 muestras\n",
      "    Procesados 20480/60785 muestras\n",
      "    Procesados 21120/60785 muestras\n",
      "    Procesados 21760/60785 muestras\n",
      "    Procesados 22400/60785 muestras\n",
      "    Procesados 23040/60785 muestras\n",
      "    Procesados 23680/60785 muestras\n",
      "    Procesados 24320/60785 muestras\n",
      "    Procesados 24960/60785 muestras\n",
      "    Procesados 25600/60785 muestras\n",
      "    Procesados 26240/60785 muestras\n",
      "    Procesados 26880/60785 muestras\n",
      "    Procesados 27520/60785 muestras\n",
      "    Procesados 28160/60785 muestras\n",
      "    Procesados 28800/60785 muestras\n",
      "    Procesados 29440/60785 muestras\n",
      "    Procesados 30080/60785 muestras\n",
      "    Procesados 30720/60785 muestras\n",
      "    Procesados 31360/60785 muestras\n",
      "    Procesados 32000/60785 muestras\n",
      "    Procesados 32640/60785 muestras\n",
      "    Procesados 33280/60785 muestras\n",
      "    Procesados 33920/60785 muestras\n",
      "    Procesados 34560/60785 muestras\n",
      "    Procesados 35200/60785 muestras\n",
      "    Procesados 35840/60785 muestras\n",
      "    Procesados 36480/60785 muestras\n",
      "    Procesados 37120/60785 muestras\n",
      "    Procesados 37760/60785 muestras\n",
      "    Procesados 38400/60785 muestras\n",
      "    Procesados 39040/60785 muestras\n",
      "    Procesados 39680/60785 muestras\n",
      "    Procesados 40320/60785 muestras\n",
      "    Procesados 40960/60785 muestras\n",
      "    Procesados 41600/60785 muestras\n",
      "    Procesados 42240/60785 muestras\n",
      "    Procesados 42880/60785 muestras\n",
      "    Procesados 43520/60785 muestras\n",
      "    Procesados 44160/60785 muestras\n",
      "    Procesados 44800/60785 muestras\n",
      "    Procesados 45440/60785 muestras\n",
      "    Procesados 46080/60785 muestras\n",
      "    Procesados 46720/60785 muestras\n",
      "    Procesados 47360/60785 muestras\n",
      "    Procesados 48000/60785 muestras\n",
      "    Procesados 48640/60785 muestras\n",
      "    Procesados 49280/60785 muestras\n",
      "    Procesados 49920/60785 muestras\n",
      "    Procesados 50560/60785 muestras\n",
      "    Procesados 51200/60785 muestras\n",
      "    Procesados 51840/60785 muestras\n",
      "    Procesados 52480/60785 muestras\n",
      "    Procesados 53120/60785 muestras\n",
      "    Procesados 53760/60785 muestras\n",
      "    Procesados 54400/60785 muestras\n",
      "    Procesados 55040/60785 muestras\n",
      "    Procesados 55680/60785 muestras\n",
      "    Procesados 56320/60785 muestras\n",
      "    Procesados 56960/60785 muestras\n",
      "    Procesados 57600/60785 muestras\n",
      "    Procesados 58240/60785 muestras\n",
      "    Procesados 58880/60785 muestras\n",
      "    Procesados 59520/60785 muestras\n",
      "    Procesados 60160/60785 muestras\n",
      "    Procesados 60800/60785 muestras\n"
     ]
    }
   ],
   "source": [
    "val_predictions, val_probabilities = evaluate_in_batches_multilingual(\n",
    "    val_texts, val_labels, model, tokenizer, batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a058f3fb",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6eab9dc0-2e4a-401b-90e6-ae1c2e69a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACCURACY FINAL: 0.7507\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(val_predictions == val_labels)\n",
    "print(f\"\\nACCURACY FINAL: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d94833",
   "metadata": {},
   "source": [
    "## 📊 Evaluación\n",
    "- Calcula métricas (accuracy/F1/precision/recall) y reportes/confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b49b36b0-19b0-4517-8856-b7ec1907b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MATRIZ DE CONFUSIÓN:\n",
      "         Predicho:\n",
      "         Malo  Neutro Bueno\n",
      "Malo    20212     42     8\n",
      "Neutro   997   19018   247\n",
      "Bueno   2592   11268  6401\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMATRIZ DE CONFUSIÓN:\")\n",
    "cm = confusion_matrix(val_labels, val_predictions)\n",
    "print(\"         Predicho:\")\n",
    "print(\"         Malo  Neutro Bueno\")\n",
    "for i, row in enumerate(cm):\n",
    "    label_name = label_names[i]\n",
    "    print(f\"{label_name:7} {row[0]:4d}   {row[1]:4d}  {row[2]:4d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b24e87",
   "metadata": {},
   "source": [
    "## 📊 Evaluación\n",
    "- Calcula métricas (accuracy/F1/precision/recall) y reportes/confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "36670f7e-ffec-4021-8af3-9ffddbf715af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "REPORTE DETALLADO:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Malo       0.85      1.00      0.92     20262\n",
      "      Neutro       0.63      0.94      0.75     20262\n",
      "       Bueno       0.96      0.32      0.48     20261\n",
      "\n",
      "    accuracy                           0.75     60785\n",
      "   macro avg       0.81      0.75      0.71     60785\n",
      "weighted avg       0.81      0.75      0.71     60785\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(val_labels, val_predictions, target_names=label_names, output_dict=True)\n",
    "print(f\"\\nREPORTE DETALLADO:\")\n",
    "print(classification_report(val_labels, val_predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f22aa",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4cc54014-ca1e-4bff-b4c4-9f0bad59029a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANÁLISIS DE SESGO:\n",
      "  Malo: F1=0.917, P=0.849, R=0.998, Bias=0.148\n",
      "  Neutro: F1=0.752, P=0.627, R=0.939, Bias=0.312\n",
      "  Bueno: F1=0.476, P=0.962, R=0.316, Bias=0.646\n",
      "\n",
      "Bias promedio: 0.3685\n",
      "Sesgo significativo (bias ≥ 0.15)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nANÁLISIS DE SESGO:\")\n",
    "bias_metrics = {}\n",
    "for i, label_name in enumerate(label_names):\n",
    "    precision = report[label_name]['precision']\n",
    "    recall = report[label_name]['recall']\n",
    "    f1 = report[label_name]['f1-score']\n",
    "    bias = abs(precision - recall)\n",
    "    \n",
    "    bias_metrics[label_name] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall, \n",
    "        'f1': f1,\n",
    "        'bias': bias\n",
    "    }\n",
    "    \n",
    "    print(f\"  {label_name}: F1={f1:.3f}, P={precision:.3f}, R={recall:.3f}, Bias={bias:.3f}\")\n",
    "\n",
    "avg_bias = np.mean([m['bias'] for m in bias_metrics.values()])\n",
    "print(f\"\\nBias promedio: {avg_bias:.4f}\")\n",
    "\n",
    "if avg_bias < 0.05:\n",
    "    print(\"Excelente balance (bias < 0.05)\")\n",
    "elif avg_bias < 0.1:\n",
    "    print(\"Buen balance (bias < 0.1)\")\n",
    "elif avg_bias < 0.15:\n",
    "    print(\"Balance moderado (0.1 ≤ bias < 0.15)\")\n",
    "else:\n",
    "    print(\"Sesgo significativo (bias ≥ 0.15)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e3722",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b383eb88-f4a6-4e1f-8194-440801e3f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilingual_examples = [\n",
    "    # Español\n",
    "    \"El servicio fue excelente y el personal muy amable\",\n",
    "    \"No me gustó para nada el producto, fue terrible\", \n",
    "    \"El producto está bien, nada especial\",\n",
    "    \n",
    "    # Inglés\n",
    "    \"The service was excellent and the staff very friendly\",\n",
    "    \"I didn't like the product at all, it was terrible\",\n",
    "    \"The product is fine, nothing special\",\n",
    "    \n",
    "    # Francés\n",
    "    \"Le service était excellent et le personnel très sympathique\",\n",
    "    \"Je n'ai pas du tout aimé le produit, c'était terrible\",\n",
    "    \"Le produit est bien, rien d'exceptionnel\",\n",
    "    \n",
    "    # Italiano\n",
    "    \"Il servizio era eccellente e il personale molto gentile\",\n",
    "    \"Non mi è piaciuto per niente il prodotto, è stato terribile\",\n",
    "    \"Il prodotto va bene, niente di speciale\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ef8bc",
   "metadata": {},
   "source": [
    "## 🔤 Tokenizador BERT\n",
    "- Configura el tokenizador (padding/truncation/max_len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6692e8f3-72fe-40e5-a7be-908a8b118bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = tokenizer(\n",
    "    multilingual_examples, \n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=128, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a060c4",
   "metadata": {},
   "source": [
    "## 🔮 Predicción / inferencia\n",
    "- Aplica el modelo a nuevos textos y obtiene probabilidades/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a9886491-d4da-4c5d-a8a3-37f589b54c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_outputs = model(**test_inputs)\n",
    "    test_predictions = torch.argmax(test_outputs.logits, dim=1)\n",
    "    test_probabilities = torch.softmax(test_outputs.logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab3f40",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "074b5495-72b7-4c34-b16e-feb81fcdf6c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cd6c087fde4d90ab8195cfacf0e8a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Detectando idiomas:   0%|          | 0/12 [00:00<?, ?texto/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1. [ESP] 'El servicio fue excelente y el personal muy amable'\n",
      "     → Bueno (99.8%)\n",
      "     M:0.00 | N:0.00 | B:1.00\n",
      "\n",
      " 2. [ESP] 'No me gustó para nada el producto, fue terrible'\n",
      "     → Malo (99.9%)\n",
      "     M:1.00 | N:0.00 | B:0.00\n",
      "\n",
      " 3. [ESP] 'El producto está bien, nada especial'\n",
      "     → Neutro (71.1%)\n",
      "     M:0.27 | N:0.71 | B:0.02\n",
      "\n",
      " 4. [ENG] 'The service was excellent and the staff very friendly'\n",
      "     → Bueno (99.9%)\n",
      "     M:0.00 | N:0.00 | B:1.00\n",
      "\n",
      " 5. [ENG] 'I didn't like the product at all, it was terrible'\n",
      "     → Malo (99.9%)\n",
      "     M:1.00 | N:0.00 | B:0.00\n",
      "\n",
      " 6. [ENG] 'The product is fine, nothing special'\n",
      "     → Neutro (97.1%)\n",
      "     M:0.00 | N:0.97 | B:0.03\n",
      "\n",
      " 7. [FRA] 'Le service était excellent et le personnel très sympathique'\n",
      "     → Bueno (99.8%)\n",
      "     M:0.00 | N:0.00 | B:1.00\n",
      "\n",
      " 8. [FRA] 'Je n'ai pas du tout aimé le produit, c'était terrible'\n",
      "     → Malo (99.9%)\n",
      "     M:1.00 | N:0.00 | B:0.00\n",
      "\n",
      " 9. [FRA] 'Le produit est bien, rien d'exceptionnel'\n",
      "     → Neutro (96.5%)\n",
      "     M:0.00 | N:0.96 | B:0.03\n",
      "\n",
      "10. [ITA] 'Il servizio era eccellente e il personale molto gentile'\n",
      "     → Bueno (99.8%)\n",
      "     M:0.00 | N:0.00 | B:1.00\n",
      "\n",
      "11. [ITA] 'Non mi è piaciuto per niente il prodotto, è stato terribile'\n",
      "     → Malo (99.9%)\n",
      "     M:1.00 | N:0.00 | B:0.00\n",
      "\n",
      "12. [ITA] 'Il prodotto va bene, niente di speciale'\n",
      "     → Neutro (96.7%)\n",
      "     M:0.00 | N:0.97 | B:0.03\n"
     ]
    }
   ],
   "source": [
    "example_results = []\n",
    "for text in tqdm(multilingual_examples, desc=\"Detectando idiomas\", unit=\"texto\"):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        lang_name = {\n",
    "            'es': 'ESP', 'en': 'ENG', 'fr': 'FRA', 'it': 'ITA', \n",
    "            'de': 'DEU', 'pt': 'POR', 'ca': 'CAT', 'ro': 'ROM',\n",
    "            'pl': 'POL', 'nl': 'NLD'\n",
    "        }.get(lang, lang.upper())\n",
    "    except:\n",
    "        lang_name = 'UNK'\n",
    "    example_results.append(lang_name)\n",
    "\n",
    "for i, (text, pred, probs, lang_name) in enumerate(zip(multilingual_examples, test_predictions, test_probabilities, example_results)):\n",
    "    pred_label = label_names[pred.item()]\n",
    "    confidence = probs[pred].item() * 100\n",
    "    \n",
    "    print(f\"\\n{i+1:2d}. [{lang_name}] '{text[:60]}{'...' if len(text) > 60 else ''}'\")\n",
    "    print(f\"     → {pred_label} ({confidence:.1f}%)\")\n",
    "    print(f\"     M:{probs[0]:.2f} | N:{probs[1]:.2f} | B:{probs[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb9edec",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3dcbfe9c-0660-4bb8-aecd-4b12430884ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MEJORES MÉTRICAS POR CLASE:\n",
      "  Malo: F1 = 0.9174\n",
      "  Neutro: F1 = 0.7518\n",
      "  Bueno: F1 = 0.4756\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMEJORES MÉTRICAS POR CLASE:\")\n",
    "for label_name in label_names:\n",
    "    f1 = bias_metrics[label_name]['f1']\n",
    "    print(f\"  {label_name}: F1 = {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7732fdf7",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7edb55ee-8ba6-4a75-9a4f-4807a5531f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Ciudad</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Sentimiento</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I do not have any good memories of the store b...</td>\n",
       "      <td>Malo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>local experiences in tenerife the best part of...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Tenerife</td>\n",
       "      <td>Ocio</td>\n",
       "      <td>2024-11-02</td>\n",
       "      <td>TUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tickets and events in seville too crowded but ...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Seville</td>\n",
       "      <td>Ocio</td>\n",
       "      <td>2022-09-08</td>\n",
       "      <td>TUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L’expérience était très sympa , mais l’hélicop...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>local experiences in malaga spectacular scener...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Malaga</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>2023-11-29</td>\n",
       "      <td>TUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303920</th>\n",
       "      <td>Esta entrada no tiene comentarios</td>\n",
       "      <td>Malo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303921</th>\n",
       "      <td>tickets and events in palma de mallorca an unf...</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>Palma de Mallorca</td>\n",
       "      <td>Desconocido</td>\n",
       "      <td>2023-09-06</td>\n",
       "      <td>TUI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303922</th>\n",
       "      <td>We somehow missed the importance of having our...</td>\n",
       "      <td>Malo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303923</th>\n",
       "      <td>Esta entrada no tiene comentarios</td>\n",
       "      <td>Malo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303924</th>\n",
       "      <td>We found out by surprise that we were combined...</td>\n",
       "      <td>Malo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303925 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Texto  Rating  \\\n",
       "0       I do not have any good memories of the store b...    Malo   \n",
       "1       local experiences in tenerife the best part of...  Neutro   \n",
       "2       tickets and events in seville too crowded but ...  Neutro   \n",
       "3       L’expérience était très sympa , mais l’hélicop...  Neutro   \n",
       "4       local experiences in malaga spectacular scener...  Neutro   \n",
       "...                                                   ...     ...   \n",
       "303920                  Esta entrada no tiene comentarios    Malo   \n",
       "303921  tickets and events in palma de mallorca an unf...  Neutro   \n",
       "303922  We somehow missed the importance of having our...    Malo   \n",
       "303923                  Esta entrada no tiene comentarios    Malo   \n",
       "303924  We found out by surprise that we were combined...    Malo   \n",
       "\n",
       "                   Ciudad    Categoria       Fecha   Fuente  Sentimiento  \\\n",
       "0                     NaN          NaN         NaN      NaN          NaN   \n",
       "1                Tenerife         Ocio  2024-11-02      TUI          NaN   \n",
       "2                 Seville         Ocio  2022-09-08      TUI          NaN   \n",
       "3               Barcelona  Desconocido  2024-05-12  Booking          NaN   \n",
       "4                  Malaga  Desconocido  2023-11-29      TUI          NaN   \n",
       "...                   ...          ...         ...      ...          ...   \n",
       "303920                NaN          NaN         NaN      NaN          NaN   \n",
       "303921  Palma de Mallorca  Desconocido  2023-09-06      TUI          NaN   \n",
       "303922                NaN          NaN         NaN      NaN          NaN   \n",
       "303923                NaN          NaN         NaN      NaN          NaN   \n",
       "303924                NaN          NaN         NaN      NaN          NaN   \n",
       "\n",
       "        Label  \n",
       "0         0.0  \n",
       "1         1.0  \n",
       "2         1.0  \n",
       "3         1.0  \n",
       "4         1.0  \n",
       "...       ...  \n",
       "303920    0.0  \n",
       "303921    1.0  \n",
       "303922    0.0  \n",
       "303923    0.0  \n",
       "303924    0.0  \n",
       "\n",
       "[303925 rows x 8 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06fe6a9",
   "metadata": {},
   "source": [
    "## 📥 Carga de datos\n",
    "- Lee dataset: dataset_maestro.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9e598e21-99d8-4ad9-b7a8-d4f8e1340792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maestro = pd.read_csv('dataset_maestro.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a3579750-7841-4bc0-b049-3ad4b65d1454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Texto</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Ciudad</th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Fecha</th>\n",
       "      <th>Fuente</th>\n",
       "      <th>Sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>É stato veloce, arrivati e passati subito. Con...</td>\n",
       "      <td>Bueno</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-18</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fantastisk oplevelse og det gik enormt hurtigt...</td>\n",
       "      <td>Bueno</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Sagrada Familia is a \"must\" in terms of par...</td>\n",
       "      <td>Bueno</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nous sommes arrivés en retard et le monsieur à...</td>\n",
       "      <td>Bueno</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Playa</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>j’ai choisi d’y aller vers 15h un mardi, il y ...</td>\n",
       "      <td>Bueno</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Playa</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>Booking</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Texto Rating     Ciudad  \\\n",
       "0  É stato veloce, arrivati e passati subito. Con...  Bueno  Barcelona   \n",
       "1  Fantastisk oplevelse og det gik enormt hurtigt...  Bueno  Barcelona   \n",
       "2  La Sagrada Familia is a \"must\" in terms of par...  Bueno  Barcelona   \n",
       "3  Nous sommes arrivés en retard et le monsieur à...  Bueno  Barcelona   \n",
       "4  j’ai choisi d’y aller vers 15h un mardi, il y ...  Bueno  Barcelona   \n",
       "\n",
       "  Categoria       Fecha   Fuente  Sentimiento  \n",
       "0      Tour  2025-07-18  Booking          NaN  \n",
       "1      Tour  2025-07-17  Booking          NaN  \n",
       "2      Tour  2025-07-17  Booking          NaN  \n",
       "3     Playa  2025-07-17  Booking          NaN  \n",
       "4     Playa  2025-07-17  Booking          NaN  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_maestro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4e9f2953-0e65-48ce-a876-581e187600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = df_maestro['Texto'].fillna('').astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c6728",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c7b5d947-d216-45ce-a75a-6e9f3306950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41860923",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0b25ef93-99cf-4355-920f-d75cba4880ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir_con_bert_gpu(df, columna_texto, ruta_modelo, batch_size=8):\n",
    "    # Limpiar caché inicial\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Configurar dispositivo (GPU si está disponible)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "    \n",
    "    if device.type == 'cpu':\n",
    "        print(\"GPU no disponible, usando CPU (será más lento)\")\n",
    "    else:\n",
    "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria GPU libre: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "    \n",
    "    # Cargar modelo y tokenizador\n",
    "    print(\"Cargando modelo...\")\n",
    "    modelo = BertForSequenceClassification.from_pretrained(ruta_modelo)\n",
    "    tokenizador = BertTokenizer.from_pretrained(ruta_modelo)\n",
    "    \n",
    "    # Mover modelo a GPU\n",
    "    modelo = modelo.to(device)\n",
    "    modelo.eval()\n",
    "    \n",
    "    # Preparar textos\n",
    "    textos = df[columna_texto].fillna('').astype(str).tolist()\n",
    "    total_textos = len(textos)\n",
    "    \n",
    "    predicciones = []\n",
    "    confianzas = []\n",
    "    \n",
    "    print(f\"Procesando {total_textos} textos en batches de {batch_size}...\")\n",
    "    print(\"Progreso:\")\n",
    "    \n",
    "    try:\n",
    "        for i in range(0, total_textos, batch_size):\n",
    "            # Mostrar progreso cada 50 batches\n",
    "            if i % (batch_size * 50) == 0:\n",
    "                porcentaje = (i / total_textos) * 100\n",
    "                print(f\"  {porcentaje:.1f}% - Procesado: {i}/{total_textos}\")\n",
    "                \n",
    "                # Mostrar uso de memoria GPU si está disponible\n",
    "                if device.type == 'cuda':\n",
    "                    memoria_usada = torch.cuda.memory_allocated() / 1024**3\n",
    "                    print(f\"Memoria GPU usada: {memoria_usada:.2f} GB\")\n",
    "            \n",
    "            batch_textos = textos[i:i+batch_size]\n",
    "            \n",
    "            # Tokenizar\n",
    "            inputs = tokenizador(\n",
    "                batch_textos,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Mover inputs a GPU\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Predecir\n",
    "            with torch.no_grad():\n",
    "                outputs = modelo(**inputs)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                # Mover resultados de vuelta a CPU y guardar\n",
    "                predicciones.extend(preds.cpu().numpy())\n",
    "                confianzas.extend(probs.max(dim=-1)[0].cpu().numpy())\n",
    "            \n",
    "            # Limpiar caché GPU cada 100 batches para evitar sobrecarga\n",
    "            if i % (batch_size * 100) == 0 and i > 0:\n",
    "                if device.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                print(\"Caché limpiado\")\n",
    "        \n",
    "        print(\"Procesamiento completado!\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"Error de memoria GPU. Reduce batch_size a {batch_size//2} e intenta de nuevo\")\n",
    "            print(\"   O usa device='cpu' para procesar en CPU\")\n",
    "            torch.cuda.empty_cache()\n",
    "            return None, None\n",
    "        else:\n",
    "            print(f\"Error durante la predicción: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    finally:\n",
    "        # Limpieza final\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    return predicciones, confianzas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4b622",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e661cd08-9159-401c-a290-a24c175167b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_gpu():\n",
    "    \"\"\"Función para verificar el estado de la GPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU disponible: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Memoria total: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB\")\n",
    "        print(f\"Memoria libre: {torch.cuda.memory_reserved(0) // 1024**3} GB\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"GPU no disponible\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6268af1c-4b1a-42bf-af3d-9c3b72184e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Memoria total: 3 GB\n",
      "Memoria libre: 5 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verificar_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "451bd88b-9355-4239-acf1-26d0ce540972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./modelo_sentimientos_multilingual_balanced\\config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.56.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "loading weights file ./modelo_sentimientos_multilingual_balanced\\model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "GPU disponible: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Memoria GPU libre: 3 GB\n",
      "Cargando modelo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./modelo_sentimientos_multilingual_balanced.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file tokenizer.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando 855800 textos en batches de 16...\n",
      "Progreso:\n",
      "  0.0% - Procesado: 0/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.1% - Procesado: 800/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.2% - Procesado: 1600/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  0.3% - Procesado: 2400/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.4% - Procesado: 3200/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  0.5% - Procesado: 4000/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.6% - Procesado: 4800/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  0.7% - Procesado: 5600/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.7% - Procesado: 6400/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  0.8% - Procesado: 7200/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  0.9% - Procesado: 8000/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  1.0% - Procesado: 8800/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  1.1% - Procesado: 9600/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  1.2% - Procesado: 10400/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  1.3% - Procesado: 11200/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  1.4% - Procesado: 12000/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  1.5% - Procesado: 12800/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  1.6% - Procesado: 13600/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  1.7% - Procesado: 14400/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n",
      "  1.8% - Procesado: 15200/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "  1.9% - Procesado: 16000/855800\n",
      "Memoria GPU usada: 5.32 GB\n",
      "Caché limpiado\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m batch_size_inicial \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m----> 3\u001b[0m preds, conf \u001b[38;5;241m=\u001b[39m predecir_con_bert_gpu(\n\u001b[0;32m      4\u001b[0m     df_maestro, \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTexto\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./modelo_sentimientos_multilingual_balanced\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size_inicial\n\u001b[0;32m      8\u001b[0m )\n",
      "Cell \u001b[1;32mIn[153], line 68\u001b[0m, in \u001b[0;36mpredecir_con_bert_gpu\u001b[1;34m(df, columna_texto, ruta_modelo, batch_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Mover resultados de vuelta a CPU y guardar\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     predicciones\u001b[38;5;241m.\u001b[39mextend(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     69\u001b[0m     confianzas\u001b[38;5;241m.\u001b[39mextend(probs\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Limpiar caché GPU cada 100 batches para evitar sobrecarga\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size_inicial = 16\n",
    "\n",
    "preds, conf = predecir_con_bert_gpu(\n",
    "    df_maestro, \n",
    "    'Texto', \n",
    "    './modelo_sentimientos_multilingual_balanced',\n",
    "    batch_size=batch_size_inicial\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b621d134",
   "metadata": {},
   "source": [
    "## 💾 Guardado / exportación\n",
    "- Exporta resultados/tablas.\n",
    "- Archivos: df_final.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e244fdc-3a5e-4bc6-9d4a-6ffb274d72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if preds is not None:\n",
    "    # Mapear a etiquetas\n",
    "    mapeo = {0: 'negativo', 1: 'neutro', 2: 'positivo'}\n",
    "    df['sentimiento'] = [mapeo[p] for p in preds]\n",
    "    df['confianza'] = conf\n",
    "    \n",
    "    # Guardar resultado\n",
    "    df.to_csv('df_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7ed67e",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: transformers, torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6fbc7a-92c9-480c-8203-bf127e851bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Cargando modelo...\n",
      "Memoria GPU usada: 0.66 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Limpiar caché inicial\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar SOLO el modelo\n",
    "print(\"Cargando modelo...\")\n",
    "modelo = BertForSequenceClassification.from_pretrained('./modelo_sentimientos_multilingual_balanced')\n",
    "tokenizador = BertTokenizer.from_pretrained('./modelo_sentimientos_multilingual_balanced')\n",
    "\n",
    "# Mover a GPU\n",
    "modelo = modelo.to(device)\n",
    "modelo.eval()\n",
    "\n",
    "print(f\"Memoria GPU usada: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c3cb6a",
   "metadata": {},
   "source": [
    "## 📦 Dependencias\n",
    "- Importa librerías: torch, pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135f331d-1ed9-418f-927e-590ae3aaf7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fb7d5c",
   "metadata": {},
   "source": [
    "## 🎲 Reproducibilidad y dispositivo\n",
    "- Configura **dispositivo** (CPU/GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e3fddae-6c6c-4abe-a4ce-dacf65d49393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_por_chunks(ruta_csv, columna_texto, ruta_modelo, chunk_size=50000, batch_size=16):\n",
    "    \"\"\"Procesa dataset por chunks para optimizar memoria\"\"\"\n",
    "    \n",
    "    # Cargar modelo una vez\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Cargando modelo en {device}...\")\n",
    "    \n",
    "    modelo = BertForSequenceClassification.from_pretrained(ruta_modelo)\n",
    "    tokenizador = BertTokenizer.from_pretrained(ruta_modelo)\n",
    "    modelo = modelo.to(device)\n",
    "    modelo.eval()\n",
    "    \n",
    "    # Contar filas totales\n",
    "    total_filas = sum(1 for line in open(ruta_csv, encoding='utf-8')) - 1\n",
    "    total_chunks = (total_filas // chunk_size) + (1 if total_filas % chunk_size else 0)\n",
    "    \n",
    "    print(f\"Total filas: {total_filas:,}\")\n",
    "    print(f\"Total chunks: {total_chunks}\")\n",
    "    \n",
    "    archivo_final = 'resultado_completo.csv'\n",
    "    primera_vez = True\n",
    "    \n",
    "    # Procesar chunk por chunk\n",
    "    for chunk_num in range(total_chunks):\n",
    "        print(f\"\\nProcesando chunk {chunk_num + 1}/{total_chunks}\")\n",
    "        \n",
    "        inicio = chunk_num * chunk_size\n",
    "        \n",
    "        # Cargar solo este chunk\n",
    "        if chunk_num == 0:\n",
    "            chunk_df = pd.read_csv(ruta_csv, nrows=chunk_size, encoding='utf-8')\n",
    "        else:\n",
    "            chunk_df = pd.read_csv(ruta_csv, skiprows=range(1, inicio + 1), nrows=chunk_size, encoding='utf-8')\n",
    "        \n",
    "        # Procesar chunk\n",
    "        textos = chunk_df[columna_texto].fillna('').astype(str).tolist()\n",
    "        predicciones = []\n",
    "        confianzas = []\n",
    "        \n",
    "        # Procesar en batches\n",
    "        for i in range(0, len(textos), batch_size):\n",
    "            batch_textos = textos[i:i+batch_size]\n",
    "            \n",
    "            inputs = tokenizador(\n",
    "                batch_textos,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = modelo(**inputs)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "                preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                \n",
    "                predicciones.extend(preds.cpu().numpy())\n",
    "                confianzas.extend(probs.max(dim=-1)[0].cpu().numpy())\n",
    "        \n",
    "        # Agregar resultados al chunk\n",
    "        mapeo = {0: 'negativo', 1: 'neutro', 2: 'positivo'}\n",
    "        chunk_df['sentimiento'] = [mapeo[p] for p in predicciones]\n",
    "        chunk_df['confianza'] = confianzas\n",
    "        \n",
    "        # Guardar chunk\n",
    "        modo = 'w' if primera_vez else 'a'\n",
    "        header = primera_vez\n",
    "        chunk_df.to_csv(archivo_final, mode=modo, header=header, index=False)\n",
    "        primera_vez = False\n",
    "        \n",
    "        print(f\"Chunk {chunk_num + 1} guardado\")\n",
    "        \n",
    "        # Limpiar memoria\n",
    "        del chunk_df, predicciones, confianzas, textos\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"\\nCompletado! Resultado en: {archivo_final}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e96ae",
   "metadata": {},
   "source": [
    "## 🧪 Bloque de código\n",
    "- Ejecución auxiliar del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89ce5ff6-e12a-433c-b836-e0fbb52f3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo en cuda...\n",
      "Total filas: 855,800\n",
      "Total chunks: 18\n",
      "\n",
      "Procesando chunk 1/18\n",
      "Chunk 1 guardado\n",
      "\n",
      "Procesando chunk 2/18\n",
      "Chunk 2 guardado\n",
      "\n",
      "Procesando chunk 3/18\n",
      "Chunk 3 guardado\n",
      "\n",
      "Procesando chunk 4/18\n",
      "Chunk 4 guardado\n",
      "\n",
      "Procesando chunk 5/18\n",
      "Chunk 5 guardado\n",
      "\n",
      "Procesando chunk 6/18\n",
      "Chunk 6 guardado\n",
      "\n",
      "Procesando chunk 7/18\n",
      "Chunk 7 guardado\n",
      "\n",
      "Procesando chunk 8/18\n",
      "Chunk 8 guardado\n",
      "\n",
      "Procesando chunk 9/18\n",
      "Chunk 9 guardado\n",
      "\n",
      "Procesando chunk 10/18\n",
      "Chunk 10 guardado\n",
      "\n",
      "Procesando chunk 11/18\n",
      "Chunk 11 guardado\n",
      "\n",
      "Procesando chunk 12/18\n",
      "Chunk 12 guardado\n",
      "\n",
      "Procesando chunk 13/18\n",
      "Chunk 13 guardado\n",
      "\n",
      "Procesando chunk 14/18\n",
      "Chunk 14 guardado\n",
      "\n",
      "Procesando chunk 15/18\n",
      "Chunk 15 guardado\n",
      "\n",
      "Procesando chunk 16/18\n",
      "Chunk 16 guardado\n",
      "\n",
      "Procesando chunk 17/18\n",
      "Chunk 17 guardado\n",
      "\n",
      "Procesando chunk 18/18\n",
      "Chunk 18 guardado\n",
      "\n",
      "Completado! Resultado en: resultado_completo.csv\n"
     ]
    }
   ],
   "source": [
    "ruta_csv = 'dataset_maestro.csv'     \n",
    "columna_texto = 'Texto'           \n",
    "ruta_modelo = './modelo_sentimientos_multilingual_balanced'      \n",
    "\n",
    "procesar_por_chunks(ruta_csv, columna_texto, ruta_modelo, chunk_size=50000, batch_size=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
