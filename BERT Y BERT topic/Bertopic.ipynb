{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad48489e",
   "metadata": {},
   "source": [
    "# üß† BERTOPIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d75eda",
   "metadata": {},
   "source": [
    "## üß∞ Librer√≠as e importaciones\n",
    "- Importa pandas para el flujo de t√≥picos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3545231-3e35-47c9-beea-50ea5503c347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6601b-1733-4c00-9ffc-ce17f47015b3",
   "metadata": {},
   "source": [
    "- El c√≥digo est√° pensado para cargar y combinar archivos .pkl (pickles) que fueron guardados en m√°quinas con CUDA/GPU, pero forzando su lectura en CPU. Lo hace con varias ‚Äúcapas de rescate‚Äù por si la carga falla, y luego concatena los datos v√°lidos en un √∫nico DataFrame final, generando adem√°s res√∫menes y an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b20cb4-857d-4cdb-8937-1092bb4db37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORZAR que PyTorch piense que CUDA no existe\n",
    "torch.cuda.is_available = lambda: False\n",
    "\n",
    "class DefinitiveCudaLoader:\n",
    "    \"\"\"Cargador definitivo que maneja archivos PKL con CUDA embedido\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.status_file = self.output_dir / 'processing_status.json'\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.load_processing_status()\n",
    "    \n",
    "    def load_processing_status(self):\n",
    "        if self.status_file.exists():\n",
    "            with open(self.status_file, 'r') as f:\n",
    "                self.status = json.load(f)\n",
    "        else:\n",
    "            self.status = {'completed_batches': []}\n",
    "    \n",
    "    def load_pkl_with_cuda_override(self, file_path):\n",
    "        \"\"\"Carga PKL sobrescribiendo completamente las referencias CUDA\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # M√©todo 1: torch.load con map_location espec√≠fico\n",
    "            self.logger.info(f\"M√©todo 1: torch.load directo para {file_path.name}\")\n",
    "            \n",
    "            with open(file_path, 'rb') as f:\n",
    "                # Usar torch.device expl√≠cito\n",
    "                data = torch.load(f, map_location=torch.device('cpu'))\n",
    "            \n",
    "            self.logger.info(f\"‚úì M√©todo 1 exitoso para {file_path.name}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e1:\n",
    "            self.logger.warning(f\"M√©todo 1 fall√≥: {str(e1)[:100]}\")\n",
    "            \n",
    "            try:\n",
    "                # M√©todo 2: Reemplazar funciones CUDA en tiempo real\n",
    "                self.logger.info(f\"M√©todo 2: Override CUDA para {file_path.name}\")\n",
    "                \n",
    "                # Backup de funciones originales\n",
    "                original_cuda_device = torch.cuda.device\n",
    "                original_cuda_set_device = torch.cuda.set_device\n",
    "                \n",
    "                # Sobrescribir funciones CUDA\n",
    "                torch.cuda.device = lambda x: None\n",
    "                torch.cuda.set_device = lambda x: None\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        data = torch.load(f, map_location='cpu')\n",
    "                    \n",
    "                    self.logger.info(f\"‚úì M√©todo 2 exitoso para {file_path.name}\")\n",
    "                    return data\n",
    "                    \n",
    "                finally:\n",
    "                    # Restaurar funciones\n",
    "                    torch.cuda.device = original_cuda_device\n",
    "                    torch.cuda.set_device = original_cuda_set_device\n",
    "                    \n",
    "            except Exception as e2:\n",
    "                self.logger.warning(f\"M√©todo 2 fall√≥: {str(e2)[:100]}\")\n",
    "                \n",
    "                try:\n",
    "                    # M√©todo 3: Unpickler con override completo\n",
    "                    self.logger.info(f\"M√©todo 3: Unpickler personalizado para {file_path.name}\")\n",
    "                    \n",
    "                    class ForceCPUUnpickler(pickle.Unpickler):\n",
    "                        def find_class(self, module, name):\n",
    "                            # Interceptar cualquier cosa relacionada con CUDA\n",
    "                            if 'cuda' in module.lower() or 'cuda' in name.lower():\n",
    "                                # Redirigir a CPU\n",
    "                                if name == 'FloatTensor':\n",
    "                                    return torch.FloatTensor\n",
    "                                elif name == 'LongTensor':\n",
    "                                    return torch.LongTensor\n",
    "                                elif name == 'device':\n",
    "                                    return lambda x: torch.device('cpu')\n",
    "                                else:\n",
    "                                    return super().find_class('torch', name)\n",
    "                            \n",
    "                            # Para storage de torch\n",
    "                            if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "                                def load_tensor(b):\n",
    "                                    return torch.load(io.BytesIO(b), map_location='cpu')\n",
    "                                return load_tensor\n",
    "                            \n",
    "                            return super().find_class(module, name)\n",
    "                    \n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        unpickler = ForceCPUUnpickler(f)\n",
    "                        data = unpickler.load()\n",
    "                    \n",
    "                    self.logger.info(f\"‚úì M√©todo 3 exitoso para {file_path.name}\")\n",
    "                    return data\n",
    "                    \n",
    "                except Exception as e3:\n",
    "                    self.logger.warning(f\"M√©todo 3 fall√≥: {str(e3)[:100]}\")\n",
    "                    \n",
    "                    # M√©todo 4: Extraer solo DataFrames usando b√∫squeda de patrones\n",
    "                    return self.extract_dataframes_only(file_path)\n",
    "    \n",
    "    def extract_dataframes_only(self, file_path):\n",
    "        \"\"\"M√©todo de √∫ltimo recurso: extraer solo DataFrames ignorando modelos\"\"\"\n",
    "        \n",
    "        self.logger.info(f\"M√©todo 4: Extracci√≥n de DataFrames para {file_path.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Leer archivo completo en memoria\n",
    "            with open(file_path, 'rb') as f:\n",
    "                raw_data = f.read()\n",
    "            \n",
    "            # Buscar patrones de pandas DataFrame en los bytes\n",
    "            # Los DataFrames tienen firmas espec√≠ficas en pickle\n",
    "            \n",
    "            # Crear estructura m√≠nima basada en el nombre del archivo\n",
    "            batch_id = file_path.name.replace(\"_results.pkl\", \"\")\n",
    "            parts = batch_id.split(\"_\")\n",
    "            sentiment = parts[0]\n",
    "            \n",
    "            self.logger.warning(f\"Creando estructura m√≠nima para {batch_id}\")\n",
    "            \n",
    "            # Intentar cargar solo metadatos b√°sicos si es posible\n",
    "            try:\n",
    "                # Buscar strings JSON embebidos\n",
    "                import re\n",
    "                json_pattern = rb'\\{\"batch_id\".*?\\}'\n",
    "                json_matches = re.findall(json_pattern, raw_data)\n",
    "                \n",
    "                if json_matches:\n",
    "                    # Intentar decodificar metadata\n",
    "                    metadata_str = json_matches[0].decode('utf-8')\n",
    "                    metadata = json.loads(metadata_str)\n",
    "                else:\n",
    "                    metadata = {}\n",
    "                \n",
    "            except Exception:\n",
    "                metadata = {}\n",
    "            \n",
    "            # Estructura m√≠nima de respaldo\n",
    "            result = {\n",
    "                'batch_id': batch_id,\n",
    "                'sentiment': sentiment,\n",
    "                'data': pd.DataFrame(),  # DataFrame vac√≠o - ser√° omitido\n",
    "                'topic_info': pd.DataFrame({'Topic': [-1], 'Count': [0]}),\n",
    "                'topic_language_analysis': {},\n",
    "                'language_distribution': {},\n",
    "                'n_topics': 0,\n",
    "                **metadata\n",
    "            }\n",
    "            \n",
    "            self.logger.warning(f\"‚ö† Estructura m√≠nima creada para {batch_id}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"M√©todo 4 fall√≥ para {file_path.name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def combine_with_override(self):\n",
    "        \"\"\"Combina todos los archivos usando el cargador con override\"\"\"\n",
    "        \n",
    "        self.logger.info(\"=== INICIANDO COMBINACI√ìN CON OVERRIDE CUDA ===\")\n",
    "        \n",
    "        result_files = list(self.output_dir.glob(\"*_results.pkl\"))\n",
    "        self.logger.info(f\"Total archivos encontrados: {len(result_files)}\")\n",
    "        \n",
    "        combined_data = []\n",
    "        language_analysis = {}\n",
    "        topic_mappings = {}\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        skipped_empty = 0\n",
    "        \n",
    "        for i, file_path in enumerate(result_files):\n",
    "            batch_id = file_path.name.replace(\"_results.pkl\", \"\")\n",
    "            \n",
    "            self.logger.info(f\"\\n--- Procesando {i+1}/{len(result_files)}: {batch_id} ---\")\n",
    "            \n",
    "            if batch_id in self.status.get('completed_batches', []):\n",
    "                \n",
    "                # Cargar con override CUDA\n",
    "                batch_data = self.load_pkl_with_cuda_override(file_path)\n",
    "                \n",
    "                if batch_data is not None:\n",
    "                    # Verificar que tenga datos v√°lidos\n",
    "                    if ('data' in batch_data and \n",
    "                        isinstance(batch_data['data'], pd.DataFrame) and \n",
    "                        len(batch_data['data']) > 0):\n",
    "                        \n",
    "                        # Procesar datos v√°lidos\n",
    "                        self._process_valid_batch(\n",
    "                            batch_data, combined_data, language_analysis, topic_mappings\n",
    "                        )\n",
    "                        successful += 1\n",
    "                        self.logger.info(f\"‚úì {batch_id}: {len(batch_data['data'])} documentos\")\n",
    "                        \n",
    "                    else:\n",
    "                        self.logger.warning(f\"‚ö† {batch_id}: DataFrame vac√≠o, omitiendo\")\n",
    "                        skipped_empty += 1\n",
    "                        \n",
    "                else:\n",
    "                    self.logger.error(f\"‚úó {batch_id}: No se pudo cargar\")\n",
    "                    failed += 1\n",
    "            else:\n",
    "                self.logger.info(f\"- {batch_id}: No en completed_batches\")\n",
    "        \n",
    "        # Resumen de carga\n",
    "        self.logger.info(f\"\\n=== RESUMEN DE CARGA ===\")\n",
    "        self.logger.info(f\"Exitosos: {successful}\")\n",
    "        self.logger.info(f\"Vac√≠os omitidos: {skipped_empty}\")\n",
    "        self.logger.info(f\"Fallidos: {failed}\")\n",
    "        \n",
    "        if combined_data:\n",
    "            self.logger.info(\"Creando DataFrame final...\")\n",
    "            final_df = pd.concat(combined_data, ignore_index=True)\n",
    "            \n",
    "            # Guardar resultado\n",
    "            output_path = self.output_dir / \"combined_multilingual_results_final.pkl\"\n",
    "            final_df.to_pickle(output_path)\n",
    "            \n",
    "            # Guardar an√°lisis\n",
    "            with open(self.output_dir / \"multilingual_language_analysis_final.json\", 'w') as f:\n",
    "                json.dump(language_analysis, f, indent=2, default=str)\n",
    "            \n",
    "            # Crear resumen final\n",
    "            summary = {\n",
    "                'total_documents': len(final_df),\n",
    "                'successful_batches': successful,\n",
    "                'skipped_empty': skipped_empty,\n",
    "                'failed_batches': failed,\n",
    "                'sentiments': final_df['sentimiento'].value_counts().to_dict() if 'sentimiento' in final_df.columns else {},\n",
    "                'languages': final_df['idioma'].value_counts().to_dict() if 'idioma' in final_df.columns else {},\n",
    "                'total_topics': final_df['global_topic'].nunique() if 'global_topic' in final_df.columns else 0\n",
    "            }\n",
    "            \n",
    "            with open(self.output_dir / \"final_summary_override.json\", 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "            self.logger.info(f\"üéâ √âXITO: {len(final_df):,} documentos combinados\")\n",
    "            return final_df, {}, language_analysis, summary\n",
    "            \n",
    "        else:\n",
    "            self.logger.error(\"üí• FALLO: No se pudieron combinar datos\")\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def _process_valid_batch(self, batch_data, combined_data, language_analysis, topic_mappings):\n",
    "        \"\"\"Procesa un lote con datos v√°lidos\"\"\"\n",
    "        \n",
    "        sentiment = batch_data.get('sentiment', 'unknown')\n",
    "        df = batch_data['data']\n",
    "        \n",
    "        # Inicializar estructuras\n",
    "        if sentiment not in topic_mappings:\n",
    "            topic_mappings[sentiment] = {}\n",
    "            language_analysis[sentiment] = {}\n",
    "        \n",
    "        # Remapear temas\n",
    "        if 'topic' in df.columns:\n",
    "            n_existing = len([t for t in topic_mappings[sentiment].values() if t != -1])\n",
    "            \n",
    "            for topic in df['topic'].unique():\n",
    "                if topic not in topic_mappings[sentiment]:\n",
    "                    topic_mappings[sentiment][topic] = -1 if topic == -1 else n_existing\n",
    "                    if topic != -1:\n",
    "                        n_existing += 1\n",
    "            \n",
    "            df['global_topic'] = df['topic'].map(topic_mappings[sentiment])\n",
    "        \n",
    "        # Agregar metadatos\n",
    "        df['batch_lang_group'] = batch_data.get('lang_group', 'unknown')\n",
    "        \n",
    "        combined_data.append(df)\n",
    "        \n",
    "        # Procesar an√°lisis de idiomas\n",
    "        topic_lang = batch_data.get('topic_language_analysis', {})\n",
    "        for topic_id, langs in topic_lang.items():\n",
    "            mapped_topic = topic_mappings[sentiment].get(topic_id, topic_id)\n",
    "            if mapped_topic not in language_analysis[sentiment]:\n",
    "                language_analysis[sentiment][mapped_topic] = {}\n",
    "            \n",
    "            for lang, count in langs.items():\n",
    "                if lang not in language_analysis[sentiment][mapped_topic]:\n",
    "                    language_analysis[sentiment][mapped_topic][lang] = 0\n",
    "                language_analysis[sentiment][mapped_topic][lang] += count\n",
    "\n",
    "\n",
    "def ultimate_combine_results(output_dir=\"E:/bertopic_800k_multilingue\"):\n",
    "    \"\"\"FUNCI√ìN DEFINITIVA - Combina archivos con override CUDA completo\"\"\"\n",
    "    \n",
    "    print(\"=== COMBINACI√ìN DEFINITIVA CON OVERRIDE CUDA ===\")\n",
    "    print(f\"Directorio: {output_dir}\")\n",
    "    \n",
    "    loader = DefinitiveCudaLoader(output_dir)\n",
    "    result = loader.combine_with_override()\n",
    "    \n",
    "    if result[0] is not None:\n",
    "        final_df, models, lang_analysis, summary = result\n",
    "        \n",
    "        print(f\"\\nüéä √âXITO DEFINITIVO üéä\")\n",
    "        print(f\"Total documentos: {len(final_df):,}\")\n",
    "        print(f\"Lotes procesados exitosamente: {summary.get('successful_batches', 0)}\")\n",
    "        print(f\"Lotes omitidos (vac√≠os): {summary.get('skipped_empty', 0)}\")\n",
    "        print(f\"Lotes fallidos: {summary.get('failed_batches', 0)}\")\n",
    "        \n",
    "        if 'sentimiento' in final_df.columns:\n",
    "            print(f\"Sentimientos: {list(final_df['sentimiento'].unique())}\")\n",
    "        if 'idioma' in final_df.columns:\n",
    "            print(f\"Idiomas: {final_df['idioma'].nunique()}\")\n",
    "        if 'global_topic' in final_df.columns:\n",
    "            print(f\"Temas: {final_df['global_topic'].nunique()}\")\n",
    "        \n",
    "        return final_df, models, lang_analysis, summary\n",
    "    else:\n",
    "        print(\"‚ùå Fall√≥ la combinaci√≥n definitiva\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743515cc",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de BERTopic\n",
    "- Define hiperpar√°metros/pipe (vectorizador, umap/hdbscan, idioma).\n",
    "- Llama a la funci√≥n ultimate_combine_results con el directorio \"E:/bertopic_800k_multilingue\" y desempaqueta lo que devuelve en cuatro variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e905d-2455-4a99-a1bb-a6a7c7a86a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINACI√ìN DEFINITIVA CON OVERRIDE CUDA ===\n",
      "Directorio: E:/bertopic_800k_multilingue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== INICIANDO COMBINACI√ìN CON OVERRIDE CUDA ===\n",
      "INFO:__main__:Total archivos encontrados: 216\n",
      "INFO:__main__:\n",
      "--- Procesando 1/216: negativo_main_batch_000 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_000_results.pkl\n",
      "C:\\Users\\swart\\AppData\\Local\\Temp\\ipykernel_16128\\86875392.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(f, map_location=torch.device('cpu'))\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_000_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_000_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_000_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_000: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 2/216: negativo_main_batch_001 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_001_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_001_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_001_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_001_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_001: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 3/216: negativo_main_batch_002 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_002_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_002_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_002_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_002_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_002: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 4/216: negativo_main_batch_003 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_003_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_003_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_003_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_003_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_003: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 5/216: negativo_main_batch_004 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_004_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_004_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_004_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_004_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_004: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 6/216: negativo_main_batch_005 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_005_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_005_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_005_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_005_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_005: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 7/216: negativo_main_batch_006 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_006_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_006_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_006_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_006_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_006: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 8/216: negativo_main_batch_007 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_007_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_007_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_007_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_007_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_007: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 9/216: negativo_main_batch_008 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_008_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_008_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_008_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_008_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_008: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 10/216: negativo_main_batch_009 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_009_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_009_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_009_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_009_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_009: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 11/216: negativo_main_batch_010 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_010_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_010_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_010_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_010_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_010: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 12/216: negativo_main_batch_011 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_011_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_011_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_011_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_011_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_011: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 13/216: negativo_main_batch_012 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_012_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_012_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_012_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_012_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_012: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 14/216: negativo_main_batch_013 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_013_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_013_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_013_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_013_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_013: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 15/216: negativo_main_batch_014 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_014_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_014_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_014_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_014_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_014: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 16/216: negativo_main_batch_015 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_015_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_015_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_015_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_015_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_015: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 17/216: negativo_main_batch_016 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_016_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_016_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_016_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_016_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_016: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 18/216: negativo_main_batch_017 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_017_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_017_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_017_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_017_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_017: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 19/216: negativo_main_batch_018 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_018_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_018_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_018_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_018_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_018: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 20/216: negativo_main_batch_019 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_019_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_019_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_019_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_019_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_019: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 21/216: negativo_main_batch_020 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_020_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_020_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_020_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_020_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_020: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 22/216: negativo_main_batch_021 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_021_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_021_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_021_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_021_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_021: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 23/216: negativo_main_batch_022 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_022_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_022_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_022_results.pkl\n",
      "INFO:__main__:‚úì M√©todo 3 exitoso para negativo_main_batch_022_results.pkl\n",
      "INFO:__main__:‚úì negativo_main_batch_022: 4000 documentos\n",
      "INFO:__main__:\n",
      "--- Procesando 24/216: negativo_main_batch_023 ---\n",
      "INFO:__main__:M√©todo 1: torch.load directo para negativo_main_batch_023_results.pkl\n",
      "WARNING:__main__:M√©todo 1 fall√≥: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are\n",
      "INFO:__main__:M√©todo 2: Override CUDA para negativo_main_batch_023_results.pkl\n",
      "WARNING:__main__:M√©todo 2 fall√≥: isinstance() arg 2 must be a type, a tuple of types, or a union\n",
      "INFO:__main__:M√©todo 3: Unpickler personalizado para negativo_main_batch_023_results.pkl\n"
     ]
    }
   ],
   "source": [
    "final_df, models, lang_analysis, summary = ultimate_combine_results(\"E:/bertopic_800k_multilingue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441b9a8",
   "metadata": {},
   "source": [
    "## üß∞ Librer√≠as e importaciones\n",
    "- Prepara todas las librer√≠as necesarias para cargar/combinar pickles y trabajar con datos y registros en CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5056b6-b32d-4dc1-8d09-4f219f8dd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d100a8a",
   "metadata": {},
   "source": [
    "## üì• Carga de datos\n",
    "- Lee el dataset de entrada (processing_status.json, multilingual_language_analysis_method3.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e909738-6b62-4acf-87c6-b763fb259a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMethod3Combiner:\n",
    "    \"\"\"Combinador optimizado que usa solo el m√©todo 3 (m√°s confiable)\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.status_file = self.output_dir / 'processing_status.json'\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.load_processing_status()\n",
    "    \n",
    "    def load_processing_status(self):\n",
    "        if self.status_file.exists():\n",
    "            with open(self.status_file, 'r') as f:\n",
    "                self.status = json.load(f)\n",
    "        else:\n",
    "            self.status = {'completed_batches': []}\n",
    "    \n",
    "    def load_with_cpu_unpickler(self, file_path):\n",
    "        \"\"\"Carga usando solo el m√©todo 3 optimizado\"\"\"\n",
    "        \n",
    "        class OptimizedCPUUnpickler(pickle.Unpickler):\n",
    "            def find_class(self, module, name):\n",
    "                # Interceptar cualquier cosa relacionada con CUDA\n",
    "                if 'cuda' in module.lower() or 'cuda' in name.lower():\n",
    "                    if name == 'FloatTensor':\n",
    "                        return torch.FloatTensor\n",
    "                    elif name == 'LongTensor':\n",
    "                        return torch.LongTensor\n",
    "                    elif name == 'device':\n",
    "                        return lambda x: torch.device('cpu')\n",
    "                    else:\n",
    "                        return super().find_class('torch', name)\n",
    "                \n",
    "                # Para storage de torch\n",
    "                if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "                    def load_tensor(b):\n",
    "                        return torch.load(io.BytesIO(b), map_location='cpu')\n",
    "                    return load_tensor\n",
    "                \n",
    "                return super().find_class(module, name)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                unpickler = OptimizedCPUUnpickler(f)\n",
    "                data = unpickler.load()\n",
    "            \n",
    "            self.logger.info(f\"‚úì Cargado exitosamente: {file_path.name}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"‚úó Error cargando {file_path.name}: {str(e)[:100]}\")\n",
    "            return None\n",
    "    \n",
    "    def combine_results_method3_only(self):\n",
    "        \"\"\"Combina resultados usando solo m√©todo 3\"\"\"\n",
    "        \n",
    "        self.logger.info(\"=== COMBINACI√ìN M√âTODO 3 OPTIMIZADO ===\")\n",
    "        \n",
    "        result_files = list(self.output_dir.glob(\"*_results.pkl\"))\n",
    "        self.logger.info(f\"Archivos encontrados: {len(result_files)}\")\n",
    "        \n",
    "        combined_data = []\n",
    "        language_analysis = {}\n",
    "        topic_mappings = {}\n",
    "        \n",
    "        successful = 0\n",
    "        failed = 0\n",
    "        empty_skipped = 0\n",
    "        \n",
    "        for i, file_path in enumerate(result_files):\n",
    "            batch_id = file_path.name.replace(\"_results.pkl\", \"\")\n",
    "            \n",
    "            # Progreso cada 10 archivos\n",
    "            if (i + 1) % 10 == 0 or i == 0:\n",
    "                self.logger.info(f\"Progreso: {i+1}/{len(result_files)} ({(i+1)/len(result_files)*100:.1f}%)\")\n",
    "            \n",
    "            if batch_id in self.status.get('completed_batches', []):\n",
    "                # Cargar con m√©todo 3\n",
    "                batch_data = self.load_with_cpu_unpickler(file_path)\n",
    "                \n",
    "                if batch_data is not None:\n",
    "                    if (isinstance(batch_data, dict) and \n",
    "                        'data' in batch_data and \n",
    "                        isinstance(batch_data['data'], pd.DataFrame) and \n",
    "                        len(batch_data['data']) > 0):\n",
    "                        \n",
    "                        # Procesar datos v√°lidos\n",
    "                        self._process_batch_data(\n",
    "                            batch_data, combined_data, language_analysis, topic_mappings\n",
    "                        )\n",
    "                        successful += 1\n",
    "                        \n",
    "                    else:\n",
    "                        self.logger.debug(f\"DataFrame vac√≠o en {batch_id}\")\n",
    "                        empty_skipped += 1\n",
    "                else:\n",
    "                    failed += 1\n",
    "        \n",
    "        # Resumen final\n",
    "        self.logger.info(f\"\\n=== RESULTADOS FINALES ===\")\n",
    "        self.logger.info(f\"Exitosos: {successful}\")\n",
    "        self.logger.info(f\"Vac√≠os omitidos: {empty_skipped}\")\n",
    "        self.logger.info(f\"Fallidos: {failed}\")\n",
    "        self.logger.info(f\"Total v√°lidos: {successful}/{len(result_files)} ({successful/len(result_files)*100:.1f}%)\")\n",
    "        \n",
    "        if combined_data:\n",
    "            self.logger.info(\"Creando DataFrame final...\")\n",
    "            final_df = pd.concat(combined_data, ignore_index=True)\n",
    "            \n",
    "            # Guardar resultado\n",
    "            output_path = self.output_dir / \"combined_multilingual_results_method3.pkl\"\n",
    "            final_df.to_pickle(output_path)\n",
    "            \n",
    "            # Guardar an√°lisis de idiomas\n",
    "            lang_file = self.output_dir / \"multilingual_language_analysis_method3.json\"\n",
    "            with open(lang_file, 'w') as f:\n",
    "                json.dump(language_analysis, f, indent=2, default=str)\n",
    "            \n",
    "            # Crear resumen completo\n",
    "            summary = self.create_comprehensive_summary(final_df, language_analysis)\n",
    "            \n",
    "            self.logger.info(f\"COMBINACI√ìN COMPLETADA: {len(final_df):,} documentos\")\n",
    "            self.logger.info(f\"Archivo guardado: {output_path}\")\n",
    "            \n",
    "            return final_df, {}, language_analysis, summary\n",
    "            \n",
    "        else:\n",
    "            self.logger.error(\"No se pudieron combinar datos\")\n",
    "            return None, None, None, None\n",
    "    \n",
    "    def _process_batch_data(self, batch_data, combined_data, language_analysis, topic_mappings):\n",
    "        \"\"\"Procesa datos de un lote v√°lido\"\"\"\n",
    "        \n",
    "        sentiment = batch_data.get('sentiment', 'unknown')\n",
    "        df = batch_data['data'].copy()\n",
    "        \n",
    "        # Inicializar estructuras para este sentimiento\n",
    "        if sentiment not in topic_mappings:\n",
    "            topic_mappings[sentiment] = {}\n",
    "            language_analysis[sentiment] = {}\n",
    "        \n",
    "        # Remapear temas para evitar conflictos\n",
    "        if 'topic' in df.columns:\n",
    "            n_existing_topics = len([t for t in topic_mappings[sentiment].values() if t != -1])\n",
    "            \n",
    "            for topic in df['topic'].unique():\n",
    "                if topic not in topic_mappings[sentiment]:\n",
    "                    if topic == -1:\n",
    "                        topic_mappings[sentiment][topic] = -1\n",
    "                    else:\n",
    "                        topic_mappings[sentiment][topic] = n_existing_topics\n",
    "                        n_existing_topics += 1\n",
    "            \n",
    "            # Aplicar mapeo global\n",
    "            df['global_topic'] = df['topic'].map(topic_mappings[sentiment])\n",
    "        \n",
    "        # Agregar metadatos del lote\n",
    "        df['batch_lang_group'] = batch_data.get('lang_group', 'unknown')\n",
    "        df['batch_id'] = batch_data.get('batch_id', 'unknown')\n",
    "        \n",
    "        combined_data.append(df)\n",
    "        \n",
    "        # Procesar an√°lisis de idiomas por tema\n",
    "        topic_lang = batch_data.get('topic_language_analysis', {})\n",
    "        for topic_id, langs in topic_lang.items():\n",
    "            mapped_topic = topic_mappings[sentiment].get(topic_id, topic_id)\n",
    "            \n",
    "            if mapped_topic not in language_analysis[sentiment]:\n",
    "                language_analysis[sentiment][mapped_topic] = {}\n",
    "            \n",
    "            for lang, count in langs.items():\n",
    "                if lang not in language_analysis[sentiment][mapped_topic]:\n",
    "                    language_analysis[sentiment][mapped_topic][lang] = 0\n",
    "                language_analysis[sentiment][mapped_topic][lang] += count\n",
    "    \n",
    "    def create_comprehensive_summary(self, df, language_analysis):\n",
    "        \"\"\"Crea resumen completo de los datos combinados\"\"\"\n",
    "        \n",
    "        summary = {\n",
    "            'processing_info': {\n",
    "                'total_documents': len(df),\n",
    "                'processing_date': pd.Timestamp.now().isoformat(),\n",
    "                'unique_batches': df['batch_id'].nunique() if 'batch_id' in df.columns else 0\n",
    "            },\n",
    "            'content_analysis': {\n",
    "                'unique_sentiments': df['sentimiento'].nunique() if 'sentimiento' in df.columns else 0,\n",
    "                'unique_languages': df['idioma'].nunique() if 'idioma' in df.columns else 0,\n",
    "                'unique_topics': df['global_topic'].nunique() if 'global_topic' in df.columns else 0,\n",
    "                'outliers_count': len(df[df['global_topic'] == -1]) if 'global_topic' in df.columns else 0\n",
    "            },\n",
    "            'distributions': {\n",
    "                'by_sentiment': df['sentimiento'].value_counts().to_dict() if 'sentimiento' in df.columns else {},\n",
    "                'by_language': df['idioma'].value_counts().to_dict() if 'idioma' in df.columns else {},\n",
    "                'by_batch_group': df['batch_lang_group'].value_counts().to_dict() if 'batch_lang_group' in df.columns else {}\n",
    "            },\n",
    "            'topics_analysis': {}\n",
    "        }\n",
    "        \n",
    "        # An√°lisis de temas por sentimiento\n",
    "        if 'sentimiento' in df.columns and 'global_topic' in df.columns:\n",
    "            for sentiment in df['sentimiento'].unique():\n",
    "                sent_data = df[df['sentimiento'] == sentiment]\n",
    "                n_topics = sent_data['global_topic'].nunique()\n",
    "                if -1 in sent_data['global_topic'].values:\n",
    "                    n_topics -= 1\n",
    "                \n",
    "                summary['topics_analysis'][sentiment] = {\n",
    "                    'total_docs': len(sent_data),\n",
    "                    'total_topics': n_topics,\n",
    "                    'outliers': len(sent_data[sent_data['global_topic'] == -1]),\n",
    "                    'avg_docs_per_topic': len(sent_data) / n_topics if n_topics > 0 else 0\n",
    "                }\n",
    "        \n",
    "        # Top idiomas y sentimientos\n",
    "        if 'idioma' in df.columns:\n",
    "            top_languages = df['idioma'].value_counts().head(10)\n",
    "            summary['top_languages'] = top_languages.to_dict()\n",
    "        \n",
    "        # Guardar resumen\n",
    "        summary_file = self.output_dir / \"comprehensive_summary_method3.json\"\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2, default=str)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "\n",
    "def optimized_combine_results(output_dir=\"E:/bertopic_800k_multilingue\"):\n",
    "    \"\"\"Funci√≥n optimizada que usa solo m√©todo 3\"\"\"\n",
    "    \n",
    "    print(\"=== COMBINACI√ìN OPTIMIZADA (M√âTODO 3 SOLAMENTE) ===\")\n",
    "    print(f\"Directorio: {output_dir}\")\n",
    "    \n",
    "    combiner = OptimizedMethod3Combiner(output_dir)\n",
    "    result = combiner.combine_results_method3_only()\n",
    "    \n",
    "    if result[0] is not None:\n",
    "        final_df, models, lang_analysis, summary = result\n",
    "        \n",
    "        print(f\"\\n‚úì √âXITO USANDO M√âTODO 3\")\n",
    "        print(f\"Documentos totales: {len(final_df):,}\")\n",
    "        print(f\"Lotes exitosos: {summary['processing_info'].get('unique_batches', 'N/A')}\")\n",
    "        \n",
    "        if 'sentimiento' in final_df.columns:\n",
    "            sentiments = list(final_df['sentimiento'].unique())\n",
    "            print(f\"Sentimientos: {sentiments}\")\n",
    "            \n",
    "        if 'idioma' in final_df.columns:\n",
    "            n_languages = final_df['idioma'].nunique()\n",
    "            top_lang = final_df['idioma'].value_counts().index[0]\n",
    "            print(f\"Idiomas: {n_languages} (principal: {top_lang})\")\n",
    "            \n",
    "        if 'global_topic' in final_df.columns:\n",
    "            n_topics = final_df['global_topic'].nunique()\n",
    "            if -1 in final_df['global_topic'].values:\n",
    "                n_topics -= 1\n",
    "            print(f\"Temas identificados: {n_topics}\")\n",
    "        \n",
    "        return final_df, models, lang_analysis, summary\n",
    "    else:\n",
    "        print(\"‚úó Fall√≥ la combinaci√≥n\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590ffc3",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de BERTopic\n",
    "- Define hiperpar√°metros/pipe (vectorizador, umap/hdbscan, idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770f8173-2d78-4b60-af34-32f4ef2a8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINACI√ìN OPTIMIZADA (M√âTODO 3 SOLAMENTE) ===\n",
      "Directorio: E:/bertopic_800k_multilingue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== COMBINACI√ìN M√âTODO 3 OPTIMIZADO ===\n",
      "INFO:__main__:Archivos encontrados: 216\n",
      "INFO:__main__:Progreso: 1/216 (0.5%)\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_000_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_001_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_002_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_003_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_004_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_005_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_006_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_007_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_008_results.pkl\n",
      "INFO:__main__:Progreso: 10/216 (4.6%)\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_009_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_010_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_011_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_012_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_013_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_014_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_015_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_016_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_017_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_018_results.pkl\n",
      "INFO:__main__:Progreso: 20/216 (9.3%)\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_019_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_020_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_021_results.pkl\n",
      "INFO:__main__:‚úì Cargado exitosamente: negativo_main_batch_022_results.pkl\n"
     ]
    }
   ],
   "source": [
    "final_df, models, lang_analysis, summary = optimized_combine_results(\"E:/bertopic_800k_multilingue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2b4d1",
   "metadata": {},
   "source": [
    "## üì¶ Dependencias\n",
    "- Importa pandas, numpy para el flujo de t√≥picos.\n",
    "- Resiliente a pickles con CUDA (fuerza CPU).\n",
    "- Baja memoria (lotes + parciales comprimidos).\n",
    "- Reanuda sin repetir trabajo (estado JSON-safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccdaf3cb-27b8-4fac-961c-c9c3bca76c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import io\n",
    "import gc\n",
    "\n",
    "class IncrementalMemoryManagedCombiner:\n",
    "    \"\"\"Combinador que procesa en lotes peque√±os y libera memoria constantemente\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir, batch_size=10, auto_save_every=5):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.status_file = self.output_dir / 'processing_status.json'\n",
    "        self.batch_size = batch_size  # Cu√°ntos archivos procesar antes de guardar\n",
    "        self.auto_save_every = auto_save_every  # Cada cu√°ntos lotes guardar progreso\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.load_processing_status()\n",
    "        \n",
    "        # Archivos de progreso incremental\n",
    "        self.progress_dir = self.output_dir / \"incremental_progress\"\n",
    "        self.progress_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Estado de combinaci√≥n\n",
    "        self.combination_state = self.load_combination_state()\n",
    "    def convert_to_json_safe(self, obj):\n",
    "        \"\"\"Convierte tipos numpy/pandas a tipos JSON-serializables\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        if isinstance(obj, dict):\n",
    "            return {str(k): self.convert_to_json_safe(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self.convert_to_json_safe(item) for item in obj]\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        else:\n",
    "            return obj\n",
    "    def load_processing_status(self):\n",
    "        if self.status_file.exists():\n",
    "            with open(self.status_file, 'r') as f:\n",
    "                self.status = json.load(f)\n",
    "        else:\n",
    "            self.status = {'completed_batches': []}\n",
    "    \n",
    "    def load_combination_state(self):\n",
    "        \"\"\"Carga el estado de la combinaci√≥n incremental\"\"\"\n",
    "        state_file = self.progress_dir / \"combination_state.json\"\n",
    "        \n",
    "        if state_file.exists():\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "            self.logger.info(f\"Recuperando desde lote procesado: {state.get('last_processed', 0)}\")\n",
    "            return state\n",
    "        else:\n",
    "            return {\n",
    "                'last_processed': 0,\n",
    "                'total_documents': 0,\n",
    "                'processed_files': [],\n",
    "                'topic_mappings': {},\n",
    "                'language_analysis': {},\n",
    "                'sentiment_counts': {},\n",
    "                'partial_results_files': []\n",
    "            }\n",
    "    \n",
    "    def save_combination_state(self):\n",
    "        \"\"\"Guarda el estado actual de la combinaci√≥n (versi√≥n JSON-safe)\"\"\"\n",
    "        state_file = self.progress_dir / \"combination_state.json\"\n",
    "        \n",
    "        # Convertir estado a JSON-safe\n",
    "        safe_state = self.convert_to_json_safe(self.combination_state)\n",
    "        \n",
    "        with open(state_file, 'w') as f:\n",
    "            json.dump(safe_state, f, indent=2)\n",
    "    \n",
    "    def load_with_cpu_unpickler(self, file_path):\n",
    "        \"\"\"M√©todo 3 optimizado con limpieza de memoria\"\"\"\n",
    "        \n",
    "        class MemoryEfficientUnpickler(pickle.Unpickler):\n",
    "            def find_class(self, module, name):\n",
    "                if 'cuda' in module.lower() or 'cuda' in name.lower():\n",
    "                    if name == 'FloatTensor':\n",
    "                        return torch.FloatTensor\n",
    "                    elif name == 'LongTensor':\n",
    "                        return torch.LongTensor\n",
    "                    elif name == 'device':\n",
    "                        return lambda x: torch.device('cpu')\n",
    "                    else:\n",
    "                        return super().find_class('torch', name)\n",
    "                \n",
    "                if module == 'torch.storage' and name == '_load_from_bytes':\n",
    "                    def load_tensor(b):\n",
    "                        tensor = torch.load(io.BytesIO(b), map_location='cpu')\n",
    "                        return tensor\n",
    "                    return load_tensor\n",
    "                \n",
    "                return super().find_class(module, name)\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                unpickler = MemoryEfficientUnpickler(f)\n",
    "                data = unpickler.load()\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error cargando {file_path.name}: {str(e)[:100]}\")\n",
    "            return None\n",
    "    \n",
    "    def process_batch_of_files(self, file_batch):\n",
    "        \"\"\"Procesa un lote peque√±o de archivos y extrae solo lo esencial\"\"\"\n",
    "        \n",
    "        batch_dataframes = []\n",
    "        \n",
    "        for file_path in file_batch:\n",
    "            batch_id = file_path.name.replace(\"_results.pkl\", \"\")\n",
    "            \n",
    "            if batch_id in self.status.get('completed_batches', []):\n",
    "                # Cargar archivo\n",
    "                self.logger.info(f\"Cargando {batch_id}...\")\n",
    "                batch_data = self.load_with_cpu_unpickler(file_path)\n",
    "                \n",
    "                if (batch_data is not None and \n",
    "                    isinstance(batch_data, dict) and \n",
    "                    'data' in batch_data and \n",
    "                    isinstance(batch_data['data'], pd.DataFrame) and \n",
    "                    len(batch_data['data']) > 0):\n",
    "                    \n",
    "                    # Extraer solo DataFrame esencial\n",
    "                    df = batch_data['data'].copy()\n",
    "                    sentiment = batch_data.get('sentiment', 'unknown')\n",
    "                    \n",
    "                    # Procesar mapeo de temas\n",
    "                    self._update_topic_mappings(df, sentiment)\n",
    "                    \n",
    "                    # Procesar an√°lisis de idiomas\n",
    "                    self._update_language_analysis(batch_data, sentiment)\n",
    "                    \n",
    "                    # Agregar metadatos m√≠nimos\n",
    "                    df['sentiment_processed'] = sentiment\n",
    "                    df['batch_id_processed'] = batch_id\n",
    "                    \n",
    "                    batch_dataframes.append(df)\n",
    "                    \n",
    "                    # Actualizar contadores\n",
    "                    self.combination_state['total_documents'] += len(df)\n",
    "                    if sentiment not in self.combination_state['sentiment_counts']:\n",
    "                        self.combination_state['sentiment_counts'][sentiment] = 0\n",
    "                    self.combination_state['sentiment_counts'][sentiment] += len(df)\n",
    "                    \n",
    "                    self.logger.info(f\"Procesado {batch_id}: {len(df)} docs\")\n",
    "                \n",
    "                # CR√çTICO: Liberar memoria del batch_data inmediatamente\n",
    "                del batch_data\n",
    "                gc.collect()\n",
    "        \n",
    "        return batch_dataframes\n",
    "    \n",
    "    def _update_topic_mappings(self, df, sentiment):\n",
    "        \"\"\"Actualiza mapeos de temas incrementalmente (versi√≥n JSON-safe)\"\"\"\n",
    "        \n",
    "        if sentiment not in self.combination_state['topic_mappings']:\n",
    "            self.combination_state['topic_mappings'][sentiment] = {}\n",
    "        \n",
    "        if 'topic' in df.columns:\n",
    "            mappings = self.combination_state['topic_mappings'][sentiment]\n",
    "            n_existing = len([t for t in mappings.values() if t != -1])\n",
    "            \n",
    "            for topic in df['topic'].unique():\n",
    "                # Convertir topic a int est√°ndar de Python\n",
    "                topic_key = int(topic) if hasattr(topic, 'dtype') else topic\n",
    "                \n",
    "                if str(topic_key) not in mappings:  # Usar string como clave\n",
    "                    if topic_key == -1:\n",
    "                        mappings[str(topic_key)] = -1\n",
    "                    else:\n",
    "                        mappings[str(topic_key)] = n_existing\n",
    "                        n_existing += 1\n",
    "            \n",
    "            # Aplicar mapeo (convirtiendo claves de vuelta a int para pandas)\n",
    "            topic_map = {int(k): v for k, v in mappings.items()}\n",
    "            df['global_topic'] = df['topic'].map(topic_map)\n",
    "    \n",
    "    def _update_language_analysis(self, batch_data, sentiment):\n",
    "        \"\"\"Actualiza an√°lisis de idiomas incrementalmente (versi√≥n JSON-safe)\"\"\"\n",
    "        \n",
    "        if sentiment not in self.combination_state['language_analysis']:\n",
    "            self.combination_state['language_analysis'][sentiment] = {}\n",
    "        \n",
    "        topic_lang = batch_data.get('topic_language_analysis', {})\n",
    "        sentiment_analysis = self.combination_state['language_analysis'][sentiment]\n",
    "        \n",
    "        for topic_id, langs in topic_lang.items():\n",
    "            # Convertir topic_id a string para JSON\n",
    "            topic_key = str(int(topic_id)) if hasattr(topic_id, 'dtype') else str(topic_id)\n",
    "            mapped_topic_id = self.combination_state['topic_mappings'][sentiment].get(topic_key, topic_id)\n",
    "            mapped_topic_key = str(mapped_topic_id)\n",
    "            \n",
    "            if mapped_topic_key not in sentiment_analysis:\n",
    "                sentiment_analysis[mapped_topic_key] = {}\n",
    "            \n",
    "            for lang, count in langs.items():\n",
    "                if lang not in sentiment_analysis[mapped_topic_key]:\n",
    "                    sentiment_analysis[mapped_topic_key][lang] = 0\n",
    "                # Convertir count a int est√°ndar\n",
    "                count_safe = int(count) if hasattr(count, 'dtype') else count\n",
    "                sentiment_analysis[mapped_topic_key][lang] += count_safe\n",
    "    \n",
    "    def save_partial_results(self, combined_df, part_number):\n",
    "        \"\"\"Guarda resultados parciales y libera memoria\"\"\"\n",
    "        \n",
    "        partial_file = self.progress_dir / f\"partial_results_{part_number:03d}.pkl\"\n",
    "        \n",
    "        # Guardar DataFrame parcial\n",
    "        combined_df.to_pickle(partial_file, compression='gzip')\n",
    "        \n",
    "        # Registrar archivo parcial\n",
    "        self.combination_state['partial_results_files'].append(str(partial_file))\n",
    "        \n",
    "        self.logger.info(f\"Guardado parcial {part_number}: {len(combined_df)} docs en {partial_file.name}\")\n",
    "        \n",
    "        # Liberar memoria\n",
    "        del combined_df\n",
    "        gc.collect()\n",
    "        \n",
    "        return partial_file\n",
    "    \n",
    "    def combine_incremental_with_memory_management(self):\n",
    "        \"\"\"Combinador principal con gesti√≥n de memoria\"\"\"\n",
    "        \n",
    "        self.logger.info(\"=== COMBINACI√ìN INCREMENTAL CON GESTI√ìN DE MEMORIA ===\")\n",
    "        \n",
    "        # Obtener archivos a procesar\n",
    "        all_result_files = list(self.output_dir.glob(\"*_results.pkl\"))\n",
    "        completed_files = [f for f in all_result_files \n",
    "                          if f.name.replace(\"_results.pkl\", \"\") in self.status.get('completed_batches', [])]\n",
    "        \n",
    "        self.logger.info(f\"Total archivos a procesar: {len(completed_files)}\")\n",
    "        \n",
    "        # Filtrar archivos ya procesados\n",
    "        processed_names = [Path(f).name for f in self.combination_state.get('processed_files', [])]\n",
    "        remaining_files = [f for f in completed_files if f.name not in processed_names]\n",
    "        \n",
    "        self.logger.info(f\"Archivos pendientes: {len(remaining_files)}\")\n",
    "        \n",
    "        if not remaining_files:\n",
    "            self.logger.info(\"Todos los archivos ya procesados, combinando resultados finales...\")\n",
    "            return self.combine_partial_results()\n",
    "        \n",
    "        # Procesar en lotes peque√±os\n",
    "        part_number = len(self.combination_state.get('partial_results_files', []))\n",
    "        \n",
    "        for i in range(0, len(remaining_files), self.batch_size):\n",
    "            batch_files = remaining_files[i:i+self.batch_size]\n",
    "            \n",
    "            self.logger.info(f\"Procesando lote {i//self.batch_size + 1}: archivos {i+1}-{min(i+self.batch_size, len(remaining_files))}\")\n",
    "            \n",
    "            # Procesar lote de archivos\n",
    "            batch_dataframes = self.process_batch_of_files(batch_files)\n",
    "            \n",
    "            if batch_dataframes:\n",
    "                # Combinar DataFrames del lote\n",
    "                combined_batch = pd.concat(batch_dataframes, ignore_index=True)\n",
    "                \n",
    "                # Guardar resultado parcial\n",
    "                part_number += 1\n",
    "                self.save_partial_results(combined_batch, part_number)\n",
    "                \n",
    "                # Actualizar estado\n",
    "                self.combination_state['last_processed'] = i + len(batch_files)\n",
    "                self.combination_state['processed_files'].extend([str(f) for f in batch_files])\n",
    "                \n",
    "                # Guardar estado cada auto_save_every lotes\n",
    "                if part_number % self.auto_save_every == 0:\n",
    "                    self.save_combination_state()\n",
    "                    self.logger.info(f\"Estado guardado - Progreso: {self.combination_state['last_processed']}/{len(remaining_files)}\")\n",
    "            \n",
    "            # Limpiar memoria despu√©s de cada lote\n",
    "            if 'batch_dataframes' in locals():\n",
    "                del batch_dataframes\n",
    "            if 'combined_batch' in locals():\n",
    "                del combined_batch\n",
    "            gc.collect()\n",
    "            \n",
    "            self.logger.info(f\"Memoria liberada despu√©s del lote {part_number}\")\n",
    "        \n",
    "        # Guardar estado final\n",
    "        self.save_combination_state()\n",
    "        \n",
    "        # Combinar todos los resultados parciales\n",
    "        self.logger.info(\"Combinando todos los resultados parciales...\")\n",
    "        return self.combine_partial_results()\n",
    "    \n",
    "    def combine_partial_results(self):\n",
    "        \"\"\"Combina todos los archivos parciales en resultado final\"\"\"\n",
    "        \n",
    "        partial_files = self.combination_state.get('partial_results_files', [])\n",
    "        \n",
    "        if not partial_files:\n",
    "            self.logger.error(\"No hay archivos parciales para combinar\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        self.logger.info(f\"Combinando {len(partial_files)} archivos parciales...\")\n",
    "        \n",
    "        # Combinar archivos parciales uno por uno\n",
    "        final_dataframes = []\n",
    "        \n",
    "        for partial_file in partial_files:\n",
    "            self.logger.info(f\"Cargando {Path(partial_file).name}...\")\n",
    "            \n",
    "            try:\n",
    "                partial_df = pd.read_pickle(partial_file, compression='gzip')\n",
    "                final_dataframes.append(partial_df)\n",
    "                \n",
    "                # Liberar inmediatamente\n",
    "                del partial_df\n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error cargando {partial_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if final_dataframes:\n",
    "            self.logger.info(\"Creando DataFrame final...\")\n",
    "            final_df = pd.concat(final_dataframes, ignore_index=True)\n",
    "            \n",
    "            # Guardar resultado final\n",
    "            output_path = self.output_dir / \"combined_multilingual_results_incremental.pkl\"\n",
    "            final_df.to_pickle(output_path, compression='gzip')\n",
    "            \n",
    "            # Crear resumen final\n",
    "            summary = {\n",
    "                'total_documents': len(final_df),\n",
    "                'total_partial_files': len(partial_files),\n",
    "                'sentiment_counts': self.combination_state['sentiment_counts'],\n",
    "                'topic_mappings_summary': {k: len(v) for k, v in self.combination_state['topic_mappings'].items()},\n",
    "                'processing_completed': True\n",
    "            }\n",
    "            \n",
    "            with open(self.output_dir / \"incremental_summary.json\", 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "            self.logger.info(f\"COMBINACI√ìN INCREMENTAL COMPLETADA: {len(final_df):,} documentos\")\n",
    "            \n",
    "            return final_df, {}, self.combination_state['language_analysis'], summary\n",
    "        \n",
    "        else:\n",
    "            self.logger.error(\"No se pudieron cargar archivos parciales\")\n",
    "            return None, None, None, None\n",
    "\n",
    "\n",
    "def incremental_combine_results(output_dir=\"E:/bertopic_800k_multilingue\", batch_size=8):\n",
    "    \"\"\"Funci√≥n principal para combinaci√≥n incremental\"\"\"\n",
    "    \n",
    "    print(f\"=== COMBINACI√ìN INCREMENTAL (LOTES DE {batch_size}) ===\")\n",
    "    print(f\"Directorio: {output_dir}\")\n",
    "    \n",
    "    combiner = IncrementalMemoryManagedCombiner(\n",
    "        output_dir=output_dir, \n",
    "        batch_size=batch_size,  # Procesar solo 8 archivos a la vez\n",
    "        auto_save_every=3       # Guardar estado cada 3 lotes\n",
    "    )\n",
    "    \n",
    "    result = combiner.combine_incremental_with_memory_management()\n",
    "    \n",
    "    if result[0] is not None:\n",
    "        final_df, models, lang_analysis, summary = result\n",
    "        \n",
    "        print(f\"\\nCOMBINACI√ìN INCREMENTAL EXITOSA\")\n",
    "        print(f\"Total documentos: {len(final_df):,}\")\n",
    "        print(f\"Archivos parciales procesados: {summary.get('total_partial_files', 0)}\")\n",
    "        print(f\"Distribuci√≥n por sentimiento: {summary.get('sentiment_counts', {})}\")\n",
    "        \n",
    "        return final_df, models, lang_analysis, summary\n",
    "    else:\n",
    "        print(\"Fall√≥ la combinaci√≥n incremental\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bdc90",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de BERTopic\n",
    "- Define hiperpar√°metros/pipe (vectorizador, umap/hdbscan, idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01249f2b-db9f-4caa-aa33-ec49c648a372",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:=== COMBINACI√ìN INCREMENTAL CON GESTI√ìN DE MEMORIA ===\n",
      "INFO:__main__:Total archivos a procesar: 216\n",
      "INFO:__main__:Archivos pendientes: 216\n",
      "INFO:__main__:Procesando lote 1: archivos 1-10\n",
      "INFO:__main__:Cargando negativo_main_batch_000...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMBINACI√ìN INCREMENTAL (LOTES DE 10) ===\n",
      "Directorio: E:/bertopic_800k_multilingue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Procesado negativo_main_batch_000: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_001...\n",
      "INFO:__main__:Procesado negativo_main_batch_001: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_002...\n",
      "INFO:__main__:Procesado negativo_main_batch_002: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_003...\n",
      "INFO:__main__:Procesado negativo_main_batch_003: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_004...\n",
      "INFO:__main__:Procesado negativo_main_batch_004: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_005...\n",
      "INFO:__main__:Procesado negativo_main_batch_005: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_006...\n",
      "INFO:__main__:Procesado negativo_main_batch_006: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_007...\n",
      "INFO:__main__:Procesado negativo_main_batch_007: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_008...\n",
      "INFO:__main__:Procesado negativo_main_batch_008: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_009...\n",
      "INFO:__main__:Procesado negativo_main_batch_009: 4000 docs\n",
      "INFO:__main__:Guardado parcial 1: 40000 docs en partial_results_001.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 1\n",
      "INFO:__main__:Procesando lote 2: archivos 11-20\n",
      "INFO:__main__:Cargando negativo_main_batch_010...\n",
      "INFO:__main__:Procesado negativo_main_batch_010: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_011...\n",
      "INFO:__main__:Procesado negativo_main_batch_011: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_012...\n",
      "INFO:__main__:Procesado negativo_main_batch_012: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_013...\n",
      "INFO:__main__:Procesado negativo_main_batch_013: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_014...\n",
      "INFO:__main__:Procesado negativo_main_batch_014: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_015...\n",
      "INFO:__main__:Procesado negativo_main_batch_015: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_016...\n",
      "INFO:__main__:Procesado negativo_main_batch_016: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_017...\n",
      "INFO:__main__:Procesado negativo_main_batch_017: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_018...\n",
      "INFO:__main__:Procesado negativo_main_batch_018: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_019...\n",
      "INFO:__main__:Procesado negativo_main_batch_019: 4000 docs\n",
      "INFO:__main__:Guardado parcial 2: 40000 docs en partial_results_002.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 2\n",
      "INFO:__main__:Procesando lote 3: archivos 21-30\n",
      "INFO:__main__:Cargando negativo_main_batch_020...\n",
      "INFO:__main__:Procesado negativo_main_batch_020: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_021...\n",
      "INFO:__main__:Procesado negativo_main_batch_021: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_022...\n",
      "INFO:__main__:Procesado negativo_main_batch_022: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_023...\n",
      "INFO:__main__:Procesado negativo_main_batch_023: 4000 docs\n",
      "INFO:__main__:Cargando negativo_main_batch_024...\n",
      "INFO:__main__:Procesado negativo_main_batch_024: 3584 docs\n",
      "INFO:__main__:Cargando negativo_others_batch_000...\n",
      "INFO:__main__:Procesado negativo_others_batch_000: 2082 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_000...\n",
      "INFO:__main__:Procesado neutro_main_batch_000: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_001...\n",
      "INFO:__main__:Procesado neutro_main_batch_001: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_002...\n",
      "INFO:__main__:Procesado neutro_main_batch_002: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_003...\n",
      "INFO:__main__:Procesado neutro_main_batch_003: 4000 docs\n",
      "INFO:__main__:Guardado parcial 3: 37666 docs en partial_results_003.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 30/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 3\n",
      "INFO:__main__:Procesando lote 4: archivos 31-40\n",
      "INFO:__main__:Cargando neutro_main_batch_004...\n",
      "INFO:__main__:Procesado neutro_main_batch_004: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_005...\n",
      "INFO:__main__:Procesado neutro_main_batch_005: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_006...\n",
      "INFO:__main__:Procesado neutro_main_batch_006: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_007...\n",
      "INFO:__main__:Procesado neutro_main_batch_007: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_008...\n",
      "INFO:__main__:Procesado neutro_main_batch_008: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_009...\n",
      "INFO:__main__:Procesado neutro_main_batch_009: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_010...\n",
      "INFO:__main__:Procesado neutro_main_batch_010: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_011...\n",
      "INFO:__main__:Procesado neutro_main_batch_011: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_012...\n",
      "INFO:__main__:Procesado neutro_main_batch_012: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_013...\n",
      "INFO:__main__:Procesado neutro_main_batch_013: 4000 docs\n",
      "INFO:__main__:Guardado parcial 4: 40000 docs en partial_results_004.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 4\n",
      "INFO:__main__:Procesando lote 5: archivos 41-50\n",
      "INFO:__main__:Cargando neutro_main_batch_014...\n",
      "INFO:__main__:Procesado neutro_main_batch_014: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_015...\n",
      "INFO:__main__:Procesado neutro_main_batch_015: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_016...\n",
      "INFO:__main__:Procesado neutro_main_batch_016: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_017...\n",
      "INFO:__main__:Procesado neutro_main_batch_017: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_018...\n",
      "INFO:__main__:Procesado neutro_main_batch_018: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_019...\n",
      "INFO:__main__:Procesado neutro_main_batch_019: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_020...\n",
      "INFO:__main__:Procesado neutro_main_batch_020: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_021...\n",
      "INFO:__main__:Procesado neutro_main_batch_021: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_022...\n",
      "INFO:__main__:Procesado neutro_main_batch_022: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_023...\n",
      "INFO:__main__:Procesado neutro_main_batch_023: 4000 docs\n",
      "INFO:__main__:Guardado parcial 5: 40000 docs en partial_results_005.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 5\n",
      "INFO:__main__:Procesando lote 6: archivos 51-60\n",
      "INFO:__main__:Cargando neutro_main_batch_024...\n",
      "INFO:__main__:Procesado neutro_main_batch_024: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_025...\n",
      "INFO:__main__:Procesado neutro_main_batch_025: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_026...\n",
      "INFO:__main__:Procesado neutro_main_batch_026: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_027...\n",
      "INFO:__main__:Procesado neutro_main_batch_027: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_028...\n",
      "INFO:__main__:Procesado neutro_main_batch_028: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_029...\n",
      "INFO:__main__:Procesado neutro_main_batch_029: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_030...\n",
      "INFO:__main__:Procesado neutro_main_batch_030: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_031...\n",
      "INFO:__main__:Procesado neutro_main_batch_031: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_032...\n",
      "INFO:__main__:Procesado neutro_main_batch_032: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_033...\n",
      "INFO:__main__:Procesado neutro_main_batch_033: 4000 docs\n",
      "INFO:__main__:Guardado parcial 6: 40000 docs en partial_results_006.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 60/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 6\n",
      "INFO:__main__:Procesando lote 7: archivos 61-70\n",
      "INFO:__main__:Cargando neutro_main_batch_034...\n",
      "INFO:__main__:Procesado neutro_main_batch_034: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_035...\n",
      "INFO:__main__:Procesado neutro_main_batch_035: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_036...\n",
      "INFO:__main__:Procesado neutro_main_batch_036: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_037...\n",
      "INFO:__main__:Procesado neutro_main_batch_037: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_038...\n",
      "INFO:__main__:Procesado neutro_main_batch_038: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_039...\n",
      "INFO:__main__:Procesado neutro_main_batch_039: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_040...\n",
      "INFO:__main__:Procesado neutro_main_batch_040: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_041...\n",
      "INFO:__main__:Procesado neutro_main_batch_041: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_042...\n",
      "INFO:__main__:Procesado neutro_main_batch_042: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_043...\n",
      "INFO:__main__:Procesado neutro_main_batch_043: 4000 docs\n",
      "INFO:__main__:Guardado parcial 7: 40000 docs en partial_results_007.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 7\n",
      "INFO:__main__:Procesando lote 8: archivos 71-80\n",
      "INFO:__main__:Cargando neutro_main_batch_044...\n",
      "INFO:__main__:Procesado neutro_main_batch_044: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_045...\n",
      "INFO:__main__:Procesado neutro_main_batch_045: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_046...\n",
      "INFO:__main__:Procesado neutro_main_batch_046: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_047...\n",
      "INFO:__main__:Procesado neutro_main_batch_047: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_048...\n",
      "INFO:__main__:Procesado neutro_main_batch_048: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_049...\n",
      "INFO:__main__:Procesado neutro_main_batch_049: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_050...\n",
      "INFO:__main__:Procesado neutro_main_batch_050: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_051...\n",
      "INFO:__main__:Procesado neutro_main_batch_051: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_052...\n",
      "INFO:__main__:Procesado neutro_main_batch_052: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_053...\n",
      "INFO:__main__:Procesado neutro_main_batch_053: 4000 docs\n",
      "INFO:__main__:Guardado parcial 8: 40000 docs en partial_results_008.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 8\n",
      "INFO:__main__:Procesando lote 9: archivos 81-90\n",
      "INFO:__main__:Cargando neutro_main_batch_054...\n",
      "INFO:__main__:Procesado neutro_main_batch_054: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_055...\n",
      "INFO:__main__:Procesado neutro_main_batch_055: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_056...\n",
      "INFO:__main__:Procesado neutro_main_batch_056: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_057...\n",
      "INFO:__main__:Procesado neutro_main_batch_057: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_058...\n",
      "INFO:__main__:Procesado neutro_main_batch_058: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_059...\n",
      "INFO:__main__:Procesado neutro_main_batch_059: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_060...\n",
      "INFO:__main__:Procesado neutro_main_batch_060: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_061...\n",
      "INFO:__main__:Procesado neutro_main_batch_061: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_062...\n",
      "INFO:__main__:Procesado neutro_main_batch_062: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_063...\n",
      "INFO:__main__:Procesado neutro_main_batch_063: 4000 docs\n",
      "INFO:__main__:Guardado parcial 9: 40000 docs en partial_results_009.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 90/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 9\n",
      "INFO:__main__:Procesando lote 10: archivos 91-100\n",
      "INFO:__main__:Cargando neutro_main_batch_064...\n",
      "INFO:__main__:Procesado neutro_main_batch_064: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_065...\n",
      "INFO:__main__:Procesado neutro_main_batch_065: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_066...\n",
      "INFO:__main__:Procesado neutro_main_batch_066: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_067...\n",
      "INFO:__main__:Procesado neutro_main_batch_067: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_068...\n",
      "INFO:__main__:Procesado neutro_main_batch_068: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_069...\n",
      "INFO:__main__:Procesado neutro_main_batch_069: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_070...\n",
      "INFO:__main__:Procesado neutro_main_batch_070: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_071...\n",
      "INFO:__main__:Procesado neutro_main_batch_071: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_072...\n",
      "INFO:__main__:Procesado neutro_main_batch_072: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_073...\n",
      "INFO:__main__:Procesado neutro_main_batch_073: 4000 docs\n",
      "INFO:__main__:Guardado parcial 10: 40000 docs en partial_results_010.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 10\n",
      "INFO:__main__:Procesando lote 11: archivos 101-110\n",
      "INFO:__main__:Cargando neutro_main_batch_074...\n",
      "INFO:__main__:Procesado neutro_main_batch_074: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_075...\n",
      "INFO:__main__:Procesado neutro_main_batch_075: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_076...\n",
      "INFO:__main__:Procesado neutro_main_batch_076: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_077...\n",
      "INFO:__main__:Procesado neutro_main_batch_077: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_078...\n",
      "INFO:__main__:Procesado neutro_main_batch_078: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_079...\n",
      "INFO:__main__:Procesado neutro_main_batch_079: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_080...\n",
      "INFO:__main__:Procesado neutro_main_batch_080: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_081...\n",
      "INFO:__main__:Procesado neutro_main_batch_081: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_082...\n",
      "INFO:__main__:Procesado neutro_main_batch_082: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_083...\n",
      "INFO:__main__:Procesado neutro_main_batch_083: 4000 docs\n",
      "INFO:__main__:Guardado parcial 11: 40000 docs en partial_results_011.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 11\n",
      "INFO:__main__:Procesando lote 12: archivos 111-120\n",
      "INFO:__main__:Cargando neutro_main_batch_084...\n",
      "INFO:__main__:Procesado neutro_main_batch_084: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_085...\n",
      "INFO:__main__:Procesado neutro_main_batch_085: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_086...\n",
      "INFO:__main__:Procesado neutro_main_batch_086: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_087...\n",
      "INFO:__main__:Procesado neutro_main_batch_087: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_088...\n",
      "INFO:__main__:Procesado neutro_main_batch_088: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_089...\n",
      "INFO:__main__:Procesado neutro_main_batch_089: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_090...\n",
      "INFO:__main__:Procesado neutro_main_batch_090: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_091...\n",
      "INFO:__main__:Procesado neutro_main_batch_091: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_092...\n",
      "INFO:__main__:Procesado neutro_main_batch_092: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_093...\n",
      "INFO:__main__:Procesado neutro_main_batch_093: 4000 docs\n",
      "INFO:__main__:Guardado parcial 12: 40000 docs en partial_results_012.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 120/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 12\n",
      "INFO:__main__:Procesando lote 13: archivos 121-130\n",
      "INFO:__main__:Cargando neutro_main_batch_094...\n",
      "INFO:__main__:Procesado neutro_main_batch_094: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_095...\n",
      "INFO:__main__:Procesado neutro_main_batch_095: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_096...\n",
      "INFO:__main__:Procesado neutro_main_batch_096: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_097...\n",
      "INFO:__main__:Procesado neutro_main_batch_097: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_098...\n",
      "INFO:__main__:Procesado neutro_main_batch_098: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_099...\n",
      "INFO:__main__:Procesado neutro_main_batch_099: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_100...\n",
      "INFO:__main__:Procesado neutro_main_batch_100: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_101...\n",
      "INFO:__main__:Procesado neutro_main_batch_101: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_102...\n",
      "INFO:__main__:Procesado neutro_main_batch_102: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_103...\n",
      "INFO:__main__:Procesado neutro_main_batch_103: 4000 docs\n",
      "INFO:__main__:Guardado parcial 13: 40000 docs en partial_results_013.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 13\n",
      "INFO:__main__:Procesando lote 14: archivos 131-140\n",
      "INFO:__main__:Cargando neutro_main_batch_104...\n",
      "INFO:__main__:Procesado neutro_main_batch_104: 4000 docs\n",
      "INFO:__main__:Cargando neutro_main_batch_105...\n",
      "INFO:__main__:Procesado neutro_main_batch_105: 1767 docs\n",
      "INFO:__main__:Cargando neutro_others_batch_000...\n",
      "INFO:__main__:Procesado neutro_others_batch_000: 2847 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_000...\n",
      "INFO:__main__:Procesado positivo_main_batch_000: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_001...\n",
      "INFO:__main__:Procesado positivo_main_batch_001: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_002...\n",
      "INFO:__main__:Procesado positivo_main_batch_002: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_003...\n",
      "INFO:__main__:Procesado positivo_main_batch_003: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_004...\n",
      "INFO:__main__:Procesado positivo_main_batch_004: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_005...\n",
      "INFO:__main__:Procesado positivo_main_batch_005: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_006...\n",
      "INFO:__main__:Procesado positivo_main_batch_006: 4000 docs\n",
      "INFO:__main__:Guardado parcial 14: 36614 docs en partial_results_014.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 14\n",
      "INFO:__main__:Procesando lote 15: archivos 141-150\n",
      "INFO:__main__:Cargando positivo_main_batch_007...\n",
      "INFO:__main__:Procesado positivo_main_batch_007: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_008...\n",
      "INFO:__main__:Procesado positivo_main_batch_008: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_009...\n",
      "INFO:__main__:Procesado positivo_main_batch_009: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_010...\n",
      "INFO:__main__:Procesado positivo_main_batch_010: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_011...\n",
      "INFO:__main__:Procesado positivo_main_batch_011: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_012...\n",
      "INFO:__main__:Procesado positivo_main_batch_012: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_013...\n",
      "INFO:__main__:Procesado positivo_main_batch_013: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_014...\n",
      "INFO:__main__:Procesado positivo_main_batch_014: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_015...\n",
      "INFO:__main__:Procesado positivo_main_batch_015: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_016...\n",
      "INFO:__main__:Procesado positivo_main_batch_016: 4000 docs\n",
      "INFO:__main__:Guardado parcial 15: 40000 docs en partial_results_015.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 150/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 15\n",
      "INFO:__main__:Procesando lote 16: archivos 151-160\n",
      "INFO:__main__:Cargando positivo_main_batch_017...\n",
      "INFO:__main__:Procesado positivo_main_batch_017: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_018...\n",
      "INFO:__main__:Procesado positivo_main_batch_018: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_019...\n",
      "INFO:__main__:Procesado positivo_main_batch_019: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_020...\n",
      "INFO:__main__:Procesado positivo_main_batch_020: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_021...\n",
      "INFO:__main__:Procesado positivo_main_batch_021: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_022...\n",
      "INFO:__main__:Procesado positivo_main_batch_022: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_023...\n",
      "INFO:__main__:Procesado positivo_main_batch_023: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_024...\n",
      "INFO:__main__:Procesado positivo_main_batch_024: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_025...\n",
      "INFO:__main__:Procesado positivo_main_batch_025: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_026...\n",
      "INFO:__main__:Procesado positivo_main_batch_026: 4000 docs\n",
      "INFO:__main__:Guardado parcial 16: 40000 docs en partial_results_016.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 16\n",
      "INFO:__main__:Procesando lote 17: archivos 161-170\n",
      "INFO:__main__:Cargando positivo_main_batch_027...\n",
      "INFO:__main__:Procesado positivo_main_batch_027: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_028...\n",
      "INFO:__main__:Procesado positivo_main_batch_028: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_029...\n",
      "INFO:__main__:Procesado positivo_main_batch_029: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_030...\n",
      "INFO:__main__:Procesado positivo_main_batch_030: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_031...\n",
      "INFO:__main__:Procesado positivo_main_batch_031: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_032...\n",
      "INFO:__main__:Procesado positivo_main_batch_032: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_033...\n",
      "INFO:__main__:Procesado positivo_main_batch_033: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_034...\n",
      "INFO:__main__:Procesado positivo_main_batch_034: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_035...\n",
      "INFO:__main__:Procesado positivo_main_batch_035: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_036...\n",
      "INFO:__main__:Procesado positivo_main_batch_036: 4000 docs\n",
      "INFO:__main__:Guardado parcial 17: 40000 docs en partial_results_017.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 17\n",
      "INFO:__main__:Procesando lote 18: archivos 171-180\n",
      "INFO:__main__:Cargando positivo_main_batch_037...\n",
      "INFO:__main__:Procesado positivo_main_batch_037: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_038...\n",
      "INFO:__main__:Procesado positivo_main_batch_038: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_039...\n",
      "INFO:__main__:Procesado positivo_main_batch_039: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_040...\n",
      "INFO:__main__:Procesado positivo_main_batch_040: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_041...\n",
      "INFO:__main__:Procesado positivo_main_batch_041: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_042...\n",
      "INFO:__main__:Procesado positivo_main_batch_042: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_043...\n",
      "INFO:__main__:Procesado positivo_main_batch_043: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_044...\n",
      "INFO:__main__:Procesado positivo_main_batch_044: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_045...\n",
      "INFO:__main__:Procesado positivo_main_batch_045: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_046...\n",
      "INFO:__main__:Procesado positivo_main_batch_046: 4000 docs\n",
      "INFO:__main__:Guardado parcial 18: 40000 docs en partial_results_018.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 180/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 18\n",
      "INFO:__main__:Procesando lote 19: archivos 181-190\n",
      "INFO:__main__:Cargando positivo_main_batch_047...\n",
      "INFO:__main__:Procesado positivo_main_batch_047: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_048...\n",
      "INFO:__main__:Procesado positivo_main_batch_048: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_049...\n",
      "INFO:__main__:Procesado positivo_main_batch_049: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_050...\n",
      "INFO:__main__:Procesado positivo_main_batch_050: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_051...\n",
      "INFO:__main__:Procesado positivo_main_batch_051: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_052...\n",
      "INFO:__main__:Procesado positivo_main_batch_052: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_053...\n",
      "INFO:__main__:Procesado positivo_main_batch_053: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_054...\n",
      "INFO:__main__:Procesado positivo_main_batch_054: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_055...\n",
      "INFO:__main__:Procesado positivo_main_batch_055: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_056...\n",
      "INFO:__main__:Procesado positivo_main_batch_056: 4000 docs\n",
      "INFO:__main__:Guardado parcial 19: 40000 docs en partial_results_019.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 19\n",
      "INFO:__main__:Procesando lote 20: archivos 191-200\n",
      "INFO:__main__:Cargando positivo_main_batch_057...\n",
      "INFO:__main__:Procesado positivo_main_batch_057: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_058...\n",
      "INFO:__main__:Procesado positivo_main_batch_058: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_059...\n",
      "INFO:__main__:Procesado positivo_main_batch_059: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_060...\n",
      "INFO:__main__:Procesado positivo_main_batch_060: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_061...\n",
      "INFO:__main__:Procesado positivo_main_batch_061: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_062...\n",
      "INFO:__main__:Procesado positivo_main_batch_062: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_063...\n",
      "INFO:__main__:Procesado positivo_main_batch_063: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_064...\n",
      "INFO:__main__:Procesado positivo_main_batch_064: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_065...\n",
      "INFO:__main__:Procesado positivo_main_batch_065: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_066...\n",
      "INFO:__main__:Procesado positivo_main_batch_066: 4000 docs\n",
      "INFO:__main__:Guardado parcial 20: 40000 docs en partial_results_020.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 20\n",
      "INFO:__main__:Procesando lote 21: archivos 201-210\n",
      "INFO:__main__:Cargando positivo_main_batch_067...\n",
      "INFO:__main__:Procesado positivo_main_batch_067: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_068...\n",
      "INFO:__main__:Procesado positivo_main_batch_068: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_069...\n",
      "INFO:__main__:Procesado positivo_main_batch_069: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_070...\n",
      "INFO:__main__:Procesado positivo_main_batch_070: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_071...\n",
      "INFO:__main__:Procesado positivo_main_batch_071: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_072...\n",
      "INFO:__main__:Procesado positivo_main_batch_072: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_073...\n",
      "INFO:__main__:Procesado positivo_main_batch_073: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_074...\n",
      "INFO:__main__:Procesado positivo_main_batch_074: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_075...\n",
      "INFO:__main__:Procesado positivo_main_batch_075: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_076...\n",
      "INFO:__main__:Procesado positivo_main_batch_076: 4000 docs\n",
      "INFO:__main__:Guardado parcial 21: 40000 docs en partial_results_021.pkl\n",
      "INFO:__main__:Estado guardado - Progreso: 210/216\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 21\n",
      "INFO:__main__:Procesando lote 22: archivos 211-216\n",
      "INFO:__main__:Cargando positivo_main_batch_077...\n",
      "INFO:__main__:Procesado positivo_main_batch_077: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_078...\n",
      "INFO:__main__:Procesado positivo_main_batch_078: 4000 docs\n",
      "INFO:__main__:Cargando positivo_main_batch_079...\n",
      "INFO:__main__:Procesado positivo_main_batch_079: 250 docs\n",
      "INFO:__main__:Cargando positivo_others_batch_000...\n",
      "INFO:__main__:Procesado positivo_others_batch_000: 4000 docs\n",
      "INFO:__main__:Cargando positivo_others_batch_001...\n",
      "INFO:__main__:Procesado positivo_others_batch_001: 4000 docs\n",
      "INFO:__main__:Cargando positivo_others_batch_002...\n",
      "INFO:__main__:Procesado positivo_others_batch_002: 2036 docs\n",
      "INFO:__main__:Guardado parcial 22: 18286 docs en partial_results_022.pkl\n",
      "INFO:__main__:Memoria liberada despu√©s del lote 22\n",
      "INFO:__main__:Combinando todos los resultados parciales...\n",
      "INFO:__main__:Combinando 22 archivos parciales...\n",
      "INFO:__main__:Cargando partial_results_001.pkl...\n",
      "INFO:__main__:Cargando partial_results_002.pkl...\n",
      "INFO:__main__:Cargando partial_results_003.pkl...\n",
      "INFO:__main__:Cargando partial_results_004.pkl...\n",
      "INFO:__main__:Cargando partial_results_005.pkl...\n",
      "INFO:__main__:Cargando partial_results_006.pkl...\n",
      "INFO:__main__:Cargando partial_results_007.pkl...\n",
      "INFO:__main__:Cargando partial_results_008.pkl...\n",
      "INFO:__main__:Cargando partial_results_009.pkl...\n",
      "INFO:__main__:Cargando partial_results_010.pkl...\n",
      "INFO:__main__:Cargando partial_results_011.pkl...\n",
      "INFO:__main__:Cargando partial_results_012.pkl...\n",
      "INFO:__main__:Cargando partial_results_013.pkl...\n",
      "INFO:__main__:Cargando partial_results_014.pkl...\n",
      "INFO:__main__:Cargando partial_results_015.pkl...\n",
      "INFO:__main__:Cargando partial_results_016.pkl...\n",
      "INFO:__main__:Cargando partial_results_017.pkl...\n",
      "INFO:__main__:Cargando partial_results_018.pkl...\n",
      "INFO:__main__:Cargando partial_results_019.pkl...\n",
      "INFO:__main__:Cargando partial_results_020.pkl...\n",
      "INFO:__main__:Cargando partial_results_021.pkl...\n",
      "INFO:__main__:Cargando partial_results_022.pkl...\n",
      "INFO:__main__:Creando DataFrame final...\n",
      "INFO:__main__:COMBINACI√ìN INCREMENTAL COMPLETADA: 852,566 documentos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMBINACI√ìN INCREMENTAL EXITOSA\n",
      "Total documentos: 852,566\n",
      "Archivos parciales procesados: 22\n",
      "Distribuci√≥n por sentimiento: {'negativo': 101666, 'neutro': 424614, 'positivo': 326286}\n"
     ]
    }
   ],
   "source": [
    "final_df, models, lang_analysis, summary = incremental_combine_results(\"E:/bertopic_800k_multilingue\", batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe82b59",
   "metadata": {},
   "source": [
    "- Ejecuci√≥n auxiliar dentro del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7211127e-f021-452c-b69f-4cbc6d6577ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['texto', 'ciudad', 'categoria', 'fecha', 'fuente', 'sentimiento',\n",
       "       'confianza', 'descripcion_sencilla', 'idioma', 'mes', 'a√±o', 'topic',\n",
       "       'batch_id', 'global_topic', 'sentiment_processed',\n",
       "       'batch_id_processed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57768b6",
   "metadata": {},
   "source": [
    "## üîç Visualizaci√≥n de Data Frame\n",
    "- Ejecuci√≥n auxiliar dentro del flujo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "26894d2c-9ff4-42f9-831c-21f6d272cb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texto</th>\n",
       "      <th>ciudad</th>\n",
       "      <th>categoria</th>\n",
       "      <th>fecha</th>\n",
       "      <th>fuente</th>\n",
       "      <th>sentimiento</th>\n",
       "      <th>confianza</th>\n",
       "      <th>descripcion_sencilla</th>\n",
       "      <th>idioma</th>\n",
       "      <th>mes</th>\n",
       "      <th>a√±o</th>\n",
       "      <th>topic</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>global_topic</th>\n",
       "      <th>sentiment_processed</th>\n",
       "      <th>batch_id_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nous sommes arriv√©s en retard et le monsieur √†...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Playa</td>\n",
       "      <td>2025-07-17</td>\n",
       "      <td>Booking</td>\n",
       "      <td>negativo</td>\n",
       "      <td>0.998544</td>\n",
       "      <td>D√≠a cielo nublado, c√°lido, sin lluvia</td>\n",
       "      <td>fr</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "      <td>0</td>\n",
       "      <td>negativo</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Read very carefully what you are booking. You ...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-11</td>\n",
       "      <td>Booking</td>\n",
       "      <td>negativo</td>\n",
       "      <td>0.999205</td>\n",
       "      <td>D√≠a con lluvia moderada, c√°lido</td>\n",
       "      <td>en</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "      <td>0</td>\n",
       "      <td>negativo</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We came during the last hour of admission so w...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-08</td>\n",
       "      <td>Booking</td>\n",
       "      <td>negativo</td>\n",
       "      <td>0.996447</td>\n",
       "      <td>D√≠a cielo con intervalos, c√°lido, sin lluvia</td>\n",
       "      <td>en</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "      <td>0</td>\n",
       "      <td>negativo</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we booked disabled access but needed proof of ...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-06</td>\n",
       "      <td>Booking</td>\n",
       "      <td>negativo</td>\n",
       "      <td>0.999162</td>\n",
       "      <td>D√≠a cielo con intervalos, c√°lido, sin lluvia</td>\n",
       "      <td>en</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "      <td>0</td>\n",
       "      <td>negativo</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Booking no nos dio el c√≥digo para descargar la...</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>Tour</td>\n",
       "      <td>2025-07-03</td>\n",
       "      <td>Booking</td>\n",
       "      <td>negativo</td>\n",
       "      <td>0.999408</td>\n",
       "      <td>D√≠a cielo con intervalos, c√°lido, sin lluvia</td>\n",
       "      <td>es</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>1</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "      <td>0</td>\n",
       "      <td>negativo</td>\n",
       "      <td>negativo_main_batch_000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texto     ciudad categoria  \\\n",
       "0  Nous sommes arriv√©s en retard et le monsieur √†...  Barcelona     Playa   \n",
       "1  Read very carefully what you are booking. You ...  Barcelona      Tour   \n",
       "2  We came during the last hour of admission so w...  Barcelona      Tour   \n",
       "3  we booked disabled access but needed proof of ...  Barcelona      Tour   \n",
       "4  Booking no nos dio el c√≥digo para descargar la...  Barcelona      Tour   \n",
       "\n",
       "       fecha   fuente sentimiento  confianza  \\\n",
       "0 2025-07-17  Booking    negativo   0.998544   \n",
       "1 2025-07-11  Booking    negativo   0.999205   \n",
       "2 2025-07-08  Booking    negativo   0.996447   \n",
       "3 2025-07-06  Booking    negativo   0.999162   \n",
       "4 2025-07-03  Booking    negativo   0.999408   \n",
       "\n",
       "                           descripcion_sencilla idioma  mes     a√±o  topic  \\\n",
       "0         D√≠a cielo nublado, c√°lido, sin lluvia     fr  7.0  2025.0      1   \n",
       "1               D√≠a con lluvia moderada, c√°lido     en  7.0  2025.0      1   \n",
       "2  D√≠a cielo con intervalos, c√°lido, sin lluvia     en  7.0  2025.0      1   \n",
       "3  D√≠a cielo con intervalos, c√°lido, sin lluvia     en  7.0  2025.0      1   \n",
       "4  D√≠a cielo con intervalos, c√°lido, sin lluvia     es  7.0  2025.0      1   \n",
       "\n",
       "                  batch_id  global_topic sentiment_processed  \\\n",
       "0  negativo_main_batch_000             0            negativo   \n",
       "1  negativo_main_batch_000             0            negativo   \n",
       "2  negativo_main_batch_000             0            negativo   \n",
       "3  negativo_main_batch_000             0            negativo   \n",
       "4  negativo_main_batch_000             0            negativo   \n",
       "\n",
       "        batch_id_processed  \n",
       "0  negativo_main_batch_000  \n",
       "1  negativo_main_batch_000  \n",
       "2  negativo_main_batch_000  \n",
       "3  negativo_main_batch_000  \n",
       "4  negativo_main_batch_000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aade359",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de BERTopic\n",
    "- Define hiperpar√°metros/pipe (vectorizador, umap/hdbscan, idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9f3d4fe-a35d-42f4-9908-1ee9e6808d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_multilingual_topics(final_df, lang_analysis, sentiment='positivo', top_n=10):\n",
    "    \"\"\"\n",
    "    üìà FUNCI√ìN DE AN√ÅLISIS: Analiza temas multiling√ºes espec√≠ficos\n",
    "    Adaptada para tu DataFrame con columnas espec√≠ficas\n",
    "    \"\"\"\n",
    "    print(f\"=== AN√ÅLISIS DE TEMAS MULTILING√úES ({sentiment.upper()}) ===\")\n",
    "    \n",
    "    # Filtrar por sentimiento usando la columna correcta\n",
    "    sent_data = final_df[final_df['sentimiento'] == sentiment]\n",
    "    \n",
    "    if len(sent_data) == 0:\n",
    "        print(f\"No hay datos para el sentimiento: {sentiment}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Total documentos para {sentiment}: {len(sent_data):,}\")\n",
    "    \n",
    "    # Top temas m√°s frecuentes\n",
    "    topic_counts = sent_data['global_topic'].value_counts().head(top_n)\n",
    "    print(f\"\\nTop {top_n} temas m√°s frecuentes:\")\n",
    "    \n",
    "    for i, (topic_id, count) in enumerate(topic_counts.items(), 1):\n",
    "        if topic_id != -1:  # Excluir outliers\n",
    "            pct = (count / len(sent_data)) * 100\n",
    "            print(f\"\\n{i}. Tema {topic_id}: {count:,} documentos ({pct:.1f}%)\")\n",
    "            \n",
    "            # Filtrar documentos de este tema\n",
    "            topic_docs = sent_data[sent_data['global_topic'] == topic_id]\n",
    "            \n",
    "            # An√°lisis de idiomas para este tema\n",
    "            lang_dist = topic_docs['idioma'].value_counts()\n",
    "            print(f\"   Idiomas: {dict(lang_dist.head(5))}\")\n",
    "            \n",
    "            # Diversidad ling√º√≠stica\n",
    "            diversity = len(lang_dist)\n",
    "            main_lang = lang_dist.index[0] if len(lang_dist) > 0 else 'unknown'\n",
    "            main_lang_pct = (lang_dist.iloc[0] / len(topic_docs)) * 100 if len(lang_dist) > 0 else 0\n",
    "            print(f\"   Diversidad: {diversity} idiomas, principal: {main_lang} ({main_lang_pct:.1f}%)\")\n",
    "            \n",
    "            # An√°lisis temporal si tienes fechas\n",
    "            if 'fecha' in topic_docs.columns:\n",
    "                years = topic_docs['a√±o'].value_counts().head(3)\n",
    "                print(f\"   A√±os principales: {dict(years)}\")\n",
    "            \n",
    "            # An√°lisis geogr√°fico si tienes ciudades\n",
    "            if 'ciudad' in topic_docs.columns:\n",
    "                cities = topic_docs['ciudad'].value_counts().head(3)\n",
    "                print(f\"   Ciudades principales: {dict(cities)}\")\n",
    "            \n",
    "            # An√°lisis de categor√≠as si est√° disponible\n",
    "            if 'categoria' in topic_docs.columns:\n",
    "                categories = topic_docs['categoria'].value_counts().head(3)\n",
    "                print(f\"   Categor√≠as: {dict(categories)}\")\n",
    "            \n",
    "            # Ejemplos de documentos\n",
    "            print(\"   Ejemplos de documentos:\")\n",
    "            sample_docs = topic_docs.sample(min(3, len(topic_docs)))\n",
    "            \n",
    "            for j, (_, doc) in enumerate(sample_docs.iterrows(), 1):\n",
    "                text_preview = doc['texto'][:100] + \"...\" if len(doc['texto']) > 100 else doc['texto']\n",
    "                ciudad_info = f\" - {doc['ciudad']}\" if pd.notna(doc.get('ciudad')) else \"\"\n",
    "                fecha_info = f\" ({doc['a√±o']})\" if pd.notna(doc.get('a√±o')) else \"\"\n",
    "                print(f\"      {j}. [{doc['idioma']}]{ciudad_info}{fecha_info}: {text_preview}\")\n",
    "\n",
    "def analyze_all_sentiments(final_df, lang_analysis, top_n=10):\n",
    "    \"\"\"Analiza todos los sentimientos disponibles\"\"\"\n",
    "    \n",
    "    print(\"=== AN√ÅLISIS COMPLETO DE TODOS LOS SENTIMIENTOS ===\")\n",
    "    \n",
    "    sentiments = final_df['sentimiento'].unique()\n",
    "    print(f\"Sentimientos encontrados: {list(sentiments)}\")\n",
    "    \n",
    "    for sentiment in sentiments:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        analyze_multilingual_topics(final_df, lang_analysis, sentiment=sentiment, top_n=top_n)\n",
    "\n",
    "def create_topic_summary_report(final_df, lang_analysis):\n",
    "    \"\"\"Crea un reporte resumen de todos los temas\"\"\"\n",
    "    \n",
    "    print(\"=== REPORTE RESUMEN DE TEMAS ===\")\n",
    "    \n",
    "    # Estad√≠sticas generales\n",
    "    total_docs = len(final_df)\n",
    "    total_topics = final_df['global_topic'].nunique() - (1 if -1 in final_df['global_topic'].values else 0)\n",
    "    outliers = len(final_df[final_df['global_topic'] == -1])\n",
    "    \n",
    "    print(f\"Total documentos: {total_docs:,}\")\n",
    "    print(f\"Total temas identificados: {total_topics}\")\n",
    "    print(f\"Documentos sin clasificar (outliers): {outliers:,} ({outliers/total_docs*100:.1f}%)\")\n",
    "    \n",
    "    # Por sentimiento\n",
    "    print(f\"\\nDistribuci√≥n por sentimiento:\")\n",
    "    sentiment_stats = final_df.groupby('sentimiento').agg({\n",
    "        'global_topic': lambda x: x.nunique() - (1 if -1 in x.values else 0),\n",
    "        'texto': 'count',\n",
    "        'idioma': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    sentiment_stats.columns = ['Temas', 'Documentos', 'Idiomas']\n",
    "    print(sentiment_stats)\n",
    "    \n",
    "    # Top idiomas globales\n",
    "    print(f\"\\nTop 10 idiomas:\")\n",
    "    lang_dist = final_df['idioma'].value_counts().head(10)\n",
    "    for lang, count in lang_dist.items():\n",
    "        pct = (count / total_docs) * 100\n",
    "        print(f\"  {lang}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # An√°lisis temporal\n",
    "    if 'a√±o' in final_df.columns:\n",
    "        print(f\"\\nDistribuci√≥n temporal:\")\n",
    "        year_dist = final_df['a√±o'].value_counts().sort_index().tail(5)\n",
    "        for year, count in year_dist.items():\n",
    "            pct = (count / total_docs) * 100\n",
    "            print(f\"  {year}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Top ciudades\n",
    "    if 'ciudad' in final_df.columns:\n",
    "        print(f\"\\nTop 10 ciudades:\")\n",
    "        city_dist = final_df['ciudad'].value_counts().head(10)\n",
    "        for city, count in city_dist.items():\n",
    "            pct = (count / total_docs) * 100\n",
    "            print(f\"  {city}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "def find_multilingual_topics(final_df, min_languages=3):\n",
    "    \"\"\"Encuentra temas que aparecen en m√∫ltiples idiomas\"\"\"\n",
    "    \n",
    "    print(f\"=== TEMAS MULTILING√úES (m√≠nimo {min_languages} idiomas) ===\")\n",
    "    \n",
    "    multilingual_topics = []\n",
    "    \n",
    "    for sentiment in final_df['sentimiento'].unique():\n",
    "        sent_data = final_df[final_df['sentimiento'] == sentiment]\n",
    "        \n",
    "        for topic_id in sent_data['global_topic'].unique():\n",
    "            if topic_id != -1:  # Excluir outliers\n",
    "                topic_data = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                languages = topic_data['idioma'].nunique()\n",
    "                \n",
    "                if languages >= min_languages:\n",
    "                    lang_dist = topic_data['idioma'].value_counts()\n",
    "                    \n",
    "                    multilingual_topics.append({\n",
    "                        'sentiment': sentiment,\n",
    "                        'topic_id': topic_id,\n",
    "                        'total_docs': len(topic_data),\n",
    "                        'num_languages': languages,\n",
    "                        'languages': dict(lang_dist),\n",
    "                        'dominant_language': lang_dist.index[0],\n",
    "                        'language_diversity': languages / len(lang_dist)\n",
    "                    })\n",
    "    \n",
    "    # Ordenar por diversidad ling√º√≠stica\n",
    "    multilingual_topics.sort(key=lambda x: x['num_languages'], reverse=True)\n",
    "    \n",
    "    print(f\"Encontrados {len(multilingual_topics)} temas multiling√ºes:\")\n",
    "    \n",
    "    for i, topic in enumerate(multilingual_topics[:10], 1):  # Top 10\n",
    "        print(f\"\\n{i}. Tema {topic['topic_id']} ({topic['sentiment']})\")\n",
    "        print(f\"   Documentos: {topic['total_docs']:,}\")\n",
    "        print(f\"   Idiomas: {topic['num_languages']}\")\n",
    "        print(f\"   Distribuci√≥n: {topic['languages']}\")\n",
    "        print(f\"   Idioma dominante: {topic['dominant_language']}\")\n",
    "    \n",
    "    return multilingual_topics\n",
    "\n",
    "def export_topic_analysis(final_df, lang_analysis, output_dir=\"E:/bertopic_800k_multilingue\"):\n",
    "    \"\"\"Exporta an√°lisis detallado a archivos\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    \n",
    "    # Crear resumen por sentimiento\n",
    "    sentiment_summary = {}\n",
    "    \n",
    "    for sentiment in final_df['sentimiento'].unique():\n",
    "        sent_data = final_df[final_df['sentimiento'] == sentiment]\n",
    "        \n",
    "        # Top 20 temas para este sentimiento\n",
    "        top_topics = sent_data['global_topic'].value_counts().head(20)\n",
    "        \n",
    "        topic_details = []\n",
    "        for topic_id, count in top_topics.items():\n",
    "            if topic_id != -1:\n",
    "                topic_docs = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                \n",
    "                topic_info = {\n",
    "                    'topic_id': int(topic_id),\n",
    "                    'document_count': int(count),\n",
    "                    'percentage': round((count / len(sent_data)) * 100, 2),\n",
    "                    'languages': topic_docs['idioma'].value_counts().to_dict(),\n",
    "                    'language_count': topic_docs['idioma'].nunique(),\n",
    "                    'dominant_language': topic_docs['idioma'].value_counts().index[0],\n",
    "                    'years': topic_docs['a√±o'].value_counts().to_dict() if 'a√±o' in topic_docs.columns else {},\n",
    "                    'cities': topic_docs['ciudad'].value_counts().head(5).to_dict() if 'ciudad' in topic_docs.columns else {},\n",
    "                    'categories': topic_docs['categoria'].value_counts().head(3).to_dict() if 'categoria' in topic_docs.columns else {},\n",
    "                    'sample_texts': topic_docs['texto'].head(3).tolist()\n",
    "                }\n",
    "                topic_details.append(topic_info)\n",
    "        \n",
    "        sentiment_summary[sentiment] = {\n",
    "            'total_documents': len(sent_data),\n",
    "            'total_topics': sent_data['global_topic'].nunique() - (1 if -1 in sent_data['global_topic'].values else 0),\n",
    "            'outliers': len(sent_data[sent_data['global_topic'] == -1]),\n",
    "            'unique_languages': sent_data['idioma'].nunique(),\n",
    "            'language_distribution': sent_data['idioma'].value_counts().to_dict(),\n",
    "            'top_topics': topic_details\n",
    "        }\n",
    "    \n",
    "    # Guardar an√°lisis\n",
    "    with open(output_path / \"detailed_topic_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(sentiment_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"An√°lisis detallado guardado en: {output_path / 'detailed_topic_analysis.json'}\")\n",
    "    \n",
    "    return sentiment_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9001fd",
   "metadata": {},
   "source": [
    "## Informe impreso\n",
    "- Generan tus funciones analyze_all_sentiments + analyze_multilingual_topics. Resume, por sentimiento (negativo / neutro / positivo), los Top temas (global_topic) y sus m√©tricas.\n",
    "- Dashboard textual que te dice, por sentimiento, cu√°les son los temas m√°s grandes, en qu√© idiomas aparecen, en qu√© a√±os/ciudades se concentran y ejemplos reales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2eb4f326-d5ba-4334-99e6-415234967ea1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AN√ÅLISIS COMPLETO DE TODOS LOS SENTIMIENTOS ===\n",
      "Sentimientos encontrados: ['negativo', 'neutro', 'positivo']\n",
      "\n",
      "============================================================\n",
      "=== AN√ÅLISIS DE TEMAS MULTILING√úES (NEGATIVO) ===\n",
      "Total documentos para negativo: 101,666\n",
      "\n",
      "Top 10 temas m√°s frecuentes:\n",
      "\n",
      "1. Tema 2: 24,381 documentos (24.0%)\n",
      "   Idiomas: {'es': 23681, 'en': 470, 'pt': 56, 'nl': 35, 'it': 19}\n",
      "   Diversidad: 26 idiomas, principal: es (97.1%)\n",
      "   A√±os principales: {2024.0: 11136, 2025.0: 7303, 2023.0: 5361}\n",
      "   Ciudades principales: {'Barcelona': 11818, 'Madrid': 5460, 'Malaga': 4584}\n",
      "   Categor√≠as: {'Tour': 10965, 'Desconocido': 4431, 'Vida nocturna': 3642}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Barcelona (2024.0): Esta entrada no tiene comentarios\n",
      "      2. [es] - Barcelona (2024.0): Esta entrada no tiene comentarios\n",
      "      3. [es] - Barcelona (2024.0): Esta entrada no tiene comentarios\n",
      "\n",
      "3. Tema 0: 5,785 documentos (5.7%)\n",
      "   Idiomas: {'en': 2483, 'es': 2237, 'fr': 404, 'de': 263, 'it': 245}\n",
      "   Diversidad: 30 idiomas, principal: en (42.9%)\n",
      "   A√±os principales: {2024.0: 2267, 2023.0: 1594, 2025.0: 1385}\n",
      "   Ciudades principales: {'Madrid': 2734, 'Barcelona': 1495, 'Malaga': 594}\n",
      "   Categor√≠as: {'Tour': 2941, 'Desconocido': 756, 'Playa': 740}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Malaga (2024.0): The road to the meeting point was temporarily closed so we couldn't make it and viator refused to re...\n",
      "      2. [es] - Madrid (2022.0): Tour Panoramico En Autobus Por Madrid Sabado dia 4, pago para pasar el dia. 2 adultos y un nino. El ...\n",
      "      3. [es] - Sevilla (2023.0): Esta entrada no tiene comentarios\n",
      "\n",
      "4. Tema 1: 2,430 documentos (2.4%)\n",
      "   Idiomas: {'es': 1943, 'en': 304, 'nl': 55, 'it': 13, 'cs': 11}\n",
      "   Diversidad: 30 idiomas, principal: es (80.0%)\n",
      "   A√±os principales: {2023.0: 709, 2025.0: 643, 2024.0: 556}\n",
      "   Ciudades principales: {'Barcelona': 667, 'Malaga': 563, 'Madrid': 525}\n",
      "   Categor√≠as: {'Tour': 918, 'Desconocido': 498, 'Playa': 320}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Barcelona (2025.0): Esta entrada no tiene comentarios\n",
      "      2. [es] - Tenerife (2025.0): Esta entrada no tiene comentarios\n",
      "      3. [es] - Barcelona: el fc barcelona no quiere cometer los mismos errores frente al atl√©tico de madrid estamos concentrad...\n",
      "\n",
      "5. Tema 3: 1,468 documentos (1.4%)\n",
      "   Idiomas: {'es': 894, 'en': 438, 'ca': 50, 'pt': 25, 'fr': 24}\n",
      "   Diversidad: 13 idiomas, principal: es (60.9%)\n",
      "   A√±os principales: {2024.0: 498, 2025.0: 486, 2023.0: 330}\n",
      "   Ciudades principales: {'Barcelona': 579, 'Madrid': 399, 'Sevilla': 133}\n",
      "   Categor√≠as: {'Tour': 506, 'Desconocido': 451, 'Museo': 147}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Barcelona (2024.0): Esta entrada no tiene comentarios\n",
      "      2. [es] - Barcelona (2023.0): Esta entrada no tiene comentarios\n",
      "      3. [es] - Madrid (2025.0): chus mateo habla en el larguero tras su despido los resultados justifican el trabajo y hemos ganado ...\n",
      "\n",
      "6. Tema 81: 1,215 documentos (1.2%)\n",
      "   Idiomas: {'es': 835, 'en': 296, 'nl': 29, 'fr': 9, 'pt': 9}\n",
      "   Diversidad: 16 idiomas, principal: es (68.7%)\n",
      "   A√±os principales: {2024.0: 419, 2025.0: 370, 2023.0: 191}\n",
      "   Ciudades principales: {'Barcelona': 390, 'Madrid': 264, 'Gran Canaria': 133}\n",
      "   Categor√≠as: {'Tour': 424, 'Desconocido': 183, 'Playa': 159}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Madrid (2021.0): september summary of juventus news from italy translated here is summary of juventus news from italy...\n",
      "      2. [en] - Valencia (2025.0): side trip to italy from barcelona is it realistic iberia changes ive booked round trip tickets for m...\n",
      "      3. [es] - Mallorca (2023.0): Esta entrada no tiene comentarios\n",
      "\n",
      "7. Tema 113: 1,136 documentos (1.1%)\n",
      "   Idiomas: {'es': 857, 'en': 163, 'fr': 22, 'it': 19, 'nl': 19}\n",
      "   Diversidad: 21 idiomas, principal: es (75.4%)\n",
      "   A√±os principales: {2025.0: 369, 2024.0: 330, 2023.0: 186}\n",
      "   Ciudades principales: {'Barcelona': 416, 'Madrid': 370, 'Sevilla': 76}\n",
      "   Categor√≠as: {'Tour': 341, 'Desconocido': 210, 'Playa': 142}\n",
      "   Ejemplos de documentos:\n",
      "      1. [it] - Madrid (2023.0): Si tratta di un cantiere poich√© lo stadio √® stato ricostruito e i lavori sono ancora in corso. Quind...\n",
      "      2. [es] - Barcelona (2023.0): Esta entrada no tiene comentarios\n",
      "      3. [it] - Madrid (2024.0): Stadio spettacolare, ma a causa dei lavori tour limitato, affascinante il museo con tutti i trofei, ...\n",
      "\n",
      "8. Tema 107: 1,008 documentos (1.0%)\n",
      "   Idiomas: {'es': 726, 'en': 213, 'nl': 16, 'de': 10, 'fr': 6}\n",
      "   Diversidad: 21 idiomas, principal: es (72.0%)\n",
      "   A√±os principales: {2024.0: 309, 2025.0: 304, 2023.0: 199}\n",
      "   Ciudades principales: {'Madrid': 365, 'Barcelona': 288, 'Sevilla': 83}\n",
      "   Categor√≠as: {'Tour': 284, 'Museo': 237, 'Desconocido': 140}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Gran Canaria (2025.0): botaravex opiniones es fiable o una estafa alguien m√°s ha notado que √∫ltimamente todo el mundo habla...\n",
      "      2. [es] - Madrid (2025.0): Esta entrada no tiene comentarios\n",
      "      3. [es] - Barcelona (2024.0): Esta entrada no tiene comentarios\n",
      "\n",
      "9. Tema 131: 926 documentos (0.9%)\n",
      "   Idiomas: {'es': 598, 'en': 232, 'fr': 28, 'nl': 17, 'it': 14}\n",
      "   Diversidad: 18 idiomas, principal: es (64.6%)\n",
      "   A√±os principales: {2025.0: 346, 2024.0: 262, 2023.0: 156}\n",
      "   Ciudades principales: {'Madrid': 361, 'Barcelona': 266, 'Sevilla': 136}\n",
      "   Categor√≠as: {'Tour': 238, 'Desconocido': 161, 'Museo': 144}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Madrid: por qu√© tienes que hacer siempre lo que ellos dicen diego le pregunt√© con la voz quebrada mientras √©...\n",
      "      2. [en] - Barcelona (2024.0): It‚Äôs unfortunate that you can‚Äôt change the tickets that you already purchased for a different day. E...\n",
      "      3. [es] - Madrid (2025.0): el bulo de vienen a trabajar con datos bueno se desmiente por parte de los progres que la inmigracio...\n",
      "\n",
      "10. Tema 103: 862 documentos (0.8%)\n",
      "   Idiomas: {'es': 695, 'en': 101, 'nl': 15, 'fr': 13, 'de': 9}\n",
      "   Diversidad: 17 idiomas, principal: es (80.6%)\n",
      "   A√±os principales: {2024.0: 337, 2025.0: 275, 2023.0: 108}\n",
      "   Ciudades principales: {'Barcelona': 287, 'Madrid': 252, 'Gran Canaria': 148}\n",
      "   Categor√≠as: {'Tour': 220, 'Playa': 186, 'Desconocido': 160}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Sevilla (2025.0): Esta entrada no tiene comentarios\n",
      "      2. [es] - Madrid (2025.0): Esta entrada no tiene comentarios\n",
      "      3. [pt] - Barcelona (2022.0): O passeio ao Park G√ºell foi excepcional, excelente guia. Mt bom A guia da sagrada fam√≠lia foi p√©ssim...\n",
      "\n",
      "============================================================\n",
      "=== AN√ÅLISIS DE TEMAS MULTILING√úES (NEUTRO) ===\n",
      "Total documentos para neutro: 424,614\n",
      "\n",
      "Top 10 temas m√°s frecuentes:\n",
      "\n",
      "2. Tema 35: 9,571 documentos (2.3%)\n",
      "   Idiomas: {'en': 7927, 'es': 897, 'fr': 289, 'ca': 183, 'it': 80}\n",
      "   Diversidad: 28 idiomas, principal: en (82.8%)\n",
      "   A√±os principales: {2024.0: 3020, 2023.0: 2919, 2025.0: 1696}\n",
      "   Ciudades principales: {'Barcelona': 1646, 'Mallorca': 1451, 'Madrid': 1441}\n",
      "   Categor√≠as: {'Tour': 3320, 'Ocio': 2751, 'Desconocido': 1360}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Mallorca (2024.0): excursions day trips in palma de mallorca great value for money and very well organized\n",
      "      2. [en] - Malaga (2023.0): transfers in malaga the best part of my trip to spain\n",
      "      3. [en] - Tenerife (2023.0): local experiences in tenerife the best part of my trip to spain\n",
      "\n",
      "3. Tema 19: 8,339 documentos (2.0%)\n",
      "   Idiomas: {'en': 7552, 'es': 386, 'fr': 112, 'ca': 105, 'it': 59}\n",
      "   Diversidad: 25 idiomas, principal: en (90.6%)\n",
      "   A√±os principales: {2024.0: 2731, 2023.0: 2642, 2025.0: 1638}\n",
      "   Ciudades principales: {'Barcelona': 1541, 'Valencia': 1267, 'Mallorca': 1252}\n",
      "   Categor√≠as: {'Tour': 3045, 'Ocio': 2642, 'Desconocido': 1286}\n",
      "   Ejemplos de documentos:\n",
      "      1. [id] - Gran Canaria: iglesia de san juan bautista arucas\n",
      "      2. [en] - Sevilla (2022.0): tickets and events in seville spectacular scenery and great staff\n",
      "      3. [en] - Madrid (2024.0): attractions guided tours in madrid great value for money and very well organized\n",
      "\n",
      "4. Tema 0: 7,405 documentos (1.7%)\n",
      "   Idiomas: {'en': 6406, 'es': 520, 'ca': 171, 'fr': 156, 'it': 52}\n",
      "   Diversidad: 25 idiomas, principal: en (86.5%)\n",
      "   A√±os principales: {2024.0: 2396, 2023.0: 2269, 2025.0: 1217}\n",
      "   Ciudades principales: {'Barcelona': 1524, 'Madrid': 1185, 'Sevilla': 1098}\n",
      "   Categor√≠as: {'Tour': 2798, 'Ocio': 2175, 'Desconocido': 1447}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Madrid (2023.0): transfers in madrid the best part of my trip to spain\n",
      "      2. [en] - Madrid (2022.0): excursions day trips in madrid the best part of my trip to spain\n",
      "      3. [en] - Sevilla (2024.0): attractions guided tours in seville great value for money and very well organized\n",
      "\n",
      "5. Tema 20: 6,744 documentos (1.6%)\n",
      "   Idiomas: {'en': 6127, 'es': 253, 'ca': 194, 'fr': 69, 'de': 33}\n",
      "   Diversidad: 18 idiomas, principal: en (90.9%)\n",
      "   A√±os principales: {2024.0: 2227, 2023.0: 2140, 2025.0: 1196}\n",
      "   Ciudades principales: {'Madrid': 1126, 'Mallorca': 1062, 'Valencia': 991}\n",
      "   Categor√≠as: {'Tour': 2981, 'Ocio': 1803, 'Desconocido': 1187}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Madrid (2025.0): tickets and events in madrid not what i expected but still enjoyable\n",
      "      2. [en] - Tenerife (2023.0): excursions day trips in tenerife the best part of my trip to spain\n",
      "      3. [en] - Valencia (2024.0): transfers in valencia too crowded but the views were stunning\n",
      "\n",
      "6. Tema 24: 6,396 documentos (1.5%)\n",
      "   Idiomas: {'en': 5692, 'es': 298, 'ca': 221, 'fr': 99, 'de': 27}\n",
      "   Diversidad: 17 idiomas, principal: en (89.0%)\n",
      "   A√±os principales: {2024.0: 2095, 2023.0: 2075, 2025.0: 1071}\n",
      "   Ciudades principales: {'Barcelona': 1037, 'Madrid': 943, 'Mallorca': 940}\n",
      "   Categor√≠as: {'Ocio': 2050, 'Tour': 1729, 'Desconocido': 1716}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Valencia (2022.0): transfers in valencia spectacular scenery and great staff\n",
      "      2. [en] - Mallorca (2024.0): local experiences in palma de mallorca the best part of my trip to spain\n",
      "      3. [en] - Madrid (2022.0): attractions guided tours in madrid great value for money and very well organized\n",
      "\n",
      "7. Tema 16: 6,160 documentos (1.5%)\n",
      "   Idiomas: {'en': 5619, 'es': 197, 'ca': 152, 'fr': 77, 'it': 39}\n",
      "   Diversidad: 20 idiomas, principal: en (91.2%)\n",
      "   A√±os principales: {2024.0: 2050, 2023.0: 1967, 2025.0: 1099}\n",
      "   Ciudades principales: {'Madrid': 956, 'Valencia': 931, 'Mallorca': 870}\n",
      "   Categor√≠as: {'Tour': 2142, 'Desconocido': 1757, 'Ocio': 1518}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Malaga (2022.0): tickets and events in malaga worth every penny highly recommend it\n",
      "      2. [en] - Madrid (2025.0): attractions guided tours in madrid great value for money and very well organized\n",
      "      3. [en] - Mallorca (2023.0): local experiences in palma de mallorca worth every penny highly recommend it\n",
      "\n",
      "8. Tema 46: 5,930 documentos (1.4%)\n",
      "   Idiomas: {'en': 5406, 'es': 222, 'ca': 173, 'fr': 42, 'de': 26}\n",
      "   Diversidad: 19 idiomas, principal: en (91.2%)\n",
      "   A√±os principales: {2024.0: 1961, 2023.0: 1874, 2025.0: 1011}\n",
      "   Ciudades principales: {'Madrid': 1047, 'Valencia': 984, 'Barcelona': 905}\n",
      "   Categor√≠as: {'Desconocido': 1855, 'Ocio': 1805, 'Tour': 1514}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Madrid (2024.0): attractions guided tours in madrid the best part of my trip to spain\n",
      "      2. [en] - Valencia (2023.0): tickets and events in valencia an unforgettable cultural experience\n",
      "      3. [en] - Mallorca (2025.0): tickets and events in palma de mallorca too crowded but the views were stunning\n",
      "\n",
      "9. Tema 21: 5,743 documentos (1.4%)\n",
      "   Idiomas: {'en': 5319, 'es': 215, 'ca': 105, 'fr': 34, 'de': 23}\n",
      "   Diversidad: 20 idiomas, principal: en (92.6%)\n",
      "   A√±os principales: {2024.0: 1902, 2023.0: 1884, 2025.0: 975}\n",
      "   Ciudades principales: {'Madrid': 983, 'Sevilla': 906, 'Valencia': 862}\n",
      "   Categor√≠as: {'Tour': 2082, 'Desconocido': 1788, 'Ocio': 1280}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Barcelona (2023.0): transfers in barcelona worth every penny highly recommend it\n",
      "      2. [es] - Madrid (2025.0): tener amistades cuando tienes problemas psiqui√°tricos tengo bipolaridad tipo y el d√≠a de hoy estoy c...\n",
      "      3. [en] - Valencia (2023.0): transfers in valencia absolutely loved this experience\n",
      "\n",
      "10. Tema 32: 5,565 documentos (1.3%)\n",
      "   Idiomas: {'en': 5153, 'ca': 187, 'es': 110, 'fr': 42, 'pt': 23}\n",
      "   Diversidad: 21 idiomas, principal: en (92.6%)\n",
      "   A√±os principales: {2023.0: 1874, 2024.0: 1790, 2025.0: 984}\n",
      "   Ciudades principales: {'Madrid': 888, 'Sevilla': 886, 'Valencia': 787}\n",
      "   Categor√≠as: {'Desconocido': 1927, 'Tour': 1906, 'Ocio': 1099}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Sevilla (2023.0): tickets and events in seville the guide was amazing and very informative\n",
      "      2. [en] - Mallorca (2024.0): tickets and events in palma de mallorca not what i expected but still enjoyable\n",
      "      3. [en] - Sevilla (2023.0): transfers in seville spectacular scenery and great staff\n",
      "\n",
      "============================================================\n",
      "=== AN√ÅLISIS DE TEMAS MULTILING√úES (POSITIVO) ===\n",
      "Total documentos para positivo: 326,286\n",
      "\n",
      "Top 10 temas m√°s frecuentes:\n",
      "\n",
      "1. Tema 8: 51,958 documentos (15.9%)\n",
      "   Idiomas: {'es': 26239, 'en': 17398, 'nl': 1306, 'pt': 1045, 'fr': 1043}\n",
      "   Diversidad: 43 idiomas, principal: es (50.5%)\n",
      "   A√±os principales: {2024.0: 16686, 2023.0: 12113, 2025.0: 9434}\n",
      "   Ciudades principales: {'Barcelona': 18834, 'Madrid': 15961, 'Sevilla': 6693}\n",
      "   Categor√≠as: {'Tour': 25552, 'Desconocido': 6430, 'Playa': 6311}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Barcelona (2024.0): Muy bonita experiencia, impresionante recorrido y la gu√≠a amable y muy recomendable\n",
      "      2. [en] - Barcelona (2024.0): The tour guides and driver were fantastic! So knowledgeable and friendly. It was an amazing experien...\n",
      "      3. [es] - Madrid: Prison Island Giant Indoor Escape Game Ticket de entrada El sitio espectacular y la atenci√≥n maravil...\n",
      "\n",
      "3. Tema 9: 25,684 documentos (7.9%)\n",
      "   Idiomas: {'es': 13856, 'en': 9523, 'fr': 919, 'it': 592, 'de': 438}\n",
      "   Diversidad: 32 idiomas, principal: es (53.9%)\n",
      "   A√±os principales: {2024.0: 8195, 2023.0: 6302, 2025.0: 3927}\n",
      "   Ciudades principales: {'Madrid': 8828, 'Barcelona': 8657, 'Tenerife': 3371}\n",
      "   Categor√≠as: {'Tour': 13018, 'Playa': 2127, 'Desconocido': 2034}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Barcelona (2019.0): Recorrido En Bicicleta Por El Arte En Graffiti De Las Calles De Barcelona We went on a street style ...\n",
      "      2. [en] - Madrid (2024.0): Had a great time exploring the three beautiful cities, their history, their architecture... Eduardo ...\n",
      "      3. [es] - Madrid (2023.0): Excursion Guiada De Medio Dia O De Dia Completo A Toledo Desde Madrid Todo fue perfecto, me encanto ...\n",
      "\n",
      "4. Tema 0: 17,668 documentos (5.4%)\n",
      "   Idiomas: {'es': 9873, 'en': 6522, 'fr': 478, 'it': 310, 'de': 237}\n",
      "   Diversidad: 28 idiomas, principal: es (55.9%)\n",
      "   A√±os principales: {2024.0: 5442, 2023.0: 4456, 2025.0: 2778}\n",
      "   Ciudades principales: {'Barcelona': 6010, 'Madrid': 5899, 'Tenerife': 1718}\n",
      "   Categor√≠as: {'Tour': 11130, 'Playa': 1744, 'Desconocido': 1434}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Malaga (2022.0): Fantastic museum, immaculate and well laid out. The work on display was stunning and the interactive...\n",
      "      2. [en] - Barcelona (2023.0): Excellent tour guide. Made the tour interesting. Took pride in his city and the cathedral. The cathe...\n",
      "      3. [es] - Madrid: Visita Guiada Experta al Palacio Real con Skip-the-Line Fuimos con alex, fue un gu√≠a maravilloso, si...\n",
      "\n",
      "5. Tema 1: 13,596 documentos (4.2%)\n",
      "   Idiomas: {'es': 7324, 'en': 5461, 'fr': 316, 'it': 170, 'de': 146}\n",
      "   Diversidad: 26 idiomas, principal: es (53.9%)\n",
      "   A√±os principales: {2024.0: 3685, 2023.0: 3319, 2025.0: 1690}\n",
      "   Ciudades principales: {'Barcelona': 5022, 'Madrid': 4318, 'Sevilla': 1331}\n",
      "   Categor√≠as: {'Tour': 7724, 'Playa': 1364, 'Desconocido': 1318}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Madrid: Espect√°culo Flamenco Local en El Cortijo Fue una actuaci√≥n hermosa y divina. Todos los int√©rpretes e...\n",
      "      2. [en] - Barcelona (2023.0): Ursula did an excellent job. She shared much info during our tour, very nice, friendly and professio...\n",
      "      3. [sl] - Mallorca (2024.0): Divno mesto i divno iskustvo\n",
      "\n",
      "6. Tema 25: 11,595 documentos (3.6%)\n",
      "   Idiomas: {'es': 6238, 'en': 4582, 'fr': 260, 'it': 225, 'de': 115}\n",
      "   Diversidad: 28 idiomas, principal: es (53.8%)\n",
      "   A√±os principales: {2024.0: 3295, 2023.0: 2848, 2025.0: 1570}\n",
      "   Ciudades principales: {'Barcelona': 4178, 'Madrid': 3754, 'Sevilla': 1469}\n",
      "   Categor√≠as: {'Tour': 6934, 'Playa': 1193, 'Desconocido': 939}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Madrid (2025.0): This was an excellent tour. Our guide was so kind and helpful. It was very hot but our stops are alw...\n",
      "      2. [es] - Barcelona (2022.0): Tossa De Mar: Tour En Kayak Y Snorkel Con Paella Desde Barcelona La mezcla entre los kayak y el snor...\n",
      "      3. [en] - Madrid (2023.0): The Tuk Tuk tour led by our guide Gustavo was excellent. It's a great way to see many of the sites o...\n",
      "\n",
      "7. Tema 7: 10,000 documentos (3.1%)\n",
      "   Idiomas: {'es': 5150, 'en': 4222, 'fr': 268, 'it': 112, 'de': 91}\n",
      "   Diversidad: 28 idiomas, principal: es (51.5%)\n",
      "   A√±os principales: {2024.0: 2829, 2023.0: 2325, 2025.0: 1264}\n",
      "   Ciudades principales: {'Barcelona': 3746, 'Madrid': 2869, 'Sevilla': 1203}\n",
      "   Categor√≠as: {'Tour': 6212, 'Playa': 1094, 'Hotel': 655}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Malaga (2024.0): Walking though the canyon and seeing the natural beauty of the area was amazing a really enjoyable d...\n",
      "      2. [es] - Madrid (2021.0): Madrid Ultimate Spanish Cuisine Food & Wine Tour !Termine con una gira en solitario con carolyn y fu...\n",
      "      3. [es] - Malaga: Desde M√°laga: Senderismo guiado en El Torcal de Antequera Un tour incre√≠ble. Justine es una gu√≠a fab...\n",
      "\n",
      "8. Tema 14: 8,926 documentos (2.7%)\n",
      "   Idiomas: {'es': 4875, 'en': 3538, 'fr': 181, 'it': 132, 'de': 95}\n",
      "   Diversidad: 19 idiomas, principal: es (54.6%)\n",
      "   A√±os principales: {2024.0: 2359, 2023.0: 2185, 2025.0: 1243}\n",
      "   Ciudades principales: {'Barcelona': 3342, 'Madrid': 2429, 'Sevilla': 1049}\n",
      "   Categor√≠as: {'Tour': 5466, 'Playa': 979, 'Hotel': 893}\n",
      "   Ejemplos de documentos:\n",
      "      1. [es] - Sevilla: Sevilla Alc√°zar, Catedral y Giralda Tour con tickets de entrada Excelente gu√≠a, muy ameno. Y muy con...\n",
      "      2. [es] - Tenerife (2022.0): Avistamiento De Cetaceos En Tenerife | Atlantic Eco Experience Una experiencia fantastica. La tripul...\n",
      "      3. [en] - Sevilla (2024.0): Our guide was very engaging and a great story teller. We really enjoyed the tour. Seville has a rich...\n",
      "\n",
      "9. Tema 12: 8,015 documentos (2.5%)\n",
      "   Idiomas: {'es': 4742, 'en': 2742, 'fr': 188, 'it': 159, 'de': 88}\n",
      "   Diversidad: 23 idiomas, principal: es (59.2%)\n",
      "   A√±os principales: {2024.0: 2242, 2023.0: 1935, 2025.0: 1177}\n",
      "   Ciudades principales: {'Barcelona': 2817, 'Madrid': 2217, 'Sevilla': 909}\n",
      "   Categor√≠as: {'Tour': 5093, 'Playa': 910, 'Desconocido': 629}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Barcelona (2022.0): The entire day was wonderful. The monastery is beautiful and the Black Madonna is breathtaking. Our ...\n",
      "      2. [es] - Mallorca: cuando hago lo que me apasiona disfruto el doble trabajar conpasi√≥n dar charlas compartir saberes y ...\n",
      "      3. [en] - Barcelona (2023.0): We were able to book the tickets as we were in the queue\n",
      "\n",
      "10. Tema 11: 7,255 documentos (2.2%)\n",
      "   Idiomas: {'es': 4039, 'en': 2721, 'fr': 185, 'de': 113, 'it': 110}\n",
      "   Diversidad: 13 idiomas, principal: es (55.7%)\n",
      "   A√±os principales: {2024.0: 2035, 2023.0: 1862, 2025.0: 1077}\n",
      "   Ciudades principales: {'Barcelona': 2445, 'Madrid': 2232, 'Sevilla': 612}\n",
      "   Categor√≠as: {'Tour': 4678, 'Playa': 810, 'Hotel': 438}\n",
      "   Ejemplos de documentos:\n",
      "      1. [en] - Barcelona (2023.0): Loved this bike tour! Got to see many different parts of Barcelona and learn about the history. Our ...\n",
      "      2. [fr] - Madrid (2024.0): Magnifique mus√©e tr√®s bien situ√©\n",
      "      3. [fr] - Madrid (2025.0): Tr√®s bonne exp√©rience qui permet de voir un maximum de la ville en peu de temps, interpr√®te sympathi...\n"
     ]
    }
   ],
   "source": [
    "analyze_all_sentiments(final_df, lang_analysis, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b66aa3",
   "metadata": {},
   "source": [
    "## Resumen: \n",
    "- Explica en bloques, c√≥mo qued√≥ el corpus tras combinar todos los pickles y remapear temas:\n",
    "- Fotograf√≠a global del dataset\n",
    "- Distribuci√≥n por sentimiento\n",
    "- Top 10 idiomas\n",
    "- Top 10 ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70230115-1618-4e7d-b557-c232334c28ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== REPORTE RESUMEN DE TEMAS ===\n",
      "Total documentos: 852,566\n",
      "Total temas identificados: 218\n",
      "Documentos sin clasificar (outliers): 89,023 (10.4%)\n",
      "\n",
      "Distribuci√≥n por sentimiento:\n",
      "             Temas  Documentos  Idiomas\n",
      "sentimiento                            \n",
      "negativo       218      101666       42\n",
      "neutro         151      424614       41\n",
      "positivo       129      326286       45\n",
      "\n",
      "Top 10 idiomas:\n",
      "  en: 518,428 (60.8%)\n",
      "  es: 276,150 (32.4%)\n",
      "  fr: 19,348 (2.3%)\n",
      "  ca: 12,601 (1.5%)\n",
      "  it: 6,988 (0.8%)\n",
      "  de: 5,539 (0.6%)\n",
      "  pt: 3,400 (0.4%)\n",
      "  nl: 2,973 (0.3%)\n",
      "  pl: 1,419 (0.2%)\n",
      "  ko: 580 (0.1%)\n",
      "\n",
      "Distribuci√≥n temporal:\n",
      "  2022.0: 95,822 (11.2%)\n",
      "  2023.0: 236,452 (27.7%)\n",
      "  2024.0: 275,482 (32.3%)\n",
      "  2025.0: 154,716 (18.1%)\n",
      "  2026.0: 114 (0.0%)\n",
      "\n",
      "Top 10 ciudades:\n",
      "  Barcelona: 208,192 (24.4%)\n",
      "  Madrid: 174,844 (20.5%)\n",
      "  Sevilla: 99,564 (11.7%)\n",
      "  Tenerife: 84,601 (9.9%)\n",
      "  Malaga: 83,350 (9.8%)\n",
      "  Gran Canaria: 72,647 (8.5%)\n",
      "  Valencia: 65,341 (7.7%)\n",
      "  Mallorca: 64,027 (7.5%)\n"
     ]
    }
   ],
   "source": [
    "create_topic_summary_report(final_df, lang_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411feb7",
   "metadata": {},
   "source": [
    "## TEMAS MULTILING√úES\n",
    "Es un ranking de topics que est√°n presentes en varios idiomas, con su tama√±o y mezcla ling√º√≠stica. Si quieres subir o bajar la exigencia, cambia el par√°metro min_languages (p. ej., find_multilingual_topics(final_df, min_languages=5)).\n",
    "\n",
    "- (m√≠nimo 3 idiomas)‚Äù: filtro aplicado (solo topics que aparecen en ‚â• 3 idiomas).\n",
    "- Idiomas: cu√°ntos idiomas distintos tiene ese topic.\n",
    "- Distribuci√≥n: conteo por idioma dentro del topic (diccionario idioma‚Üí#docs).\n",
    "- Idioma dominante: el idioma con m√°s documentos en ese topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aa2efc7-dc93-4f73-a8b6-16d778277958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEMAS MULTILING√úES (m√≠nimo 3 idiomas) ===\n",
      "Encontrados 408 temas multiling√ºes:\n",
      "\n",
      "1. Tema 8 (positivo)\n",
      "   Documentos: 51,958\n",
      "   Idiomas: 43\n",
      "   Distribuci√≥n: {'es': 26239, 'en': 17398, 'nl': 1306, 'pt': 1045, 'fr': 1043, 'pl': 745, 'it': 666, 'de': 519, 'ko': 434, 'ca': 380, 'ro': 208, 'hu': 206, 'ru': 189, 'sv': 167, 'da': 139, 'cs': 138, 'zh-tw': 121, 'no': 108, 'af': 95, 'tr': 83, 'el': 80, 'ja': 71, 'fi': 67, 'sk': 66, 'lt': 56, 'hr': 50, 'bg': 49, 'sl': 44, 'zh-cn': 41, 'uk': 35, 'et': 28, 'lv': 26, 'ar': 20, 'tl': 16, 'th': 15, 'id': 14, 'so': 13, 'mk': 9, 'sw': 8, 'cy': 7, 'he': 7, 'vi': 6, 'sq': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "2. Tema 9 (positivo)\n",
      "   Documentos: 25,684\n",
      "   Idiomas: 32\n",
      "   Distribuci√≥n: {'es': 13856, 'en': 9523, 'fr': 919, 'it': 592, 'de': 438, 'pt': 91, 'nl': 79, 'pl': 29, 'hu': 17, 'ru': 14, 'sv': 12, 'fi': 12, 'ro': 12, 'da': 10, 'el': 9, 'sl': 9, 'cs': 9, 'no': 9, 'ko': 8, 'ca': 6, 'af': 6, 'ja': 4, 'sk': 4, 'hr': 3, 'et': 3, 'zh-cn': 2, 'zh-tw': 2, 'tl': 2, 'bg': 1, 'ar': 1, 'lt': 1, 'uk': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "3. Tema 0 (negativo)\n",
      "   Documentos: 5,785\n",
      "   Idiomas: 30\n",
      "   Distribuci√≥n: {'en': 2483, 'es': 2237, 'fr': 404, 'de': 263, 'it': 245, 'pt': 46, 'nl': 26, 'pl': 13, 'ca': 9, 'ru': 8, 'he': 6, 'ar': 6, 'sv': 5, 'ro': 4, 'tr': 4, 'ko': 4, 'no': 3, 'cs': 2, 'tl': 2, 'da': 2, 'hr': 2, 'bg': 2, 'el': 2, 'af': 1, 'mk': 1, 'sk': 1, 'sl': 1, 'fi': 1, 'lv': 1, 'hu': 1}\n",
      "   Idioma dominante: en\n",
      "\n",
      "4. Tema 1 (negativo)\n",
      "   Documentos: 2,430\n",
      "   Idiomas: 30\n",
      "   Distribuci√≥n: {'es': 1943, 'en': 304, 'nl': 55, 'it': 13, 'cs': 11, 'de': 11, 'sv': 10, 'pl': 10, 'fr': 8, 'da': 8, 'ro': 7, 'ru': 7, 'pt': 7, 'no': 6, 'ca': 3, 'hu': 3, 'fi': 3, 'sk': 3, 'lt': 2, 'ko': 2, 'zh-tw': 2, 'tr': 2, 'sl': 2, 'zh-cn': 2, 'he': 1, 'vi': 1, 'lv': 1, 'af': 1, 'el': 1, 'ja': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "5. Tema 35 (neutro)\n",
      "   Documentos: 9,571\n",
      "   Idiomas: 28\n",
      "   Distribuci√≥n: {'en': 7927, 'es': 897, 'fr': 289, 'ca': 183, 'it': 80, 'de': 54, 'pt': 40, 'nl': 26, 'fi': 12, 'ko': 11, 'pl': 11, 'sv': 6, 'el': 4, 'ru': 4, 'hu': 3, 'cs': 3, 'sk': 3, 'ro': 3, 'ja': 3, 'ar': 2, 'zh-tw': 2, 'sl': 2, 'tr': 1, 'da': 1, 'lt': 1, 'zh-cn': 1, 'th': 1, 'hr': 1}\n",
      "   Idioma dominante: en\n",
      "\n",
      "6. Tema 0 (positivo)\n",
      "   Documentos: 17,668\n",
      "   Idiomas: 28\n",
      "   Distribuci√≥n: {'es': 9873, 'en': 6522, 'fr': 478, 'it': 310, 'de': 237, 'pt': 70, 'nl': 51, 'pl': 32, 'ko': 15, 'cs': 13, 'zh-tw': 8, 'ro': 7, 'hu': 6, 'no': 5, 'sk': 5, 'ru': 4, 'fi': 4, 'da': 3, 'uk': 3, 'ca': 3, 'af': 3, 'sv': 3, 'sl': 3, 'zh-cn': 3, 'et': 2, 'el': 2, 'ja': 2, 'lt': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "7. Tema 7 (positivo)\n",
      "   Documentos: 10,000\n",
      "   Idiomas: 28\n",
      "   Distribuci√≥n: {'es': 5150, 'en': 4222, 'fr': 268, 'it': 112, 'de': 91, 'pt': 67, 'nl': 22, 'pl': 21, 'ru': 6, 'hu': 5, 'fi': 5, 'ca': 5, 'he': 3, 'ar': 3, 'ja': 2, 'no': 2, 'lv': 2, 'ko': 2, 'bg': 2, 'el': 2, 'da': 1, 'et': 1, 'cs': 1, 'lt': 1, 'ro': 1, 'sk': 1, 'sv': 1, 'zh-cn': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "8. Tema 25 (positivo)\n",
      "   Documentos: 11,595\n",
      "   Idiomas: 28\n",
      "   Distribuci√≥n: {'es': 6238, 'en': 4582, 'fr': 260, 'it': 225, 'de': 115, 'pt': 47, 'nl': 43, 'pl': 19, 'ro': 10, 'el': 8, 'hu': 5, 'no': 4, 'fi': 4, 'da': 4, 'cs': 4, 'ja': 4, 'tr': 3, 'ca': 3, 'hr': 3, 'ar': 2, 'ko': 2, 'ru': 2, 'sv': 2, 'lt': 2, 'lv': 1, 'bg': 1, 'uk': 1, 'sl': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "9. Tema 2 (negativo)\n",
      "   Documentos: 24,381\n",
      "   Idiomas: 26\n",
      "   Distribuci√≥n: {'es': 23681, 'en': 470, 'pt': 56, 'nl': 35, 'it': 19, 'fr': 18, 'de': 18, 'ko': 11, 'pl': 11, 'tr': 8, 'cs': 7, 'sv': 6, 'ru': 6, 'zh-cn': 5, 'ro': 5, 'hu': 5, 'ja': 4, 'he': 3, 'el': 3, 'hr': 2, 'fi': 2, 'bg': 2, 'da': 1, 'lv': 1, 'ca': 1, 'no': 1}\n",
      "   Idioma dominante: es\n",
      "\n",
      "10. Tema 1 (positivo)\n",
      "   Documentos: 13,596\n",
      "   Idiomas: 26\n",
      "   Distribuci√≥n: {'es': 7324, 'en': 5461, 'fr': 316, 'it': 170, 'de': 146, 'pt': 40, 'nl': 36, 'pl': 25, 'hu': 13, 'ar': 8, 'ro': 7, 'fi': 7, 'cs': 7, 'da': 5, 'ru': 5, 'af': 4, 'ca': 4, 'sv': 3, 'el': 3, 'hr': 3, 'no': 2, 'lt': 2, 'et': 2, 'sl': 1, 'sk': 1, 'tr': 1}\n",
      "   Idioma dominante: es\n"
     ]
    }
   ],
   "source": [
    "multilingual_topics = find_multilingual_topics(final_df, min_languages=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40ee950",
   "metadata": {},
   "source": [
    "## üì¶ Dependencias\n",
    "- Importa pandas, numpy para el flujo de t√≥picos.\n",
    "- Generador de datasets para un dashboard en Streamlit a partir de tu final_df (datos combinados) y lang_analysis. Crea archivos JSON/CSV/PKL listos para cargar en Streamlit (m√°s un config.json con qu√© hay disponible).\n",
    "- Produce (carpeta: E:/bertopic_800k_multilingue/streamlit_data)\n",
    "- Temporal\n",
    "- Geogr√°fico\n",
    "- Multiling√ºe\n",
    "- Clima si existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f3eefca-9792-4c20-99ae-c1cab3c3eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "class StreamlitDataGenerator:\n",
    "    \"\"\"Genera datos estructurados y optimizados para Streamlit\"\"\"\n",
    "    \n",
    "    def __init__(self, final_df, lang_analysis, output_dir=\"E:/bertopic_800k_multilingue\"):\n",
    "        self.final_df = final_df\n",
    "        self.lang_analysis = lang_analysis\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.streamlit_dir = self.output_dir / \"streamlit_data\"\n",
    "        self.streamlit_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        print(f\"Preparando datos para Streamlit en: {self.streamlit_dir}\")\n",
    "    \n",
    "    def generate_overview_metrics(self):\n",
    "        \"\"\"Genera m√©tricas generales para dashboard\"\"\"\n",
    "        \n",
    "        metrics = {\n",
    "            'total_documents': len(self.final_df),\n",
    "            'total_topics': self.final_df['global_topic'].nunique() - (1 if -1 in self.final_df['global_topic'].values else 0),\n",
    "            'total_languages': self.final_df['idioma'].nunique(),\n",
    "            'total_cities': self.final_df['ciudad'].nunique() if 'ciudad' in self.final_df.columns else 0,\n",
    "            'total_categories': self.final_df['categoria'].nunique() if 'categoria' in self.final_df.columns else 0,\n",
    "            'date_range': {\n",
    "                'start': str(self.final_df['fecha'].min()) if 'fecha' in self.final_df.columns else None,\n",
    "                'end': str(self.final_df['fecha'].max()) if 'fecha' in self.final_df.columns else None\n",
    "            },\n",
    "            'outliers_count': len(self.final_df[self.final_df['global_topic'] == -1]),\n",
    "            'outliers_percentage': round(len(self.final_df[self.final_df['global_topic'] == -1]) / len(self.final_df) * 100, 2)\n",
    "        }\n",
    "        \n",
    "        # Guardar m√©tricas\n",
    "        with open(self.streamlit_dir / \"overview_metrics.json\", 'w') as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"M√©tricas generales guardadas: {len(self.final_df):,} documentos, {metrics['total_topics']} temas\")\n",
    "        return metrics\n",
    "    \n",
    "    def generate_sentiment_analysis(self):\n",
    "        \"\"\"Genera an√°lisis detallado por sentimiento\"\"\"\n",
    "        \n",
    "        sentiment_data = {}\n",
    "        \n",
    "        for sentiment in self.final_df['sentimiento'].unique():\n",
    "            sent_data = self.final_df[self.final_df['sentimiento'] == sentiment]\n",
    "            \n",
    "            # An√°lisis de temas para este sentimiento\n",
    "            topic_analysis = []\n",
    "            topic_counts = sent_data['global_topic'].value_counts()\n",
    "            \n",
    "            for topic_id, count in topic_counts.items():\n",
    "                if topic_id != -1:  # Excluir outliers\n",
    "                    topic_docs = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                    \n",
    "                    # An√°lisis de idiomas\n",
    "                    lang_dist = topic_docs['idioma'].value_counts()\n",
    "                    \n",
    "                    # An√°lisis temporal\n",
    "                    temporal_dist = {}\n",
    "                    if 'a√±o' in topic_docs.columns:\n",
    "                        temporal_dist = topic_docs['a√±o'].value_counts().to_dict()\n",
    "                    \n",
    "                    # An√°lisis geogr√°fico\n",
    "                    geo_dist = {}\n",
    "                    if 'ciudad' in topic_docs.columns:\n",
    "                        geo_dist = topic_docs['ciudad'].value_counts().head(10).to_dict()\n",
    "                    \n",
    "                    # An√°lisis categ√≥rico\n",
    "                    cat_dist = {}\n",
    "                    if 'categoria' in topic_docs.columns:\n",
    "                        cat_dist = topic_docs['categoria'].value_counts().head(5).to_dict()\n",
    "                    \n",
    "                    # Ejemplos representativos\n",
    "                    samples = []\n",
    "                    sample_docs = topic_docs.sample(min(5, len(topic_docs)))\n",
    "                    \n",
    "                    for _, doc in sample_docs.iterrows():\n",
    "                        samples.append({\n",
    "                            'texto': doc['texto'][:200] + \"...\" if len(doc['texto']) > 200 else doc['texto'],\n",
    "                            'idioma': doc['idioma'],\n",
    "                            'ciudad': doc.get('ciudad', 'N/A'),\n",
    "                            'categoria': doc.get('categoria', 'N/A'),\n",
    "                            'fecha': str(doc.get('fecha', 'N/A')),\n",
    "                            'confianza': doc.get('confianza', 'N/A')\n",
    "                        })\n",
    "                    \n",
    "                    topic_analysis.append({\n",
    "                        'topic_id': int(topic_id),\n",
    "                        'document_count': int(count),\n",
    "                        'percentage': round((count / len(sent_data)) * 100, 2),\n",
    "                        'language_diversity': lang_dist.nunique(),\n",
    "                        'dominant_language': lang_dist.index[0] if len(lang_dist) > 0 else 'unknown',\n",
    "                        'language_distribution': lang_dist.to_dict(),\n",
    "                        'temporal_distribution': temporal_dist,\n",
    "                        'geographic_distribution': geo_dist,\n",
    "                        'category_distribution': cat_dist,\n",
    "                        'sample_documents': samples,\n",
    "                        'multilingual': lang_dist.nunique() >= 3\n",
    "                    })\n",
    "            \n",
    "            # Ordenar por n√∫mero de documentos\n",
    "            topic_analysis.sort(key=lambda x: x['document_count'], reverse=True)\n",
    "            \n",
    "            sentiment_data[sentiment] = {\n",
    "                'total_documents': len(sent_data),\n",
    "                'total_topics': len([t for t in topic_analysis]),\n",
    "                'outliers': len(sent_data[sent_data['global_topic'] == -1]),\n",
    "                'language_summary': sent_data['idioma'].value_counts().to_dict(),\n",
    "                'temporal_summary': sent_data['a√±o'].value_counts().to_dict() if 'a√±o' in sent_data.columns else {},\n",
    "                'geographic_summary': sent_data['ciudad'].value_counts().head(20).to_dict() if 'ciudad' in sent_data.columns else {},\n",
    "                'category_summary': sent_data['categoria'].value_counts().to_dict() if 'categoria' in sent_data.columns else {},\n",
    "                'topics': topic_analysis\n",
    "            }\n",
    "        \n",
    "        # Guardar an√°lisis por sentimiento\n",
    "        with open(self.streamlit_dir / \"sentiment_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(sentiment_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"An√°lisis por sentimiento guardado: {len(sentiment_data)} sentimientos\")\n",
    "        return sentiment_data\n",
    "    \n",
    "    def generate_language_analysis(self):\n",
    "        \"\"\"Genera an√°lisis detallado por idioma\"\"\"\n",
    "        \n",
    "        language_data = {}\n",
    "        \n",
    "        for language in self.final_df['idioma'].unique():\n",
    "            lang_data = self.final_df[self.final_df['idioma'] == language]\n",
    "            \n",
    "            # An√°lisis por sentimiento para este idioma\n",
    "            sentiment_breakdown = {}\n",
    "            for sentiment in lang_data['sentimiento'].unique():\n",
    "                sent_lang_data = lang_data[lang_data['sentimiento'] == sentiment]\n",
    "                \n",
    "                # Top temas para este idioma-sentimiento\n",
    "                top_topics = sent_lang_data['global_topic'].value_counts().head(10)\n",
    "                topic_list = []\n",
    "                \n",
    "                for topic_id, count in top_topics.items():\n",
    "                    if topic_id != -1:\n",
    "                        topic_list.append({\n",
    "                            'topic_id': int(topic_id),\n",
    "                            'document_count': int(count),\n",
    "                            'percentage': round((count / len(sent_lang_data)) * 100, 2)\n",
    "                        })\n",
    "                \n",
    "                sentiment_breakdown[sentiment] = {\n",
    "                    'document_count': len(sent_lang_data),\n",
    "                    'topic_count': sent_lang_data['global_topic'].nunique() - (1 if -1 in sent_lang_data['global_topic'].values else 0),\n",
    "                    'top_topics': topic_list\n",
    "                }\n",
    "            \n",
    "            language_data[language] = {\n",
    "                'total_documents': len(lang_data),\n",
    "                'document_percentage': round((len(lang_data) / len(self.final_df)) * 100, 2),\n",
    "                'unique_topics': lang_data['global_topic'].nunique() - (1 if -1 in lang_data['global_topic'].values else 0),\n",
    "                'sentiment_breakdown': sentiment_breakdown,\n",
    "                'geographic_distribution': lang_data['ciudad'].value_counts().head(10).to_dict() if 'ciudad' in lang_data.columns else {},\n",
    "                'temporal_distribution': lang_data['a√±o'].value_counts().to_dict() if 'a√±o' in lang_data.columns else {},\n",
    "                'category_distribution': lang_data['categoria'].value_counts().head(5).to_dict() if 'categoria' in lang_data.columns else {}\n",
    "            }\n",
    "        \n",
    "        # Ordenar por n√∫mero de documentos\n",
    "        language_data = dict(sorted(language_data.items(), \n",
    "                                  key=lambda x: x[1]['total_documents'], \n",
    "                                  reverse=True))\n",
    "        \n",
    "        # Guardar an√°lisis por idioma\n",
    "        with open(self.streamlit_dir / \"language_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(language_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"An√°lisis por idioma guardado: {len(language_data)} idiomas\")\n",
    "        return language_data\n",
    "    \n",
    "    def generate_topic_explorer_data(self):\n",
    "        \"\"\"Genera datos para explorador interactivo de temas\"\"\"\n",
    "        \n",
    "        topic_explorer = []\n",
    "        \n",
    "        for sentiment in self.final_df['sentimiento'].unique():\n",
    "            sent_data = self.final_df[self.final_df['sentimiento'] == sentiment]\n",
    "            \n",
    "            for topic_id in sent_data['global_topic'].unique():\n",
    "                if topic_id != -1:\n",
    "                    topic_docs = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                    \n",
    "                    # Estad√≠sticas del tema\n",
    "                    lang_stats = topic_docs['idioma'].value_counts()\n",
    "                    \n",
    "                    topic_explorer.append({\n",
    "                        'sentiment': sentiment,\n",
    "                        'topic_id': int(topic_id),\n",
    "                        'document_count': len(topic_docs),\n",
    "                        'percentage_in_sentiment': round((len(topic_docs) / len(sent_data)) * 100, 2),\n",
    "                        'language_count': lang_stats.nunique(),\n",
    "                        'dominant_language': lang_stats.index[0] if len(lang_stats) > 0 else 'unknown',\n",
    "                        'languages': lang_stats.to_dict(),\n",
    "                        'is_multilingual': lang_stats.nunique() >= 3,\n",
    "                        'top_cities': topic_docs['ciudad'].value_counts().head(5).to_dict() if 'ciudad' in topic_docs.columns else {},\n",
    "                        'top_categories': topic_docs['categoria'].value_counts().head(3).to_dict() if 'categoria' in topic_docs.columns else {},\n",
    "                        'year_range': {\n",
    "                            'start': int(topic_docs['a√±o'].min()) if 'a√±o' in topic_docs.columns and topic_docs['a√±o'].notna().any() else None,\n",
    "                            'end': int(topic_docs['a√±o'].max()) if 'a√±o' in topic_docs.columns and topic_docs['a√±o'].notna().any() else None\n",
    "                        },\n",
    "                        'sample_text': topic_docs['texto'].iloc[0][:300] + \"...\" if len(topic_docs['texto'].iloc[0]) > 300 else topic_docs['texto'].iloc[0]\n",
    "                    })\n",
    "        \n",
    "        # Ordenar por n√∫mero de documentos\n",
    "        topic_explorer.sort(key=lambda x: x['document_count'], reverse=True)\n",
    "        \n",
    "        # Convertir a DataFrame para Streamlit\n",
    "        topic_df = pd.DataFrame(topic_explorer)\n",
    "        topic_df.to_csv(self.streamlit_dir / \"topic_explorer.csv\", index=False, encoding='utf-8')\n",
    "        topic_df.to_pickle(self.streamlit_dir / \"topic_explorer.pkl\")\n",
    "        \n",
    "        print(f\"Explorador de temas guardado: {len(topic_explorer)} temas\")\n",
    "        return topic_df\n",
    "    \n",
    "    def generate_time_series_data(self):\n",
    "        \"\"\"Genera datos para an√°lisis temporal - VERSI√ìN CORREGIDA para datetime64[ns]\"\"\"\n",
    "        \n",
    "        if 'fecha' not in self.final_df.columns:\n",
    "            print(\"No hay columna 'fecha' disponible\")\n",
    "            return None\n",
    "        \n",
    "        # Filtrar datos con fechas v√°lidas (no nulas)\n",
    "        valid_dates = self.final_df[self.final_df['fecha'].notna()].copy()\n",
    "        \n",
    "        if len(valid_dates) == 0:\n",
    "            print(\"No hay fechas v√°lidas en los datos\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Procesando {len(valid_dates):,} registros con fechas v√°lidas de {len(self.final_df):,} totales\")\n",
    "        \n",
    "        # Extraer componentes de fecha directamente desde datetime64[ns]\n",
    "        valid_dates['a√±o_extraido'] = valid_dates['fecha'].dt.year\n",
    "        valid_dates['mes_extraido'] = valid_dates['fecha'].dt.month\n",
    "        valid_dates['dia_extraido'] = valid_dates['fecha'].dt.day\n",
    "        \n",
    "        # Crear year_month string para agrupaci√≥n\n",
    "        valid_dates['year_month'] = valid_dates['fecha'].dt.strftime('%Y-%m')\n",
    "        \n",
    "        # An√°lisis temporal por sentimiento\n",
    "        time_series = []\n",
    "        \n",
    "        for sentiment in valid_dates['sentimiento'].unique():\n",
    "            sent_data = valid_dates[valid_dates['sentimiento'] == sentiment]\n",
    "            \n",
    "            # Agrupaci√≥n temporal por a√±o y mes usando los componentes extra√≠dos\n",
    "            temporal_groups = sent_data.groupby(['a√±o_extraido', 'mes_extraido']).agg({\n",
    "                'global_topic': 'nunique',\n",
    "                'idioma': 'nunique',\n",
    "                'texto': 'count',\n",
    "                'ciudad': 'nunique'\n",
    "            }).reset_index()\n",
    "            \n",
    "            temporal_groups.columns = ['a√±o', 'mes', 'unique_topics', 'unique_languages', 'document_count', 'unique_cities']\n",
    "            temporal_groups['sentiment'] = sentiment\n",
    "            \n",
    "            # CORRECCI√ìN: Crear fecha usando pd.date_range o construcci√≥n manual\n",
    "            temporal_groups['date'] = pd.to_datetime({\n",
    "                'year': temporal_groups['a√±o'],\n",
    "                'month': temporal_groups['mes'], \n",
    "                'day': 1  # Usar d√≠a 1 para todos\n",
    "            })\n",
    "            temporal_groups['year_month'] = temporal_groups['date'].dt.strftime('%Y-%m')\n",
    "            \n",
    "            time_series.append(temporal_groups)\n",
    "        \n",
    "        if time_series:\n",
    "            time_df = pd.concat(time_series, ignore_index=True)\n",
    "            \n",
    "            # Ordenar por fecha\n",
    "            time_df = time_df.sort_values(['a√±o', 'mes'])\n",
    "            \n",
    "            # Agregar estad√≠sticas adicionales\n",
    "            time_df['outliers_count'] = 0\n",
    "            time_df['outliers_percentage'] = 0.0\n",
    "            \n",
    "            # Calcular outliers por per√≠odo de manera m√°s eficiente\n",
    "            for sentiment in valid_dates['sentimiento'].unique():\n",
    "                sent_data = valid_dates[valid_dates['sentimiento'] == sentiment]\n",
    "                \n",
    "                # Agrupar outliers por a√±o-mes\n",
    "                outliers_by_period = sent_data[sent_data['global_topic'] == -1].groupby(['a√±o_extraido', 'mes_extraido']).size()\n",
    "                \n",
    "                # Actualizar DataFrame principal\n",
    "                for (a√±o, mes), outlier_count in outliers_by_period.items():\n",
    "                    mask = (time_df['sentiment'] == sentiment) & (time_df['a√±o'] == a√±o) & (time_df['mes'] == mes)\n",
    "                    time_df.loc[mask, 'outliers_count'] = outlier_count\n",
    "            \n",
    "            # Calcular porcentajes\n",
    "            time_df['outliers_percentage'] = (time_df['outliers_count'] / time_df['document_count'] * 100).round(2)\n",
    "            \n",
    "            # Guardar datos temporales\n",
    "            time_df.to_csv(self.streamlit_dir / \"time_series.csv\", index=False)\n",
    "            time_df.to_pickle(self.streamlit_dir / \"time_series.pkl\")\n",
    "            \n",
    "            # Crear resumen mensual agregado (todos los sentimientos)\n",
    "            monthly_summary = valid_dates.groupby(['a√±o_extraido', 'mes_extraido']).agg({\n",
    "                'global_topic': 'nunique',\n",
    "                'idioma': 'nunique',\n",
    "                'sentimiento': 'nunique',\n",
    "                'texto': 'count',\n",
    "                'ciudad': 'nunique'\n",
    "            }).reset_index()\n",
    "            \n",
    "            monthly_summary.columns = ['a√±o', 'mes', 'unique_topics', 'unique_languages', 'unique_sentiments', 'document_count', 'unique_cities']\n",
    "            \n",
    "            # CORRECCI√ìN: Misma construcci√≥n de fecha para resumen mensual\n",
    "            monthly_summary['date'] = pd.to_datetime({\n",
    "                'year': monthly_summary['a√±o'],\n",
    "                'month': monthly_summary['mes'],\n",
    "                'day': 1\n",
    "            })\n",
    "            monthly_summary['year_month'] = monthly_summary['date'].dt.strftime('%Y-%m')\n",
    "            \n",
    "            monthly_summary.to_csv(self.streamlit_dir / \"monthly_summary.csv\", index=False)\n",
    "            \n",
    "            print(f\"Datos temporales guardados: {len(time_df)} puntos de tiempo\")\n",
    "            print(f\"Rango temporal: {time_df['a√±o'].min()}-{time_df['mes'].min():02d} a {time_df['a√±o'].max()}-{time_df['mes'].max():02d}\")\n",
    "            \n",
    "            return time_df\n",
    "        \n",
    "        return None\n",
    "    def generate_daily_trends(self):\n",
    "        \"\"\"Genera tendencias diarias - VERSI√ìN CORREGIDA\"\"\"\n",
    "        \n",
    "        if 'fecha' not in self.final_df.columns:\n",
    "            return None\n",
    "        \n",
    "        # Filtrar datos v√°lidos\n",
    "        valid_dates = self.final_df[self.final_df['fecha'].notna()].copy()\n",
    "        \n",
    "        if len(valid_dates) == 0:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Extraer solo la fecha (sin hora) para agrupaci√≥n diaria\n",
    "            valid_dates['date_only'] = valid_dates['fecha'].dt.date\n",
    "            \n",
    "            # An√°lisis diario (solo si hay suficientes datos)\n",
    "            daily_counts = valid_dates['date_only'].value_counts()\n",
    "            \n",
    "            if len(daily_counts) > 30:  # Solo si tenemos m√°s de 30 d√≠as de datos\n",
    "                daily_trends = []\n",
    "                \n",
    "                for sentiment in valid_dates['sentimiento'].unique():\n",
    "                    sent_data = valid_dates[valid_dates['sentimiento'] == sentiment]\n",
    "                    \n",
    "                    daily_sent = sent_data.groupby('date_only').agg({\n",
    "                        'texto': 'count',\n",
    "                        'global_topic': 'nunique',\n",
    "                        'idioma': 'nunique'\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    daily_sent.columns = ['date', 'document_count', 'unique_topics', 'unique_languages']\n",
    "                    daily_sent['sentiment'] = sentiment\n",
    "                    daily_sent['date_str'] = daily_sent['date'].astype(str)\n",
    "                    \n",
    "                    # Convertir date a datetime para ordenamiento\n",
    "                    daily_sent['date_dt'] = pd.to_datetime(daily_sent['date'])\n",
    "                    \n",
    "                    daily_trends.append(daily_sent)\n",
    "                \n",
    "                if daily_trends:\n",
    "                    daily_df = pd.concat(daily_trends, ignore_index=True)\n",
    "                    daily_df = daily_df.sort_values('date_dt')\n",
    "                    \n",
    "                    # Remover la columna auxiliar antes de guardar\n",
    "                    daily_df = daily_df.drop('date_dt', axis=1)\n",
    "                    \n",
    "                    daily_df.to_csv(self.streamlit_dir / \"daily_trends.csv\", index=False)\n",
    "                    \n",
    "                    print(f\"Tendencias diarias guardadas: {len(daily_df)} puntos diarios\")\n",
    "                    return daily_df\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error generando tendencias diarias: {e}\")\n",
    "        \n",
    "        return None\n",
    "    def generate_enhanced_temporal_analysis(self):\n",
    "        \"\"\"An√°lisis temporal mejorado - VERSI√ìN CORREGIDA\"\"\"\n",
    "        \n",
    "        if 'fecha' not in self.final_df.columns:\n",
    "            print(\"No hay datos de fecha disponibles\")\n",
    "            return None\n",
    "        \n",
    "        # Verificar datos v√°lidos\n",
    "        valid_count = self.final_df['fecha'].notna().sum()\n",
    "        total_count = len(self.final_df)\n",
    "        \n",
    "        print(f\"Fechas v√°lidas: {valid_count:,} de {total_count:,} ({valid_count/total_count*100:.1f}%)\")\n",
    "        \n",
    "        if valid_count == 0:\n",
    "            print(\"No hay fechas v√°lidas para an√°lisis temporal\")\n",
    "            return None\n",
    "        \n",
    "        # Generar series temporales principales\n",
    "        time_series = self.generate_time_series_data()\n",
    "        \n",
    "        # Generar tendencias diarias si hay suficientes datos\n",
    "        daily_trends = self.generate_daily_trends()\n",
    "        \n",
    "        # An√°lisis de estacionalidad\n",
    "        valid_dates = self.final_df[self.final_df['fecha'].notna()].copy()\n",
    "        \n",
    "        if len(valid_dates) > 0:\n",
    "            try:\n",
    "                # Extraer componentes temporales\n",
    "                valid_dates['month'] = valid_dates['fecha'].dt.month\n",
    "                valid_dates['weekday'] = valid_dates['fecha'].dt.dayofweek\n",
    "                valid_dates['quarter'] = valid_dates['fecha'].dt.quarter\n",
    "                valid_dates['year'] = valid_dates['fecha'].dt.year\n",
    "                \n",
    "                # An√°lisis estacional\n",
    "                seasonal_analysis = {}\n",
    "                \n",
    "                # Por mes\n",
    "                month_sentiment = valid_dates.groupby(['month', 'sentimiento']).size().unstack(fill_value=0)\n",
    "                seasonal_analysis['by_month'] = month_sentiment.to_dict('index')\n",
    "                \n",
    "                # Por d√≠a de la semana\n",
    "                weekday_sentiment = valid_dates.groupby(['weekday', 'sentimiento']).size().unstack(fill_value=0)\n",
    "                seasonal_analysis['by_weekday'] = weekday_sentiment.to_dict('index')\n",
    "                \n",
    "                # Por trimestre\n",
    "                quarter_sentiment = valid_dates.groupby(['quarter', 'sentimiento']).size().unstack(fill_value=0)\n",
    "                seasonal_analysis['by_quarter'] = quarter_sentiment.to_dict('index')\n",
    "                \n",
    "                # Por a√±o\n",
    "                year_sentiment = valid_dates.groupby(['year', 'sentimiento']).size().unstack(fill_value=0)\n",
    "                seasonal_analysis['by_year'] = year_sentiment.to_dict('index')\n",
    "                \n",
    "                with open(self.streamlit_dir / \"seasonal_analysis.json\", 'w') as f:\n",
    "                    json.dump(seasonal_analysis, f, indent=2, default=str)\n",
    "                \n",
    "                print(\"An√°lisis estacional guardado\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error en an√°lisis estacional: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'time_series': time_series,\n",
    "            'daily_trends': daily_trends,\n",
    "            'seasonal_available': True,\n",
    "            'valid_dates_count': valid_count,\n",
    "            'total_dates_count': total_count\n",
    "        }\n",
    "    def generate_geographic_data(self):\n",
    "        \"\"\"Genera datos para an√°lisis geogr√°fico\"\"\"\n",
    "        \n",
    "        if 'ciudad' not in self.final_df.columns:\n",
    "            print(\"No hay datos geogr√°ficos disponibles\")\n",
    "            return None\n",
    "        \n",
    "        geo_data = []\n",
    "        \n",
    "        for city in self.final_df['ciudad'].value_counts().head(50).index:  # Top 50 ciudades\n",
    "            city_data = self.final_df[self.final_df['ciudad'] == city]\n",
    "            \n",
    "            sentiment_breakdown = city_data['sentimiento'].value_counts().to_dict()\n",
    "            language_breakdown = city_data['idioma'].value_counts().to_dict()\n",
    "            \n",
    "            geo_data.append({\n",
    "                'ciudad': city,\n",
    "                'total_documents': len(city_data),\n",
    "                'unique_topics': city_data['global_topic'].nunique() - (1 if -1 in city_data['global_topic'].values else 0),\n",
    "                'unique_languages': city_data['idioma'].nunique(),\n",
    "                'sentiment_breakdown': sentiment_breakdown,\n",
    "                'language_breakdown': language_breakdown,\n",
    "                'dominant_sentiment': max(sentiment_breakdown, key=sentiment_breakdown.get),\n",
    "                'dominant_language': max(language_breakdown, key=language_breakdown.get)\n",
    "            })\n",
    "        \n",
    "        geo_df = pd.DataFrame(geo_data)\n",
    "        geo_df.to_csv(self.streamlit_dir / \"geographic_data.csv\", index=False, encoding='utf-8')\n",
    "        geo_df.to_pickle(self.streamlit_dir / \"geographic_data.pkl\")\n",
    "        \n",
    "        print(f\"Datos geogr√°ficos guardados: {len(geo_df)} ciudades\")\n",
    "        return geo_df\n",
    "    \n",
    "    def generate_multilingual_insights(self):\n",
    "        \"\"\"Genera insights espec√≠ficos para temas multiling√ºes\"\"\"\n",
    "        \n",
    "        multilingual_topics = []\n",
    "        \n",
    "        for sentiment in self.final_df['sentimiento'].unique():\n",
    "            sent_data = self.final_df[self.final_df['sentimiento'] == sentiment]\n",
    "            \n",
    "            for topic_id in sent_data['global_topic'].unique():\n",
    "                if topic_id != -1:\n",
    "                    topic_docs = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                    language_count = topic_docs['idioma'].nunique()\n",
    "                    \n",
    "                    if language_count >= 3:  # Temas con 3+ idiomas\n",
    "                        lang_dist = topic_docs['idioma'].value_counts()\n",
    "                        \n",
    "                        # Calcular diversidad ling√º√≠stica (√≠ndice de Shannon)\n",
    "                        total_docs = len(topic_docs)\n",
    "                        shannon_diversity = -sum((count/total_docs) * np.log(count/total_docs) \n",
    "                                               for count in lang_dist.values)\n",
    "                        \n",
    "                        multilingual_topics.append({\n",
    "                            'sentiment': sentiment,\n",
    "                            'topic_id': int(topic_id),\n",
    "                            'document_count': total_docs,\n",
    "                            'language_count': language_count,\n",
    "                            'shannon_diversity': round(shannon_diversity, 3),\n",
    "                            'languages': lang_dist.to_dict(),\n",
    "                            'dominant_language': lang_dist.index[0],\n",
    "                            'dominance_percentage': round((lang_dist.iloc[0] / total_docs) * 100, 2),\n",
    "                            'geographic_spread': topic_docs['ciudad'].nunique() if 'ciudad' in topic_docs.columns else 0,\n",
    "                            'category_spread': topic_docs['categoria'].nunique() if 'categoria' in topic_docs.columns else 0\n",
    "                        })\n",
    "        \n",
    "        # Ordenar por diversidad ling√º√≠stica\n",
    "        multilingual_topics.sort(key=lambda x: x['shannon_diversity'], reverse=True)\n",
    "        \n",
    "        # Guardar insights multiling√ºes\n",
    "        with open(self.streamlit_dir / \"multilingual_insights.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(multilingual_topics, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        multilingual_df = pd.DataFrame(multilingual_topics)\n",
    "        if len(multilingual_df) > 0:\n",
    "            multilingual_df.to_csv(self.streamlit_dir / \"multilingual_topics.csv\", index=False, encoding='utf-8')\n",
    "            multilingual_df.to_pickle(self.streamlit_dir / \"multilingual_topics.pkl\")\n",
    "        \n",
    "        print(f\"Insights multiling√ºes guardados: {len(multilingual_topics)} temas multiling√ºes\")\n",
    "        return multilingual_topics\n",
    "    def generate_weather_sentiment_analysis(self):\n",
    "        \"\"\"Genera an√°lisis de correlaci√≥n entre clima y sentimiento\"\"\"\n",
    "        \n",
    "        if 'descripcion_sencilla' not in self.final_df.columns:\n",
    "            print(\"No hay datos clim√°ticos disponibles\")\n",
    "            return None\n",
    "        \n",
    "        # Filtrar datos con informaci√≥n clim√°tica v√°lida\n",
    "        weather_data = self.final_df.dropna(subset=['descripcion_sencilla'])\n",
    "        \n",
    "        if len(weather_data) == 0:\n",
    "            print(\"No hay descripciones clim√°ticas v√°lidas\")\n",
    "            return None\n",
    "        \n",
    "        weather_analysis = {}\n",
    "        \n",
    "        # An√°lisis por condici√≥n clim√°tica\n",
    "        for weather in weather_data['descripcion_sencilla'].unique():\n",
    "            weather_subset = weather_data[weather_data['descripcion_sencilla'] == weather]\n",
    "            \n",
    "            # Distribuci√≥n de sentimientos por clima\n",
    "            sentiment_dist = weather_subset['sentimiento'].value_counts()\n",
    "            \n",
    "            # An√°lisis de temas por clima\n",
    "            topic_analysis = {}\n",
    "            for sentiment in weather_subset['sentimiento'].unique():\n",
    "                sent_weather_data = weather_subset[weather_subset['sentimiento'] == sentiment]\n",
    "                top_topics = sent_weather_data['global_topic'].value_counts().head(5)\n",
    "                \n",
    "                topic_list = []\n",
    "                for topic_id, count in top_topics.items():\n",
    "                    if topic_id != -1:\n",
    "                        topic_list.append({\n",
    "                            'topic_id': int(topic_id),\n",
    "                            'document_count': int(count),\n",
    "                            'percentage': round((count / len(sent_weather_data)) * 100, 2)\n",
    "                        })\n",
    "                \n",
    "                topic_analysis[sentiment] = {\n",
    "                    'document_count': len(sent_weather_data),\n",
    "                    'top_topics': topic_list\n",
    "                }\n",
    "            \n",
    "            # An√°lisis temporal por clima\n",
    "            temporal_dist = {}\n",
    "            if 'fecha' in weather_subset.columns:\n",
    "                weather_subset_copy = weather_subset.copy()\n",
    "                weather_subset_copy['fecha_parsed'] = pd.to_datetime(weather_subset_copy['fecha'])\n",
    "                weather_subset_copy['month'] = weather_subset_copy['fecha_parsed'].dt.month\n",
    "                temporal_dist = weather_subset_copy['month'].value_counts().to_dict()\n",
    "            \n",
    "            weather_analysis[weather] = {\n",
    "                'total_documents': len(weather_subset),\n",
    "                'percentage_of_total': round((len(weather_subset) / len(weather_data)) * 100, 2),\n",
    "                'sentiment_distribution': sentiment_dist.to_dict(),\n",
    "                'dominant_sentiment': sentiment_dist.index[0] if len(sentiment_dist) > 0 else 'unknown',\n",
    "                'sentiment_percentage': round((sentiment_dist.iloc[0] / len(weather_subset)) * 100, 2) if len(sentiment_dist) > 0 else 0,\n",
    "                'topic_analysis_by_sentiment': topic_analysis,\n",
    "                'monthly_distribution': temporal_dist,\n",
    "                'unique_languages': weather_subset['idioma'].nunique(),\n",
    "                'unique_cities': weather_subset['ciudad'].nunique() if 'ciudad' in weather_subset.columns else 0\n",
    "            }\n",
    "        \n",
    "        # Ordenar por frecuencia\n",
    "        weather_analysis = dict(sorted(weather_analysis.items(), \n",
    "                                     key=lambda x: x[1]['total_documents'], \n",
    "                                     reverse=True))\n",
    "        \n",
    "        # Guardar an√°lisis clim√°tico\n",
    "        with open(self.streamlit_dir / \"weather_sentiment_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(weather_analysis, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"An√°lisis clim√°tico guardado: {len(weather_analysis)} condiciones clim√°ticas\")\n",
    "        return weather_analysis\n",
    "\n",
    "    def generate_weather_temporal_correlation(self):\n",
    "        \"\"\"Correlaciones clima-tiempo - VERSI√ìN CORREGIDA para fechas con nulos\"\"\"\n",
    "        \n",
    "        if 'descripcion_sencilla' not in self.final_df.columns or 'fecha' not in self.final_df.columns:\n",
    "            print(\"Datos insuficientes para correlaci√≥n clim√°tico-temporal\")\n",
    "            return None\n",
    "        \n",
    "        # Filtrar datos v√°lidos (clima y fecha no nulos)\n",
    "        valid_data = self.final_df[\n",
    "            (self.final_df['descripcion_sencilla'].notna()) & \n",
    "            (self.final_df['fecha'].notna())\n",
    "        ].copy()\n",
    "        \n",
    "        if len(valid_data) == 0:\n",
    "            print(\"No hay datos v√°lidos para correlaci√≥n clim√°tico-temporal\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Datos v√°lidos para an√°lisis clim√°tico-temporal: {len(valid_data):,}\")\n",
    "        \n",
    "        try:\n",
    "            # Extraer componentes temporales\n",
    "            valid_data['month'] = valid_data['fecha'].dt.month\n",
    "            valid_data['year'] = valid_data['fecha'].dt.year\n",
    "            valid_data['season'] = valid_data['month'].map({\n",
    "                12: 'Invierno', 1: 'Invierno', 2: 'Invierno',\n",
    "                3: 'Primavera', 4: 'Primavera', 5: 'Primavera',\n",
    "                6: 'Verano', 7: 'Verano', 8: 'Verano',\n",
    "                9: 'Oto√±o', 10: 'Oto√±o', 11: 'Oto√±o'\n",
    "            })\n",
    "            \n",
    "            # An√°lisis estacional del clima\n",
    "            seasonal_weather = []\n",
    "            \n",
    "            for season in valid_data['season'].unique():\n",
    "                season_data = valid_data[valid_data['season'] == season]\n",
    "                \n",
    "                for weather in season_data['descripcion_sencilla'].unique():\n",
    "                    weather_season_data = season_data[season_data['descripcion_sencilla'] == weather]\n",
    "                    \n",
    "                    if len(weather_season_data) > 0:  # Verificar que hay datos\n",
    "                        sentiment_dist = weather_season_data['sentimiento'].value_counts()\n",
    "                        \n",
    "                        seasonal_weather.append({\n",
    "                            'season': season,\n",
    "                            'weather': weather,\n",
    "                            'total_documents': len(weather_season_data),\n",
    "                            'sentiment_breakdown': sentiment_dist.to_dict(),\n",
    "                            'dominant_sentiment': sentiment_dist.index[0] if len(sentiment_dist) > 0 else 'unknown',\n",
    "                            'unique_topics': weather_season_data['global_topic'].nunique() - (1 if -1 in weather_season_data['global_topic'].values else 0),\n",
    "                            'unique_languages': weather_season_data['idioma'].nunique(),\n",
    "                            'unique_cities': weather_season_data['ciudad'].nunique() if 'ciudad' in weather_season_data.columns else 0\n",
    "                        })\n",
    "            \n",
    "            if seasonal_weather:\n",
    "                # Convertir a DataFrame para an√°lisis\n",
    "                seasonal_df = pd.DataFrame(seasonal_weather)\n",
    "                seasonal_df.to_csv(self.streamlit_dir / \"seasonal_weather_correlation.csv\", index=False, encoding='utf-8')\n",
    "                \n",
    "                # Crear matriz de correlaci√≥n clima-sentimiento\n",
    "                correlation_matrix = valid_data.groupby(['descripcion_sencilla', 'sentimiento']).size().unstack(fill_value=0)\n",
    "                correlation_matrix.to_csv(self.streamlit_dir / \"weather_sentiment_matrix.csv\", encoding='utf-8')\n",
    "                \n",
    "                print(f\"Correlaci√≥n clim√°tico-temporal guardada: {len(seasonal_weather)} combinaciones estaci√≥n-clima\")\n",
    "                return seasonal_df\n",
    "            else:\n",
    "                print(\"No se generaron datos de correlaci√≥n estacional\")\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en correlaci√≥n clim√°tico-temporal: {e}\")\n",
    "            return None\n",
    "\n",
    "    def generate_enhanced_topic_analysis_with_weather(self):\n",
    "        \"\"\"Genera an√°lisis de temas enriquecido con datos clim√°ticos\"\"\"\n",
    "        \n",
    "        if 'descripcion_sencilla' not in self.final_df.columns:\n",
    "            print(\"No hay datos clim√°ticos para enriquecer an√°lisis de temas\")\n",
    "            return None\n",
    "        \n",
    "        weather_topic_analysis = {}\n",
    "        \n",
    "        for sentiment in self.final_df['sentimiento'].unique():\n",
    "            sent_data = self.final_df[self.final_df['sentimiento'] == sentiment]\n",
    "            \n",
    "            topic_weather_analysis = []\n",
    "            \n",
    "            # Analizar top 20 temas para este sentimiento\n",
    "            top_topics = sent_data['global_topic'].value_counts().head(20)\n",
    "            \n",
    "            for topic_id, count in top_topics.items():\n",
    "                if topic_id != -1:\n",
    "                    topic_data = sent_data[sent_data['global_topic'] == topic_id]\n",
    "                    \n",
    "                    # An√°lisis clim√°tico para este tema\n",
    "                    weather_dist = topic_data['descripcion_sencilla'].value_counts()\n",
    "                    \n",
    "                    # An√°lisis temporal-clim√°tico\n",
    "                    temporal_weather = {}\n",
    "                    if 'fecha' in topic_data.columns:\n",
    "                        try:\n",
    "                            topic_data_copy = topic_data.copy()\n",
    "                            topic_data_copy['fecha_parsed'] = pd.to_datetime(topic_data_copy['fecha'])\n",
    "                            topic_data_copy['month'] = topic_data_copy['fecha_parsed'].dt.month\n",
    "                            \n",
    "                            for month in topic_data_copy['month'].unique():\n",
    "                                month_data = topic_data_copy[topic_data_copy['month'] == month]\n",
    "                                month_weather = month_data['descripcion_sencilla'].value_counts()\n",
    "                                temporal_weather[int(month)] = month_weather.to_dict()\n",
    "                        except:\n",
    "                            pass\n",
    "                    \n",
    "                    # Ejemplos con contexto clim√°tico\n",
    "                    weather_examples = {}\n",
    "                    for weather in weather_dist.head(3).index:\n",
    "                        weather_docs = topic_data[topic_data['descripcion_sencilla'] == weather]\n",
    "                        sample = weather_docs.sample(min(2, len(weather_docs)))\n",
    "                        \n",
    "                        examples = []\n",
    "                        for _, doc in sample.iterrows():\n",
    "                            examples.append({\n",
    "                                'texto': doc['texto'][:150] + \"...\" if len(doc['texto']) > 150 else doc['texto'],\n",
    "                                'idioma': doc['idioma'],\n",
    "                                'ciudad': doc.get('ciudad', 'N/A'),\n",
    "                                'fecha': str(doc.get('fecha', 'N/A'))\n",
    "                            })\n",
    "                        \n",
    "                        weather_examples[weather] = examples\n",
    "                    \n",
    "                    topic_weather_analysis.append({\n",
    "                        'topic_id': int(topic_id),\n",
    "                        'document_count': int(count),\n",
    "                        'percentage': round((count / len(sent_data)) * 100, 2),\n",
    "                        'weather_distribution': weather_dist.to_dict(),\n",
    "                        'dominant_weather': weather_dist.index[0] if len(weather_dist) > 0 else 'unknown',\n",
    "                        'weather_diversity': weather_dist.nunique(),\n",
    "                        'temporal_weather_patterns': temporal_weather,\n",
    "                        'weather_examples': weather_examples,\n",
    "                        'weather_sentiment_coherence': self._calculate_weather_coherence(weather_dist, sentiment)\n",
    "                    })\n",
    "            \n",
    "            weather_topic_analysis[sentiment] = topic_weather_analysis\n",
    "        \n",
    "        # Guardar an√°lisis enriquecido\n",
    "        with open(self.streamlit_dir / \"topics_with_weather_analysis.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(weather_topic_analysis, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"An√°lisis de temas con clima guardado para {len(weather_topic_analysis)} sentimientos\")\n",
    "        return weather_topic_analysis\n",
    "\n",
    "    def _calculate_weather_coherence(self, weather_dist, sentiment):\n",
    "        \"\"\"Calcula coherencia entre clima y sentimiento\"\"\"\n",
    "        \n",
    "        # Mapeo simple de coherencia clima-sentimiento\n",
    "        positive_weather = ['soleado', 'despejado', 'parcialmente nublado', 'claro']\n",
    "        negative_weather = ['lluvioso', 'tormentoso', 'nublado', 'nevando']\n",
    "        \n",
    "        if len(weather_dist) == 0:\n",
    "            return 0\n",
    "        \n",
    "        dominant_weather = weather_dist.index[0].lower()\n",
    "        \n",
    "        if sentiment == 'positivo':\n",
    "            return 1 if any(pw in dominant_weather for pw in positive_weather) else 0\n",
    "        elif sentiment == 'negativo':\n",
    "            return 1 if any(nw in dominant_weather for nw in negative_weather) else 0\n",
    "        else:\n",
    "            return 0.5  # neutro\n",
    "    \n",
    "    def generate_weather_insights_summary(self):\n",
    "        \"\"\"Genera resumen de insights clim√°ticos\"\"\"\n",
    "        \n",
    "        if 'descripcion_sencilla' not in self.final_df.columns:\n",
    "            return None\n",
    "        \n",
    "        weather_data = self.final_df.dropna(subset=['descripcion_sencilla'])\n",
    "        \n",
    "        insights = {\n",
    "            'total_documents_with_weather': len(weather_data),\n",
    "            'unique_weather_conditions': weather_data['descripcion_sencilla'].nunique(),\n",
    "            'weather_frequency': weather_data['descripcion_sencilla'].value_counts().to_dict(),\n",
    "            'sentiment_weather_correlation': {},\n",
    "            'weather_language_patterns': {},\n",
    "            'seasonal_weather_summary': {}\n",
    "        }\n",
    "        \n",
    "        # Correlaci√≥n sentimiento-clima\n",
    "        for sentiment in weather_data['sentimiento'].unique():\n",
    "            sent_weather = weather_data[weather_data['sentimiento'] == sentiment]\n",
    "            insights['sentiment_weather_correlation'][sentiment] = {\n",
    "                'most_common_weather': sent_weather['descripcion_sencilla'].value_counts().head(5).to_dict(),\n",
    "                'document_count': len(sent_weather)\n",
    "            }\n",
    "        \n",
    "        # Patrones idioma-clima\n",
    "        for weather in weather_data['descripcion_sencilla'].value_counts().head(10).index:\n",
    "            weather_subset = weather_data[weather_data['descripcion_sencilla'] == weather]\n",
    "            insights['weather_language_patterns'][weather] = {\n",
    "                'languages': weather_subset['idioma'].value_counts().head(5).to_dict(),\n",
    "                'dominant_language': weather_subset['idioma'].value_counts().index[0]\n",
    "            }\n",
    "        \n",
    "        # Guardar insights\n",
    "        with open(self.streamlit_dir / \"weather_insights_summary.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(insights, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Resumen de insights clim√°ticos guardado\")\n",
    "        return insights\n",
    "    def generate_all_streamlit_data_with_weather(self):\n",
    "        \"\"\"Genera todos los datasets incluyendo an√°lisis clim√°tico completo - VERSI√ìN CORREGIDA\"\"\"\n",
    "        \n",
    "        print(\"=== GENERANDO DATOS PARA STREAMLIT (CON AN√ÅLISIS CLIM√ÅTICO) ===\")\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # An√°lisis b√°sicos existentes\n",
    "        try:\n",
    "            results['overview'] = self.generate_overview_metrics()\n",
    "            print(\"‚úì M√©tricas generales completadas\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en m√©tricas generales: {e}\")\n",
    "            results['overview'] = None\n",
    "        \n",
    "        try:\n",
    "            results['sentiment_analysis'] = self.generate_sentiment_analysis()\n",
    "            print(\"‚úì An√°lisis por sentimiento completado\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en an√°lisis por sentimiento: {e}\")\n",
    "            results['sentiment_analysis'] = None\n",
    "        \n",
    "        try:\n",
    "            results['language_analysis'] = self.generate_language_analysis()\n",
    "            print(\"‚úì An√°lisis por idioma completado\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en an√°lisis por idioma: {e}\")\n",
    "            results['language_analysis'] = None\n",
    "        \n",
    "        try:\n",
    "            results['topic_explorer'] = self.generate_topic_explorer_data()\n",
    "            print(\"‚úì Explorador de temas completado\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en explorador de temas: {e}\")\n",
    "            results['topic_explorer'] = None\n",
    "        \n",
    "        # An√°lisis temporal mejorado\n",
    "        try:\n",
    "            results['temporal_analysis'] = self.generate_enhanced_temporal_analysis()\n",
    "            if results['temporal_analysis']:\n",
    "                print(\"‚úì An√°lisis temporal completado\")\n",
    "            else:\n",
    "                print(\"- An√°lisis temporal no disponible\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en an√°lisis temporal: {e}\")\n",
    "            results['temporal_analysis'] = None\n",
    "        \n",
    "        # An√°lisis geogr√°fico\n",
    "        try:\n",
    "            results['geographic'] = self.generate_geographic_data()\n",
    "            if results['geographic'] is not None:\n",
    "                print(\"‚úì Datos geogr√°ficos completados\")\n",
    "            else:\n",
    "                print(\"- Datos geogr√°ficos no disponibles\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en datos geogr√°ficos: {e}\")\n",
    "            results['geographic'] = None\n",
    "        \n",
    "        # An√°lisis clim√°ticos\n",
    "        try:\n",
    "            results['weather_sentiment'] = self.generate_weather_sentiment_analysis()\n",
    "            if results['weather_sentiment']:\n",
    "                print(\"‚úì An√°lisis clim√°tico-sentimiento completado\")\n",
    "            else:\n",
    "                print(\"- An√°lisis clim√°tico no disponible\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en an√°lisis clim√°tico: {e}\")\n",
    "            results['weather_sentiment'] = None\n",
    "        \n",
    "        try:\n",
    "            results['weather_temporal'] = self.generate_weather_temporal_correlation()\n",
    "            if results['weather_temporal'] is not None:\n",
    "                print(\"‚úì Correlaci√≥n clim√°tico-temporal completada\")\n",
    "            else:\n",
    "                print(\"- Correlaci√≥n clim√°tico-temporal no disponible\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en correlaci√≥n clim√°tico-temporal: {e}\")\n",
    "            results['weather_temporal'] = None\n",
    "        \n",
    "        try:\n",
    "            results['topics_with_weather'] = self.generate_enhanced_topic_analysis_with_weather()\n",
    "            if results['topics_with_weather']:\n",
    "                print(\"‚úì An√°lisis de temas con clima completado\")\n",
    "            else:\n",
    "                print(\"- An√°lisis de temas con clima no disponible\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en an√°lisis de temas con clima: {e}\")\n",
    "            results['topics_with_weather'] = None\n",
    "        \n",
    "        try:\n",
    "            results['weather_insights'] = self.generate_weather_insights_summary()\n",
    "            if results['weather_insights']:\n",
    "                print(\"‚úì Resumen de insights clim√°ticos completado\")\n",
    "            else:\n",
    "                print(\"- Insights clim√°ticos no disponibles\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en insights clim√°ticos: {e}\")\n",
    "            results['weather_insights'] = None\n",
    "        \n",
    "        try:\n",
    "            results['multilingual'] = self.generate_multilingual_insights()\n",
    "            print(\"‚úì Insights multiling√ºes completados\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error en insights multiling√ºes: {e}\")\n",
    "            results['multilingual'] = None\n",
    "        \n",
    "        # CORRECCI√ìN: Crear configuraci√≥n con validaci√≥n de tipos apropiada\n",
    "        available_datasets = []\n",
    "        \n",
    "        # Funci√≥n auxiliar para verificar si un resultado es v√°lido\n",
    "        def is_valid_result(result):\n",
    "            if result is None:\n",
    "                return False\n",
    "            if isinstance(result, dict) and len(result) == 0:\n",
    "                return False\n",
    "            if isinstance(result, list) and len(result) == 0:\n",
    "                return False\n",
    "            if hasattr(result, '__len__') and len(result) == 0:\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        # Verificar cada dataset individualmente\n",
    "        if is_valid_result(results.get('overview')):\n",
    "            available_datasets.append('overview_metrics.json')\n",
    "        \n",
    "        if is_valid_result(results.get('sentiment_analysis')):\n",
    "            available_datasets.append('sentiment_analysis.json')\n",
    "        \n",
    "        if is_valid_result(results.get('language_analysis')):\n",
    "            available_datasets.append('language_analysis.json')\n",
    "        \n",
    "        if is_valid_result(results.get('topic_explorer')):\n",
    "            available_datasets.append('topic_explorer.csv')\n",
    "        \n",
    "        if is_valid_result(results.get('temporal_analysis')):\n",
    "            available_datasets.append('time_series.csv')\n",
    "            available_datasets.append('monthly_summary.csv')\n",
    "        \n",
    "        if is_valid_result(results.get('geographic')):\n",
    "            available_datasets.append('geographic_data.csv')\n",
    "        \n",
    "        if is_valid_result(results.get('weather_sentiment')):\n",
    "            available_datasets.append('weather_sentiment_analysis.json')\n",
    "        \n",
    "        if is_valid_result(results.get('weather_temporal')):\n",
    "            available_datasets.append('seasonal_weather_correlation.csv')\n",
    "            available_datasets.append('weather_sentiment_matrix.csv')\n",
    "        \n",
    "        if is_valid_result(results.get('topics_with_weather')):\n",
    "            available_datasets.append('topics_with_weather_analysis.json')\n",
    "        \n",
    "        if is_valid_result(results.get('weather_insights')):\n",
    "            available_datasets.append('weather_insights_summary.json')\n",
    "        \n",
    "        if is_valid_result(results.get('multilingual')):\n",
    "            available_datasets.append('multilingual_insights.json')\n",
    "        \n",
    "        # Crear configuraci√≥n\n",
    "        config = {\n",
    "            'data_path': str(self.streamlit_dir),\n",
    "            'total_documents': len(self.final_df),\n",
    "            'total_topics': 0,\n",
    "            'sentiments': list(self.final_df['sentimiento'].unique()),\n",
    "            'languages': list(self.final_df['idioma'].unique()[:20]),\n",
    "            'weather_conditions': list(self.final_df['descripcion_sencilla'].unique()) if 'descripcion_sencilla' in self.final_df.columns else [],\n",
    "            'available_datasets': available_datasets,\n",
    "            'weather_analysis_available': is_valid_result(results.get('weather_sentiment')),\n",
    "            'temporal_analysis_available': is_valid_result(results.get('temporal_analysis')),\n",
    "            'geographic_analysis_available': is_valid_result(results.get('geographic')),\n",
    "            'generation_timestamp': pd.Timestamp.now().isoformat(),\n",
    "            'datasets_generated': {\n",
    "                'overview': is_valid_result(results.get('overview')),\n",
    "                'sentiment_analysis': is_valid_result(results.get('sentiment_analysis')),\n",
    "                'language_analysis': is_valid_result(results.get('language_analysis')),\n",
    "                'topic_explorer': is_valid_result(results.get('topic_explorer')),\n",
    "                'temporal_analysis': is_valid_result(results.get('temporal_analysis')),\n",
    "                'geographic': is_valid_result(results.get('geographic')),\n",
    "                'weather_sentiment': is_valid_result(results.get('weather_sentiment')),\n",
    "                'weather_temporal': is_valid_result(results.get('weather_temporal')),\n",
    "                'topics_with_weather': is_valid_result(results.get('topics_with_weather')),\n",
    "                'weather_insights': is_valid_result(results.get('weather_insights')),\n",
    "                'multilingual': is_valid_result(results.get('multilingual'))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Obtener total de temas si hay datos de overview v√°lidos\n",
    "        if is_valid_result(results.get('overview')):\n",
    "            config['total_topics'] = results['overview'].get('total_topics', 0)\n",
    "        \n",
    "        with open(self.streamlit_dir / \"config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=2)\n",
    "        \n",
    "        # Resumen final\n",
    "        successful_datasets = sum(config['datasets_generated'].values())\n",
    "        total_datasets = len(config['datasets_generated'])\n",
    "        \n",
    "        print(f\"\\nDATOS CON AN√ÅLISIS CLIM√ÅTICO GENERADOS\")\n",
    "        print(f\"Directorio: {self.streamlit_dir}\")\n",
    "        print(f\"Datasets exitosos: {successful_datasets}/{total_datasets}\")\n",
    "        print(f\"Archivos disponibles: {len(available_datasets)}\")\n",
    "        print(f\"An√°lisis clim√°tico: {'S√≠' if config['weather_analysis_available'] else 'No'}\")\n",
    "        print(f\"An√°lisis temporal: {'S√≠' if config['temporal_analysis_available'] else 'No'}\")\n",
    "        print(f\"An√°lisis geogr√°fico: {'S√≠' if config['geographic_analysis_available'] else 'No'}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def create_streamlit_data(final_df, lang_analysis, output_dir=\"E:/bertopic_800k_multilingue\"):\n",
    "    \"\"\"Funci√≥n principal para generar datos de Streamlit\"\"\"\n",
    "    \n",
    "    generator = StreamlitDataGenerator(final_df, lang_analysis, output_dir)\n",
    "    results = generator.generate_all_streamlit_data_with_weather()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436e1404",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuraci√≥n de BERTopic\n",
    "- Define hiperpar√°metros/pipe (vectorizador, umap/hdbscan, idioma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cda75a47-2f50-45c3-9172-2edad3d0a671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparando datos para Streamlit en: E:\\bertopic_800k_multilingue\\streamlit_data\n",
      "=== GENERANDO DATOS PARA STREAMLIT (CON AN√ÅLISIS CLIM√ÅTICO) ===\n",
      "M√©tricas generales guardadas: 852,566 documentos, 218 temas\n",
      "‚úì M√©tricas generales completadas\n",
      "An√°lisis por sentimiento guardado: 3 sentimientos\n",
      "‚úì An√°lisis por sentimiento completado\n",
      "An√°lisis por idioma guardado: 45 idiomas\n",
      "‚úì An√°lisis por idioma completado\n",
      "Explorador de temas guardado: 498 temas\n",
      "‚úì Explorador de temas completado\n",
      "Fechas v√°lidas: 807,124 de 852,566 (94.7%)\n",
      "Procesando 807,124 registros con fechas v√°lidas de 852,566 totales\n",
      "Datos temporales guardados: 489 puntos de tiempo\n",
      "Rango temporal: 2009-01 a 2026-12\n",
      "Tendencias diarias guardadas: 5278 puntos diarios\n",
      "An√°lisis estacional guardado\n",
      "‚úì An√°lisis temporal completado\n",
      "Datos geogr√°ficos guardados: 8 ciudades\n",
      "‚úì Datos geogr√°ficos completados\n",
      "An√°lisis clim√°tico guardado: 35 condiciones clim√°ticas\n",
      "‚úì An√°lisis clim√°tico-sentimiento completado\n",
      "Datos v√°lidos para an√°lisis clim√°tico-temporal: 701,635\n",
      "Correlaci√≥n clim√°tico-temporal guardada: 104 combinaciones estaci√≥n-clima\n",
      "‚úì Correlaci√≥n clim√°tico-temporal completada\n",
      "An√°lisis de temas con clima guardado para 3 sentimientos\n",
      "‚úì An√°lisis de temas con clima completado\n",
      "Resumen de insights clim√°ticos guardado\n",
      "‚úì Resumen de insights clim√°ticos completado\n",
      "Insights multiling√ºes guardados: 408 temas multiling√ºes\n",
      "‚úì Insights multiling√ºes completados\n",
      "\n",
      "DATOS CON AN√ÅLISIS CLIM√ÅTICO GENERADOS\n",
      "Directorio: E:\\bertopic_800k_multilingue\\streamlit_data\n",
      "Datasets exitosos: 11/11\n",
      "Archivos disponibles: 13\n",
      "An√°lisis clim√°tico: S√≠\n",
      "An√°lisis temporal: S√≠\n",
      "An√°lisis geogr√°fico: S√≠\n"
     ]
    }
   ],
   "source": [
    "streamlit_results = create_streamlit_data(final_df, lang_analysis, \"E:/bertopic_800k_multilingue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a811c695-97a7-4ba9-84ff-27deb54b8466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
